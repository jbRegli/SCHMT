%%%%%%%%%%%%%%%%%%%%%
% Type of document: %
%%%%%%%%%%%%%%%%%%%%%
\documentclass[a4paper,11pt]{report}

%%%%%%%%%%%%%%%%%%%%%
% My usual settings %
%%%%%%%%%%%%%%%%%%%%%
\usepackage{JB_config_article}

%%%%%%%%%%%%%%%%%%%%%%%
% Headers and footers %
%%%%%%%%%%%%%%%%%%%%%%%
%%% Color definition:
\definecolor{lgray}{gray}{0.6}

\usepackage{fancyhdr}
\renewcommand{\headrulewidth}{0pt}
\lhead{}
\rhead{}
\lfoot{\vspace{0.3cm}\small\color{lgray}Research report, UCL Department of Statistical Science}
\rfoot{\vspace{-0.3cm}\includegraphics[width=1.5cm]{ucl.jpg}
\hspace{0.5cm}
\includegraphics[width=1.5cm]{dstl.png}}
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Location of the figures %
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\graphicspath{{Images/}} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Start the document
\begin{document}

\chapter{Conclusion:}
  \label{chap:ccl}

  \section{Future work:}
		\label{sec:Ccl/FutW}
  
		Their are many directions to be studied to improve the SCHMTs. For sake of time and personal interest we limit ourself to four main sub-projects at either short or long term.\\
		
		\subsection{Inference:}
			\label{subsec:Ccl/FutW/Inference}
		
			Even though theoretically SCHMTs are much more versatile, this document only reports classification task. We would like to test other inference options offered by our model.\\
		
			An interesting direction would be to use sensitivity analysis (see Sub-section~\ref{subsec:PGMs/MM/Inference}) to prune the tree and remove the least informative branch of the scattering convolutional network. Reducing the complexity of the tree would have several advantages. First a smaller tree would reduce the computational time required to perform inference. Second a smaller tree is expected to perform better when used for discrimination as the pruning will transform the over-complete representation generated by the SCHMT into a simpler, less redundant representation.\\
				
			Another interesting task would be generation. This means generating the most likely image associated to a given class. This is a two steps process. First one needs to infer the most likely scattering tree given a class. This can be done fairly easily by using the SCHMT model's learned parameters. This done one is left with the most likely scattering convolutional network. Thus the second step is to invert the scattering transform to generate an image. Unfortunately, the invertibility of the scattering transform is still an open question~\citep{cheng2015deep}.\\
				
			All the building blocs for implementing the sensitivity analysis are ready, we just need to define the cleverest way to implement it. Hence we estimate at about a few weeks the time require to do it --- finish mid-month 16. Then testing should take at least a month as the experiments will be fairly long to run (training HMTs for all possible pruning) --- finish end of month 17. Good results in pruning would provide sufficient material for a conference article, especially since it can be combined with other improvements (see subsection~\ref{subsec:Ccl/FutW/VB}). Regarding data generation we will wait until S\'ephane Mallat has properly define the invertibility of the scattering transform~\citep{cheng2015deep} before taking any action on this.
			
		\subsection{Variational Bayes for hidden Markov trees:}
			\label{subsec:Ccl/FutW/VB}

			The version of the EM algorithm considered in this report uses full Bayesian inference. Despite providing interesting results this methods is computationally expensive and only provides with a point-wise estimate of the model's parameters. A way around those issues is to use the Variational Bayes (VB) approximation for inference~\cite{wainwright2008graphical}.\\
			
			The idea behind variational Bayes is to substitute the -most of the time- intractable posterior inference evaluation by an easier to solve optimization problem. This is done by approximating the target posterior by a variational distribution from a simpler parametric family. Meaning that, given a set of data $\bfX$ and the corresponding targets $\bfY$ as well as the model's parameters $\Theta$, the task at end is now the minimization of the Kullback-Leibler divergence between the true target posterior $P(\bfY | \bfX, \Theta)$ and the variational distribution $q(\Theta, \bfY)$. This trick has been applied successfully to a variety of Bayesian models~\citep{attias2000variational}~\citep{wainwright2008graphical}, and even to Hidden Markov Trees~\citep{dasgupta2006texture}~\citep{olariu2009modified}.\\

			Recently Variational Bayes has been improved by leveraging methods borrowed to the optimisation community. \citet{hoffman2013stochastic} have developed a scalable modification to VB using stochastic gradients for optimisation -Stochastic Variational Bayes (SVB). Even more recently,~\citet{NIPS2014_5560} developed a method to apply SVB to hidden Markov Models. \\

			We propose over the next few months to develop a stochastic variational Bayes version of the EM algorithm for hidden Markov trees and apply it to scattering convolutional hidden Markov trees. This project can be broken down into two major steps:\\

			\begin{itemize}
				\item First we first plan to implement a variational version of the EM algorithm for SCHMTs following~\citep{olariu2009modified}. Here again the algorithm has been developed for binary regular trees, but can be adapted to the non-binary non-regular trees we are interested in. We thus forecast a month for the implementation and at least two months for testing and experiments. Hopefully the VB version of SCHMTs will provide good enough results for a conference paper --- implementation finished by the end of month 16, experiments by the end of month 18.\\
				
				\item The logical next step is to develop a stochastic version of the variational Bayes expectation maximization algorithm for hidden Markov trees, leading to the development of SVB-SCHMTs.  This step is expected to require more work than the previous one as the SVB framework as not yet been developed for HMTs. However we can follow the framework defined by JB. Durand~\citep{durand2004computational} when adapting ~\citep{devijver1985baum} smoothed-EM for HMM to HMTs to inspire us. We plan to spend about a month on developing the algorithm -can be done in parallel of coding VB-SCHMTS, another month for coding it and then a couple months for experiments. Hopefully the SVB-SCHMTs will provide enough material to lead to both a conference and a journal paper --- theory by end of month 18 and experiments by the end of month 20.\\
			\end{itemize}

			Variational Bayes provides a powerful framework to simplify and improve the quality of the parameters learned for our graphical model. However the quality of approximation made in the variational Bayes framework is highly correlated to the chosen variational distribution family. Recently~\citet{ranganath2015hierarchical} have developed a framework where the variational approximation is augmented with a prior on its parameters. This method offers a mean to learn the variational distribution family. We would like to adapt this framework to hidden Markov models and more specifically to SVB-SCHMTs --- theory by end of month 21, coding by end of month 22, experiments by end of month 24.
			
		\subsection{Hierarchical graphical models:}
			\label{subsec:Ccl/FutW/HGM}
			
			One of the richness of graphical model is that they can be incorporated into a broader network as a node of this structure. This idea leads to hierarchical graphical models~\citep{fine1998hierarchical}. Using such an architecture opens tremendous number of possibilities. In Section~\ref{sec:Exp/Sonar:} we have mentioned that the UDRC dataset also provided meta-data among which are the spatial coordinates of the sensor for each imagery; a hierarchical graphical model would allow us to leverage those information by modelling the acquisition area as a graphical model whose node would be the outcome of the SCHMTs.\\
			
			To a certain extend this project can be run in parallel to the previous one. We aim at developing a variational framework for this type of model and a procedure for training the full hierarchical model at once ---as opposed to sequential training of each ``layer'' of the hierarchy. The year to come will be used to develop the theory and then a long time will be dedicated to experiments and testing --- end of month 28.     
		
		
		\subsection{Bayesian neural network:}
			\label{subsec:Ccl/FutW/BNN}

			Finally we would like to consider other architectures for the graphical model and make it include the representation learning step. This would be possible using Bayesian neural networks and probabilistic back-propagation~\citep{hernandez2015probabilistic} or a variational bayes approach~\citep{blundell2015weight}. Those neural networks encode the full distribution over their weights ---instead of the standard point wise estimate--- and thus posses all the advantages of graphical models. This project is a very long term one and the idea is to adapt

    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%   
% Bibliography %
%%%%%%%%%%%%%%%%
\renewcommand\bibname{Bibliography:}
\bibliographystyle{apalike}%plainnat}
\bibliography{bib_SCHMT}
% this shows references stored in the bib_SCHMT.bib file
\nocite{*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THE END
\end{document}