%%%%%%%%%%%%%%%%%%%%%
% Type of document: %
%%%%%%%%%%%%%%%%%%%%%
\documentclass[a4paper,11pt]{report}

%%%%%%%%%%%%%%%%%%%%%
% My usual settings %
%%%%%%%%%%%%%%%%%%%%%
\usepackage{JB_config_article}

%%%%%%%%%%%%%%%%%%%%%%%
% Headers and footers %
%%%%%%%%%%%%%%%%%%%%%%%
%%% Color definition:
\definecolor{lgray}{gray}{0.6}

\usepackage{fancyhdr}
\renewcommand{\headrulewidth}{0pt}
\lhead{}
\rhead{}
\lfoot{\vspace{0.3cm}\small\color{lgray}Research report, UCL Department of Statistical Science}
\rfoot{\vspace{-0.3cm}\includegraphics[width=1cm]{placeholder.jpg}
\hspace{0.5cm}
\includegraphics[width=1.5cm]{placeholder.jpg}}
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Location of the figures %
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\graphicspath{{Images/}} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Start the document
\begin{document}

	%% Cover page:
	\begin{titlepage}
	%\vspace*{1cm}
		\hspace{-1.5cm}\includegraphics[width=9cm,height= 3cm]{placeholder.jpg}
		\hfill{
		  \raggedleft \includegraphics[width=3cm]{placeholder.jpg}
		}
		\vspace*{5cm}
	
		\begin{center}
			\begin{sc} 	
				%%% Title:
				\huge SCATTERING HIDDEN MARKOV TREES
				\vspace*{0.2cm}
				%%% Subtitle:
				\\ \large IMAGE REPRESENTATION AND SCATTERING TRANSFORM MODELING
				\vspace*{2cm}
			\end{sc}
			%%% Author:
			\\ \LARGE Jean-Baptiste REGLI
			\vspace*{0.2cm}
			%%% Date
			\\ \large 2013-2014
		\end{center}
		%\vspace*{5cm}

		%%% Supervisors:
		\vfill
		\begin{center}
			\vspace*{1cm}
			\Large RESEARCH REPORT
			\vspace*{0.5cm}
			\\ \large Academic supervisor: James Nelson 
			\\ \large Sponsor: Dstl/UCL Impact studentship
			
			\vspace*{1cm}
			\Large UCL
			\\ \normalsize Department of Statistical Science
			\\ London
		\end{center}
	\end{titlepage}
	\clearpage

	\vfill	
		
	% Blank page
	\newpage
	\thispagestyle{empty}
	\mbox{}
 
	% Table of contents
	\setcounter{tocdepth}{3}
	\renewcommand{\contentsname}{Contents:}
	\tableofcontents
	\clearpage

	% List of figures
	\renewcommand{\listfigurename}{List of figures:}
	\listoffigures

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction:}
	\label{chap:Intro}
	
	Nowadays statistical signal processing and machine learning methods are applied to a wide range of problems from prediction to optimal control. While some of those application involves low-dimensional data, full branches of those fields are dedicated to studying high-dimensional learning problems. High-dimensional problems have been made very popular lately for several reasons. First over the couple last decades data got cheaper to collect. This mainly because of the progress made on digital cameras, microphones or any sensors. Second the cost of data storage also massively decrease during that time. And finally the computational power available today allows to try more computationally intensive methods. And thus methods to leverage those data have been developed across the range of applications in machine learning and statistical signal processing.\\
	
	This document is restricted to one of those applications, high-dimensional classification problems. And even more precisely it aims at introducing a probabilistic approach to high-dimensional classification problems.
		   
	\section{Need for a better signal representation:}
		\label{sec:Intro/Need}
		
		Consider the problem of learning the labeling function $f$, say, given $N$ sampled training values $\{bfx_{i}, y_{i}=f(\bfx_{i})\}_{i\leq N}$ where $x(i) =\{x_{1} \, \dots \, x_{s}\}$ where for all $i \in \llbracket 1 , N \rrbracket$, $d \sim 10^{6}$, $bfx(.) \in \dsR$ and $y_{i} \in \dsN$. Examples of such signals are speech waveforms or digital photographies.  

		\begin{figure}[t!]
			\centering
			\begin{subfigure}[t]{0.5\textwidth}
				\centering
				\includegraphics[height=2.2in]{waveform_flute.eps}
				\caption{Sound of a flute}
			\end{subfigure}%
			~ 
			\begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=2.2in]{im_mandrill.eps}
        \caption{Picture of a mandrill}
			\end{subfigure}
			\caption{High dimensional signals}
			\label{fig:highDim signals}
		\end{figure}
		
		A naive solution to this problem would be to infer the class of a new realization $\bfx$ by looking at its neighbors in a similar fashion to K-Nearest Neighbors (KNN), for example. This approach is sound for low-dimensional problems \cite{cover1967nearest}. However it shows limitations in high dimensional cases \cite{beyer1999nearest} because the number of samples required to find a neighbor of a new realization $\bfx_{new}$ grows exponentially with the number of dimensions.\\
			
		However one can make the assumption that the signal $\bfx$ belongs to a manifold $\Omega$ of $\dsR^{d}$ and this leads to two cases. The subset $\Omega$ can be low dimensional and there is no curse of dimensionality once the manifold has been isolated. The task at hand is thus manifold learning (~\cite{lin2008riemannian},\cite{zhang2012adaptive}) or sparse dictionary representation (~\cite{kreutz2003dictionary}). However for complex signals the manifold $\Omega$ is also high dimensional. In this case one has to reduce the volume of the space without loosing the crucial information necessary to recognized the signal. Hence one has to reduce the volume of $\Omega$ according to the property of the classification function $f$.
		
	\section{Image representation:}
		\label{sec:Intro/Image rep}
		
		A mapping $\Phi$ is sought which represents the signal in a new space such that the classification task is simpler. The space should not only capture the main information and discriminatory content in the data but it should also remain stable with respect to appropriate transformations and deformations. Before providing a formal mathematical description it is instructive to consider the following intuitive example.
%     
		\subsection{Intuition of a ``good'' image representation:}
      \label{sec:Intro/Image rep/Intuition}
      
      Properties of a ``good'' representation for classification can be intuited by looking how humans handle the classification/labeling of visual stimulus and what are the elements ensuring good generalization capacities. Following this approach one can intuit the following properties:
      
      
			\begin{itemize}
				\item The projection has to be informative enough to permit classification. This is ensure that $\Phi$ preserves separability between the different classes.\\

				\item The mapping also has to be invariant to translations. Indeed to a human eye there is no difference in the information carried by a signal if it is shifted. This means that the $\Phi$ has to provide similar, if not equal, outputs for shifted versions of the same signal.\\
		
				\begin{figure}
				  \begin{center}
				    \includegraphics[width=3.5in]{placeholder.jpg}
				    \caption[Translation invariance]{A human can easily tell that those two images are from the same class.}
				    \label{fig:Illustration translation invariance}
				  \end{center}
				  %TODO: (a) initial image (b) sifted one
				\end{figure}
	
				\item To some extend the mapping has to be stable under deformations is also a required. Once again to a human eye, it is still possible to recognize a signal if it has undergone -small- deformations. Yet if the deformations are too important the informational content of the signal can be lost. This means that to a certain degree the answer of the projection to morphed realizations of the same signal should be similar. However one need to define a limit to this invariance to ensure that the representation created is still informative enough.\\
				
				\begin{figure}
				  \begin{center}
				    \includegraphics[width=3.5in]{placeholder.jpg}
				    \caption[Stability to deformations]{A human can easily tell that (a) and (b) are from the same class. (c) can still be recognized even though it is slightly more challenging.}
				    \label{fig:illustration stability deformations}
				    %TODO: (a) initial image (b) sightly deformed one (c) heavy deformation
				  \end{center}
				\end{figure}
				
				\item Again to a certain degree the projection has to be invariant to rotations. Only local invariant to rotation is wanted because excessive rotation applied to the original signal can be destructive for the information carried (see Figure~\ref{fig:Illustration rotation invariance}). Solutions based on the method described in this document exist \cite{sifre2013rotation} but they are still work in progress and thus they will not be addressed nor used in this review. 
				
				\begin{figure}
				  \begin{center}
				    \includegraphics[width=3.5in]{placeholder.jpg}
				    \caption[Rotation invariance]{A human can easily tell that (a) and (b) are from the same class. (c) could be a  '6' slightly rotated or a '9' heavily rotated.}
				    \label{fig:Illustration rotation invariance}
				    %TODO: (a) initial image (b) sightly rotated one (c) 180 rotation
				  \end{center}
				\end{figure}	
			\end{itemize}
      
		\subsection{Formalization of a ``good'' image representation:}
      \label{sec:Intro/Image rep/Formalization}
      
			Throughout, attention will be restricted to square-integrable $d$-dimensional functions over the reals, namely $\bfx \in \mcalL^{2}(\dsR^{d})$. For more accuracy, the classification function is split in two stages. $f$ will now denote the soft classification function, \ie $f(\bfx) \in \dsR^(K)$, and represents the distance to the separating surface. $h$ is the labeling function and $y=h \circ f(\bfx))$ is now the label associated to an signal $\bfx$. To be informative enough, a representation must preserve separability between elements of different classes. This is encapsulated by the following definition.
      
			\begin{defn} \textbf{Preservation of the separability between classes}\\
				A representation $\Phi$ preserves separability if all elements of two different classes are distant of at least a margin $C$ in the representation space, \ie:
				\begin{equation*}
					\forall x,x' \in \dsR^{d} \; \exists C \in \dsR \;\;\; \st \;\;\; h \circ f(x) \neq h \circ f(x') \;\; \Rightarrow \;\; \norm{\Phi(x)-\Phi(x')} \geq C^{-1}
				\end{equation*}
				\label{def:Separability}
			\end{defn}
			
      Translations in the input space should not affect the representation. In this document let $L_{(.)}$ be the translation operator for the function in $\mcalL^{2}(\dsR^{d})$, \ie for $\bfx \in \mcalL^{2}(\dsR^{d}) \;\; \text{and} \;\; (u,c) \in (\dsR^{d})^{2} \;\;\;\; L_{c}\bfx(u) = \bfx(u-c)$. A mapping $\Phi$ is translation invariant -respectively: canonical translation invariant if it maps a translated signal to the same point as its original version.

      \begin{defn} \textbf{Translation invariant}\\ 
				An operator $\Phi: \mcalL^{2}(\dsR^{d}) \rightarrow \mcalH$ where $\mcalH$ is an Hilbert space is translation invariant if:
	      	\begin{equation*}
			  		\forall c \in \dsR^{d} 
			  		\;\; \text{and}  \;\;
			  		\forall \bfx \in \mcalL^{2}(\dsR^{d}) \;\;
			  		\Phi(L_{c}\bfx) = \Phi(\bfx).
				\end{equation*}
				\label{def:Translation invariance - intuition}
      \end{defn}
      \vspace{-30pt}

      \begin{defn} \textbf{Canonical translation invariant}\\ 
				An operator $\Phi: \mcalL^{2}(\dsR^{d}) \rightarrow \mcalH$ where $\mcalH$ is an Hilbert space is canonical translation invariant if:
        \begin{equation*}
			  		\forall \bfx \in \mcalL^{2}(\dsR^{d}) \;\;
			  		\Phi(L_{a}\bfx) = \Phi(\bfx) 
			  		\;\; \text{where} \; a \in \dsR^{d} \; \text{is function of} \; \bfx.
				\end{equation*}
				\label{def:Canonical translation invariance - intuition}
      \end{defn}
      
      For the usual representation operators instabilities to deformations are known to appear -especially at high frequencies. To prevent this, one would like the representation to be non-expansive.
      
      \begin{defn} \textbf{Non-expensive representation}
				A representation $\Phi$ is non-expensive if,
				\begin{equation}
			  		\forall (\bfx,\bfx') \in (\mcalL^{2}(\dsR^{d}))^{2} \;\; 
			  		\norm{\Phi(\bfx) - \Phi(\bfx')} \leq \norm{\bfx-\bfx'}.
				\end{equation}
				\label{def:Non-expansivity - intuition}
      \end{defn}
      
      The stability to deformations of a non-expansive operator can be expressed as its Lipschitz continuity to the action of deformations close to translations \cite{mallat2012gis}. Such a diffeormorphism transform can be expressed as,
      \begin{equation*}
      		\begin{split}
      			L_{\tau}	: \; & \mcalL^{2}(\dsR^{d}) \rightarrow \mcalL^{2}(\dsR^{d})\\
      							  & \;\;\;\; \bfx \;\;\;\;\; \rightarrow  \bfx((.) - \tau(.))
				\end{split}
      \end{equation*}

			where $\tau(u) \in \dsR^{d}$ is a displacement field.
			
			\begin{defn} \textbf{Lipschitz continuous}
				A translation invariant operator $\Phi$ is said to be Lipschitz continuous to the action of $\mcalC^{2}$ diffeomorphisms if for any compact $\Omega \in \dsR^{d}$ there exists $C$ such that for all $f \in \mcalL^{2}(\dsR^{d})$ supported in $\Omega$ and all $\tau \in \mcalC^{2}(\dsR^{d})$,
				\begin{equation}
					\norm{\Phi(\bfx) - \Phi(L_{\tau}\bfx)}_{\mathit{H}} \leq 
					C \norm{\bfx} \left(\sup_{u \in \dsR^{d}} \abs{\nabla \tau(u)} + \sup_{u \in \dsR^{d}} \abs{\mathit{H}\tau(u)}\right)
					\label{eq:Lipschitz continuity}
				\end{equation}
				\label{pty:Lipschitz continuity - intuition}
							
				where $\nabla \tau(u)$ is a matrix whose norm $\abs{\nabla \tau(u)}$ measure the deformation amplitude at $u$ and  $\mathit{H}\tau(u)$ is the Hessian matrix of the deformation whose norm $\abs{\mathit{H}\tau(u)}$ measure its amplitude.
      \end{defn}
      Hence a Lipschitz continuous operator $\Phi$ is almost invariant to deformations by $\tau(.)$, up to the first and second order deformations terms. The Equation~\ref{eq:Lipschitz continuity} also implies that $\Phi$ is invariant to global translations but this is already enforced by the translation invariance requirement.\\ 

    \subsection{State of the art in image representation:}
      \label{sec:Intro/Image rep/State of the art}      
          
			A classic representational method is the modulus of the Fourier transform. This operator is informational enough to allow -to a certain extent- discrimination between different type of signal \cite{baker2014using}. It is also translation invariant \cite{bracewell1965fourier}. However it is well known that those operators present instabilities to deformation at high frequencies \cite{hormander1971fourier} and thus are not Lipschitz continuous to the action of diffeomorphisms.\\ %TODO: what does it mean? Guess: non continuity to echelon -(FT)-> high freq 
      
			Wavelet transform is another popular representation method. Again they provide a representation suitable for classification of different signals \cite{de2000using}. Plus by grouping high frequencies into dyadic packet in $\dsR^{d}$, wavelet operators are stable to -small- deformations \cite{bruna2013invariant}.  %TODO: what does it mean? To be confirm
			
			\begin{equation}
				\begin{matrix}
					W\bfx =
					\begin{pmatrix}
						\bfx \ast \phi \\[0.5em]
						\bfx \ast \psi_{\lambda} \\[0.5em]
					\end{pmatrix}
					\begin{aligned}
						\begin{matrix}
							\rightarrow \text{averaging part}				\\[0.5em]
							\rightarrow \text{high frequency part}	\\[0.5em]
						\end{matrix}
					\end{aligned}
				\end{matrix}
			\end{equation}
		

			However only the averaging part of a wavelet is invariant to translation ans thus wavelets themselves are known to be non-invariant to translations.\\
			
			Another signal representation method popular are the convolutional neural networks \cite{lecun1995convolutional}. As opposed to the two previously mentioned methods, those operators are not fixed but learned from the data \cite{simard2003best}. Over the past few year they have provided state of the art results on many standard classification tasks, on images datasets such as \textit{MNIST} , \textit{CIFAR} \cite{hinton2012improving} and \textit{ImageNet} \cite{krizhevsky2012imagenet} as well as on speech processing problem such as \textit{TIMIT} \cite{abdel2012applying}. Those good results are used to advocate that those networks are learning "good" representations. However it seems that in certain cases they learn representation of the data that are, for example, not invariant to deformations \cite{szegedy2013intriguing}.\\
			         
	\section{Probabilistic graphical model:}
		??? - not sure yet
		
	\section{Outline of the report:}
    \label{sec:Intro/Outline of the report}    
    
    The remainder of this document is organized as follow. Section~\ref{chap:ST} summarizes and explains recent work St\'ephane Mallat and his group on the Scattering Transform (ST), a wavelet-based operator fulfilling all the properties of what we have defined as a ``good'' representation for signal classification. Section~\ref{chap:PGMs} introduces the concept of Probabilistic Graphical Models (PGMs) as generative models that can be used -among other tasks- for classification. Section~\ref{chap:SHMT} describes how the representation produced by the scattering transform can be modeled by an hidden Markov tree, using what we have named Scattering Hidden Markov Trees (SCHMTs). Finally Section~\ref{chap:Exp} provides some examples of applications.
    
	\section{Contribution:}
    \label{sec:Intro/Contrib}    
    
    The first two sections of this documents are mostly dedicated at introducing preexisting notions and concepts that are useful to later construct the Scattering Hidden Markov Trees. Hence readers already familiar with with Scattering Transforms or Probabilistic Graphical Models can respectively skip Section~\ref{chap:ST} or~\ref{chap:PGMs} and focus on the novelties introduced in Section~\ref{chap:SHMT}.

    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{The Scattering transform:}
  \label{chap:ST}
 
	This section describes the construction of a mathematical operator - the scattering transform (ST)- designed to generate what has been defined earlier as an interesting representation of signal (see Section~\ref{sec:Intro/Image rep}). This operator delocalizes signal informational content into scattering decomposition paths, computed by cascading wavelet/modulus operators through an architecture similar to a Convolutional Neural Network (CNN) where the synaptic weights would be given by a wavelet operator instead of learned.\\ 
	
	The remainder of the chapter is organized as follow. First, Section~\ref{sec:ST/SCN} defines the scattering operators. Second, Section~\ref{sec:ST/SCN} describes how those operators can be stacked to create a Scattering Convolutional Network (SCN), an architecture comparable to CNNs. Then Section~\ref{sec:ST/Pties} reviews some of the SCNs' important properties. Finally, Section~\ref{sec:ST/Applications to clf} presents how the scattering transform is usually used in classification tasks.
	
% 
% 	    
%     In this section we first introduce a wavelet-based scattering transform built to have interesting properties for classification tasks, meaning being translation invariant and stable to $\mathbf{\mcalL^{2}}$ deformations, while preserving the separability between classes. Then we explained how those operators can be stacked to create a ``deep'' scattering transform using a convolutional architecture.
      
		\section{Scattering wavelets:}
			\label{sec:ST/Scattering wavelets}

			A two-dimensional directional wavelet is obtained by scaling and rotating a single band-pass filter $\psi$. If one let $G$ be a discrete, finite rotation group of $\dsR^{2}$, multi-scale directional wavelet filters are defined for any scale $j \in \dsZ$ and rotation $r \in G$ by,
      
      \begin{equation}
				\label{eq:multi-scale directional wavelet}
				\psi_{2^{j}r}(u) = 2^{2j} \psi(2^{j}r^{-1}u).
      \end{equation}
      
      To simplify the notations, let now $\lambda = \lambda(j,r) \; \eqdef \; 2^{j}r \; \in \; G \times \dsZ$.\\
      
      A wavelet transform filters the signal $\bfx$ using a family of wavelets $\{\bfx \ast \psi_{\lambda}(u)\}_{\lambda}$ computed from a filter bank of dilated and rotated wavelets having no orthogonality property. This generates a multi-scales and multi-orientations representation of the input signal.\\ 
      
      If $u.u'$ and $\|u\|$ define respectively the inner product and the norm in $\dsR^{2}$, the Morlet wavelet $\psi$ is an example of wavelet given by,
      
      \begin{equation*} 
				\label{eq:Morlet wavelet}
				\psi(u) = C_{1}(e^{iu.\xi} - C_{2}) e^{\|u\|^{2}/(2 \sigma^{2})},
      \end{equation*}

      where $C_{1}$, $\xi$ and $\sigma$ are meta-parameters of the wavelet and $C_{2}$ is adjusted so that $\int \psi(u) du = 0$. Figure~\ref{fig:Morlet wavelet} shows a Morlet wavelet for  $\xi= 3\pi/4$ and $\sigma=0.85$.\\
      
      \begin{figure}
				\begin{center}
					\includegraphics[width=3.5in]{placeholder.jpg}
					\caption{Complex Morlet wavelet.}
					\label{fig:Morlet wavelet}
				\end{center}	
      \end{figure}
      
      As opposed to the Fourier sinusoidal waves, wavelets are operators stable to local $\mathbf{\mcalL^{2}}$ deformations as they can be expressed as localized waveforms \cite{mallat1999wavelet}. However, as wavelet transform computes a convolutions with a wavelet basis, the resulting transform is a translation covariant operator \cite{bruna2013invariant}.\\
        
      To ensure a translation invariant behavior for an operator initially commuting with translations, one has to introduce a non-linearity. Integration is an example of such a non-linearity. For example, let $R$ be a linear or non-linear operator commuting with translations $L_{c}$, \ie $R(L_{c}\bfx) = L_{c}R(\bfx)$), then the integral $\int R(\bfx(u))du$ is  translation invariant. Applying such a non-linearity to a wavelet transform is equivalent to setting $R(\bfx) = \bfx \ast \psi_{\lambda}$ and one gets the trivial invariant, 
      
      \begin{equation*}
				\label{eq:Trivial invariant}
				\int \bfx \ast \psi_{\lambda}(u)du = 0,
      \end{equation*}
      
      for all signal $\bfx$ as $\int \psi_{\lambda}(u)du = 0$.\\
      
      However for the scattering operator to preserve its informative character, one has to ensure a non-vanishing integral. To do so a second operator $M$ has to be introduced such that $M \circ R(\bfx) = M(\bfx \ast \psi_{\lambda})$. If $M$ was a linear  transformation commuting with translation then the integral still vanishes. Hence one has to choose $M$ among the non-linear operators.\\
      
      Keeping in mind that the scattering transform has to be stable to deformations and taking advantages of the wavelet transform stability to small deformations in the input space, one also imposes that $M$ commutes with deformations, 
      
      \begin{equation*}
				\label{eq:Commute with deformations}
				\forall \tau(u) \; , \; M L_{\tau} = L_{\tau} M.
      \end{equation*}
      
      If a weak differentiability condition is added, one can prove \cite{bruna2012commute} that $M$ must necessarily be a point-wise operator, \ie $M \circ R(\bfx(u)))$ only depends on the value of $\bfx(u)$.\\
      
      Finally, by adding an $\mathbf{\mcalL^{2}}(\dsR^{2})$ stability constraint,
      
      \begin{equation*}
				\label{eq:L2 stability}
				\forall (\bfx,\bfx') \in \mathbf{\mcalL^{2}}(\dsR^{2})^{2} \; , \; 
				\norm{M \circ R(\bfx)} = \norm{\bfx} 
				\; \text{and} \;
				\norm{M \circ R(\bfx) - M \circ R(\bfx')} \leq \norm{\bfx-\bfx'},
      \end{equation*}     
      
      one can also show \cite{bruna2012commute} that necessarily $M(R(\bfx))= e^{i\alpha}\abs{R(\bfx)}$. For the scattering transform, the simplest solution $\alpha = 0$ is chosen and therefore the resulting coefficients are the $\mathbf{\mcalL^{1}}(\dsR^{2})$ norms:
       
      \begin{equation*}
				\label{eq:L1 norm}
				\norm{\bfx \ast \psi_{\lambda}}_{1} = \int \abs{\bfx \ast \psi_{\lambda}} du
      \end{equation*}      
      
      The family of the $\mathbf{\mcalL^{1}}(\dsR^{2})$ normed wavelet $\{ \norm{\bfx \ast \psi_{\lambda}}_{1}\}_{\lambda}$ generates a crude signal representation which measures the sparsity of the wavelet coefficients. It can be proven that the signal $\bfx$ can be reconstructed from $\{\abs{\bfx \ast \psi_{\lambda}(u)} \}_{\lambda}$ up to a multiplicative constant \cite{waldspurger2015phase}. This means that the information loss in $\{\norm{\bfx \ast \psi_{\lambda}}_{1} \}_{\lambda}$ occurs during the integration of the absolute value $\abs{\bfx \ast \psi_{\lambda}(u)}$ which removes all non-zero frequencies. However those components can be recovered by calculating the wavelet coefficients $\abs{\bfx \ast \psi_{\lambda_{1}}} \ast \psi_{\lambda_{2}}(u)$ of the new signal $\abs{\bfx \ast \psi_{\lambda_{1}}}$. By doing so their $\mathbf{\mcalL^{1}}(\dsR^{2})$ norms define a much larger family of invariants:
      
      \begin{equation*}
				\label{eq:2nd order}
				\forall (\lambda_{1}, \lambda_{2}) \;\;\; 
				\norm{ \abs{\bfx \ast \psi_{\lambda_{1}}} \ast \lambda_{2}}_{1} =
				\int \abs{ \abs{\bfx \ast \psi_{\lambda_{1}}(u)} \ast \psi_{\lambda_{2}}} du
      \end{equation*}          
      
      By further iterating on the wavelet/modulus operators more translation invariant coefficients can be computed. The building bloc of such a model -the scattering propagator- is thus the absolute value of the convolution between a wavelet and the input signal.
      
      \begin{defn} \textbf{Scattering Propagator}\\ 
				The scattering operator $U$ for a scale and an orientation $\lambda \in G \times \dsZ$ is defined as the absolute value of the input convolved with the wavelet operator at this scale and orientation.
				
				\begin{equation}
					\label{eq:scattering propagator}
					U[\lambda](\bfx) \eqdef \abs{\bfx \ast \psi_{\lambda}}
				\end{equation}
				\label{def:SO}
			\end{defn}
						
			\begin{defn} \textbf{Path Ordered Scattering Propagators}\\ 
				Any sequence $p = (\lambda_{1}, \lambda_{2},\dots ,\lambda_{m})$ where $\forall i \in \llbracket 1,m \rrbracket \;\; \lambda_{i} \in G \times \dsZ$ defines a path of length $m$, \ie the ordered product of non-linear and non-commuting operators,
				
				\begin{equation}
					\label{eq:path ordered SP}
					\begin{split}
						U[p]\bfx &\eqdef U[\lambda_{m}]\dots U[\lambda_{2}]U[\lambda_{1}](\bfx) \\
							&= \abs{\abs{\abs{\abs{\bfx \ast \psi_{\lambda_{1}}} \ast \psi_{\lambda_{2}}} 
							\dots } \ast \psi_{\lambda_{m}}}.   
					\end{split}
				\end{equation}
				
				With the convention: $U[\emptyset]\bfx = \bfx$\\
				\label{def:path ordered SO}
      \end{defn} 
      
      From there one can provide a first formal definition of the scattering transform.
      
      \begin{defn} \textbf{Scattering Coefficient}\\
				A scattering coefficient along the path $p$ is defined as an integral of the $p$ ordered scattering propagators, normalized by the response of a Dirac:

				\begin{equation}
					\label{eq:ST1}
					\bar{S}[p](\bfx) \eqdef \mu_{p}^{-1} \int U[p]\bfx(u) du
				\end{equation}
				
				with,
				
				\begin{equation*}
					\label{eq:ST normalization}
					\mu_{p} \eqdef \int U[p]\delta(u)du      
				\end{equation*}
				\label{def:SC}
      \end{defn}
      
      We shall see later (see Section~\ref{sec:ST/Pties}) that each scattering coefficient $\bar{S}[p](\bfx)$ is -as desired- invariant to translation of the input $\bfx$ and Lipschitz continuous to deformations. But for classification tasks, one might only want to compute localized descriptors only invariant to translations smaller than a predefined scale $2^{J}$, while keeping the spatial variability at 
      scales larger than $2^{J}$. This can be achieved by localizing the scattering integral with a scaled spatial window $\phi_{2^{J}}(u) = 2^{-2J} \phi(2^{-2J}u)$. And this yields to the definition of the windowed scattering transform.
      
      \begin{defn} \textbf{-Windowed- Scattering Coefficient Of Order $m$}\\
				If $p$ is a path of length $m \in \mathds{N}$, the -windowed- scattering coefficient of order $m$ at scale $2^{J}$ ($J \in \dsN$) is defined as:
				
				\begin{equation}
					\label{eq:ST windowed}
					\begin{split}
						S_{J}[p](\bfx) &\eqdef U[p]\bfx \ast \phi_{2^{J}}(u) \\
									&= \int U[p]\bfx(v) \phi_{2^{J}}(u-v) dv \\
									&= \abs{\abs{\abs{\abs{\bfx \ast \psi_{\lambda_{1}}} \ast \psi_{\lambda_{2}}} 
							\dots } \ast \psi_{\lambda_{m}}} \ast \phi_{2^{J}}(u)
					\end{split}
				\end{equation}
				
				With the convention: $S_{J}[\emptyset]\bfx = \bfx \ast \phi_{2^{J}}$\\
      	\label{def:SC windozed}
      \end{defn}

		\section{Scattering Convolution Network:}
			\label{sec:ST/SCN}
			
		
			This section introduces the scattering transform as an iterative process over a one-step operator and creates a parallel with convolutional neural networks \cite{lecun2010convolutional}. For $J \in \dsN$ let $U_{J}[\Omega] \eqdef \{U_{J}[p]\}_{p \in \Omega}$ and $S_{J}[\Omega] \eqdef \{S_{J}[p]\}_{p \in \Omega}$. They defines families of operators indexed by a path set $\Omega$.\\
			
			One can compute a windowed scattering transform by iterating over the one-step -windowed- propagator $U_{J}$.
			
			\begin{defn}  \textbf{One-step propagator}\\
				The one-step propagator $U_{J}$ can be defined as,
				\begin{equation}
					\forall \bfx \in \mcalL^{2}(\dsR^{d}) \;\;\; U_{J} \bfx = \{ A_{J}\bfx , \; (U[\lambda]\bfx)_{\lambda \in \Lambda_{J}} \},
					\label{eq:One step propagator}
				\end{equation}
				
				where $A_{J}\bfx = \bfx \ast \phi_{2J}$, $U[\lambda]\bfx = \abs{\bfx \ast \psi_{\lambda}}$ and $\Lambda_{J} = G \times \llbracket0, J \rrbracket$.\\
				\label{def:One step propagator}
			\end{defn}
			
			Indeed after calculating $U_{J}\bfx$, applying $U_{J}$ again to each $U[\lambda]\bf$ generates a larger infinite family of functions. And since $U[\lambda]U[p] = U[p+\lambda]$ and $ A_{J}U[p] = S_{J}[p]$ it holds that,
			
			\begin{equation}
					\forall \bfx \in \mcalL^{2}(\dsR^{d}) \;\;\; U_{J}U[p] \bfx = \{ S_{J}[p]\bfx , \; (U[p+\lambda]\bfx)_{\lambda \in \Lambda_{J}} \},
					\label{eq:SCN propagation 1}
			\end{equation}\\
			
			Let now $\Lambda_{J}^{m}$ be a set of path of length $m$ with the convention $\Lambda_{J}^{0} = \{ \emptyset \}$, its propagation is,
			
			\begin{equation}
					\forall \bfx \in \mcalL^{2}(\dsR^{d}) \;\;\; U_{J}U[\Lambda_{J}^{m}] \bfx = \{ S_{J}[\Lambda_{J}^{m}]\bfx , \; (U[\Lambda_{J}^{m+1}]\bfx)_{\lambda \in \Lambda_{J}} \}.
					\label{eq:SCN propagation 2}
			\end{equation}\\
			
			\begin{defn}  \textbf{Scattering transform}\\			  
			  The scattering transform of order $m$ at scale $J$ can finally be define as,
			  \begin{equation}
			    S_{J}[\mcalP_{J}^{m}]\bfx = [\{S_{J}[\Lambda_{J}^{0}]\bfx\}_{\lambda \in \Lambda_{J}} \dots \{S_{J}[\Lambda_{J}^{m}]\bfx\}_{\lambda \in \Lambda_{J}}].\\
					\label{eq:scattering transform}
			  \end{equation}\\ 
			\end{defn}
			
			Hence the scattering transform of infinite depth $S_{J}[\mcalP_{J}]\bfx$ can be computed from $\bfx= U[\emptyset]\bfx$ by iteratively computing $U_{J}U[\Lambda_{J}^{m}] \bfx$ for $m$ going from $0$ to $\infty$. This iterative process is illustrated in Figure~\ref{fig:SCN} and one can notice that the scattering calculation as the same general architecture as the convolutional neural networks introduced by \cite{lecun1995convolutional}. Both CNN and scattering convolutional network (SCN) cascade convolutions and a ``pooling'' non linearity. However while convolutional neural networks use kernel filters learned from the data with back-propagation algorithm, SCNs use a fixed wavelet filter bank. 
			%TODO: more on the topic from the roto translation paper

      \begin{figure}
				\begin{center}
					\includegraphics[width=3.5in]{placeholder.jpg}
					\caption[The scattering convolutional network architecture]{see mallat long paper p1341}
					\label{fig:SCN}
				\end{center}	
      \end{figure}
      
      
      
	\section{Properties of the scattering transform:}
		\label{sec:ST/Pties}
		
		Now that the scattering coefficient have be defined one can be interested in the characteristics of such a data representation. This section provides an overview of some of the properties of the scattering transform. It also describes an approximation to the scattering convolution network defined in the previous section, leading to computationally tractable networks. 

		\subsection{Non-expansivity:}
			\label{subsec:ST/Pties/Non-expansivity}
			%TODO-DSTL: no proof.
			%TODO-UCL: add proof of the property (in line as it is short)
			
			The scattering propagator $U_{J} \bfx = \{ A_{J}\bfx , \; (\abs{W_{J}\bfx})_{\lambda \in \Lambda_{J}} \}$ results of the composition of a wavelet transform $W_{J}$ that is unitary and of a modulus operator that is non-expansive - as $\forall (a,b) \in \dsC^{2} \abs{\abs{a}-\abs{b}} \leq \abs{a-b}$- and is thus non-expansive. Since $S_{J}[\mcalP_{J}]$ iterates on $U_{J}$, which is non-expansive, the proposition (proved by \cite{lohmiller1998contraction}) proves that $S_{J}[\mcalP_{J}]$ is also non-expansive.
			
			\begin{prop} \textbf{Non-expansive}\\ 
				The scattering transform is non expansive.
				
				\begin{equation}
				  \forall (\bfx,\bfx') \in \mcalL^{2}(\dsR^{d})^{2} \;\;\; \norm{S_{J}[\mcalP_{J}]\bfx - S_{J}[\mcalP_{J}]\bfx'} \leq \norm{\bfx-\bfx'}
				\end{equation}

			  \label{pty:Nonexpansivity}
			\end{prop}

		\subsection{Energy preservation:}
			\label{subsec:ST/Pties/Energy}
			%TODO-DSTL: no proof.
			%TODO-UCL: add (proof for theorem ref available in annexe ref \textbf{\textit{proof in annexe}})
			
			Each propagator $U[\lambda]\bfx = \abs{\bfx \ast \psi_{\lambda}}$ captures the frequency energy contained in the signal $\bfx$ over a frequency band covered by the Fourier transform $\hat{\psi}_{\lambda}$ and propagates this energy towards lower frequencies. It can thus be proved that under some assumption on the wavelet -admissible wavelet, the whole scattering energy ultimately reaches the minimum frequency $2^{-J}$ and is trapped by the low-pass filter $\phi_{2^{J}}$. Thus the energy propagated by a -windowed- scattering transform goes to $0$ as the path length increases, implying that $\norm{S_{J}[\mcalP_{J}]} = \norm{x}$\\
			
			But prior to showing this, one must states the necessary assumptions to be made on the wavelet used.
			
			\begin{note}
			  Formal proofs of most of those properties can be found in \cite{mallat2012gis}.
			\end{note}
			
			\begin{defn} \textbf{Admissible scattering wavelet}\\ 
				A scattering wavelet $\psi$ is admissible if there exist $\eta \in \dsR^{d}$ and $\rho \geq 0$, with $\abs{\hat{\rho}(\omega)} \leq \abs{\hat{\phi}(2\omega)}$ and $\hat{\rho}(\omega)=0$, such that the function,
				
				\begin{equation}
					\hat{\Psi}(\omega) = \abs{\hat{\rho}(\omega - \eta)}^{2} - \sum_{k=1}^{+\infty}k(1-\abs{\hat{\rho}(2^{-k}(\omega - \eta)}^{2}),
				\end{equation}
				
				satisfies,
				
				\begin{equation}
				  \alpha = \inf_{1 \leq \abs{\omega}\leq 2} \sum_{j=-\infty}^{+\infty}\sum_{r \in G} \hat{\Psi}(2^{-j}r^{-1}\omega) \abs{\hat{\psi}(2^{-j}r^{-1}\omega)}^{2} > 0.\\
				\end{equation}
				
				\label{def:Admissible wavelet}
			\end{defn}

			For an admissible wavelet one can prove that the scattering transform conserves the energy of the signal.
			
			\begin{thm} \textbf{Energy conservation}\\ 
				If the scattering wavelet $\psi$ is admissible, then for all signal $\bfx \in \mcalL^{2}(\dsR^{d})$, %TODO: define \Lambda_{J}^{m}
				
				\begin{equation}
				  \lim_{m \rightarrow +\infty} \norm{U[\Lambda_{J}^{m}]\bfx}^{2} = \lim_{m \rightarrow +\infty} \sum_{n=m}^{+\infty} \norm{S_{J}[\Lambda_{J}^{n}]\bfx}^{2} = 0,
					\label{eq:energy conservation 1}
				\end{equation}
				
				and
				
				\begin{equation}
				  \norm{S_{J}[P_{J}]\bfx}^{2} = \norm{\bfx}.
				  \label{eq:energy conservation 2}
				\end{equation}
				\label{thm:Energy conservation}
			\end{thm}

			The proof of the Theorem~\ref{thm:Energy conservation} also shows that the scattering energy propagates progressively towards lower frequencies and that the energy of $U[p]\bfx$ is mainly concentrated along frequency decreasing paths $p=(\lambda_{k})_{k\leq m}$, \ie for which $\abs{\lambda_{k+1}} \leq \abs{\lambda_{k}}$. The energy contained in the other paths is negligible and thus for the rest of the document only frequency decreasing paths are be considered.\\
			
			Moreover, the decay of $\sum_{n=m}^{+\infty} \norm{S_{J}[\Lambda_{J}^{n}]x}^{2}$ implies that there exist a path length $m > 0$ after which all longer path can be neglected. For signal processing applications, this decay appears to be exponential. And for classification applications path of length $m = 3$ provides the most interesting results \cite{anden2011multiscale}, \cite{bruna2010classification}.\\
			
			The two restrictions stated above yield to an easier parametrization of a scattering network. Indeed when only the frequency decreasing paths up until a given order a scattering network is completely defined by:
			\begin{itemize}
				\item $\psi$: The admissible wavelet used. Unless stated otherwise the Morlet wavelet is used.
			  \item $M$: The maximum path length considered.
			  \item $J$: The coarsest scale level considered.
			  \item $L$: The number of orientation considered, which can be define as the cardinality of the previously define $G$.
			\end{itemize}
			
			Hence for a given set of parameter $(\psi, M,J,L)$, one can generate one and only one scattering network -with frequency decreasing paths. Each node $i$ of this network generates a -possibly empty- set of of nodes of size $(j_{i}-1) \times |G|$ where $j_{i}$ is the scale of node $i$ and $|G|$ is the number of orientations considered. Finally the number of nodes $O$ of this network can be expressed as,
			
			\begin{equation}
			  O = \sum_{m=0}^{M-1} \binom{J}{m} . |G| ^{m}
			  \label{eq:ST number of node}
			\end{equation}

			and it has the architecture displayed by Figure~\ref{fig:SCN 2}. 
			
			\begin{figure}
				\begin{center}
					\includegraphics[width=3.5in]{placeholder.jpg}
					\caption{Frequency decreasing scattering convolution network}
					\label{fig:SCN 2}
				\end{center}	
      \end{figure}
      
		\subsection{Translation invariance:}
			\label{subsec:ST/Pties/Translation}
			%TODO-DSTL: no proof.
			%TODO-UCL: no proof neither -maybe an intuition.
			
			The translation invariance of the scattering transform $S_{J}[\mcalP_{J}]$ can be proved for a limit metric when $J$ goes to infinity. To do so one can first prove that the scattering distance $\norm{S_{J}[\mcalP_{J}]\bfx - S_{J}[\mcalP_{J}]\bfx'}$ converges when $J$ goes to infinity - as it is non-increasing when $J$ increases (see section~\ref{subsec:ST/Pties/Non-expansivity}). From there one can bound the distance between the scattering transform of the signal and the one of its translated version $\norm{S_{J}[ S_{J}[\mcalP_{J}]\mcalL_{c}\bfx - \mcalP_{J}]\bfx}$ and prove that this bound tends to $0$ when $J$ goes to infinity. This proves the translation invariance.
			
			\begin{thm} \textbf{Translation invariance}\\
				For admissible scattering wavelets,
				
				\begin{equation}
					\forall \bfx \in \mcalL^{2}(\dsR^{d}), \; \forall c \in \dsR^{d} \;\;\; \lim_{J \rightarrow \infty} \norm{S_{J}[\mcalP_{J}]\mcalL_{c}\bfx - S_{J}[\mcalP_{J}]\bfx} = 0
					\label{eq:Tranlation invariance}
				\end{equation}
				\label{thm:Translation invariance}
			  
			\end{thm}

		\subsection{Lipschitz continuity to the action of diffeomorphisms:}
			\label{subsec:ST/Pties/Lipschitz continuity}
			%TODO-DSTL: no proof.
			%TODO-UCL: no proof neither -maybe an intuition.
			%TODO: change for the version from ISCN
			
			The Lipschitz continuity to the action of diffeomorphisms of $\dsR^{d}$ can be proved for deformations sufficiently close to a translation. Such diffeomorphisms map $u$ to $u-\tau(u)$ where $\tau(u)$ is a displacement filed such that $\norm{\nabla \tau}_{\infty} < 1$ -\ie invertible transformations \cite{bruna2013invariant}. Let $L_{\tau}\bfx(u)=\bfx(u-\tau(u))$ denotes the action of such diffeomorphisms on the signal $\bfx$. Once again one can find an upper bound to the distance between the scattering transform of the signal and the one of its deformed version $\norm{S_{J}[\mcalP_{J}]\mcalL_{\tau}\bfx - S_{J}[\mcalP_{J}]\bfx}$. With a bit of work on this bound one can then proved that the consequences of the action of $L_{\tau}$ is bounded by a translation term proportional to $2^{-J} \norm{\tau}_{\infty}$ and a deformation error proportional to $\norm{\nabla \tau}_{\infty}$. Finally some more work on the bounding term provides the Lipschitz continuity.
			
			\begin{thm} \textbf{Lipschitz continuity to the action of diffeomorphisms} \\
			  There exists $C$ such that all $\bfx \in \mcalL(\dsR^{d})$ with $\norm{U[\mcalP_{J}]\bfx}_{1} < \infty$ and all $\tau \in \mcalC^{2}(\dsR^{d})$ with $\norm{\nabla \tau}_{\infty} < \frac{1}{2}$ satisfy,
			  
			  \begin{equation}
					\norm{S_{J}[\mcalP_{J}]\mcalL_{\tau}\bfx - S_{J}[\mcalP_{J}]\bfx + \tau . \nabla S_{J}[\mcalP_{J}]\bfx} \leq C \norm{U[\mcalP_{J}]\bfx}_{1} K(\tau),
					\label{eq:Lipschitz continuity 1}
			  \end{equation}

			  with
			  
			  \begin{equation}
					K(\tau) = 2^{-2J} \norm{\tau}_{\infty}^{2} + \norm{\nabla \tau}_{\infty} \left(\max \left( \log \frac{\norm{\Delta \tau}_{\infty}}{\norm{\nabla \tau}_{\infty}},1 \right) \right) + \norm{H\tau}_{\infty} 
					\label{eq:Lipschitz continuity K(tau)}
			  \end{equation}
			  
			  \label{thm:Lipschitz continuity}
			\end{thm}
			
			\begin{rem}
				If the case where $2^{J} \gg \norm{\tau}_{\infty}$ and $\norm{\nabla \tau}_{\infty} + \norm{H \tau}_{\infty} \ll 1$, then $K(\tau)$ becomes negligible and the displacement field $\tau(u)$ can be estimated at each $u \in \dsR^{d}$. This can be done by solving the linear equation resulting from~\ref{eq:Lipschitz continuity 1} under the assumptions mentioned above,
				
				\begin{equation}
					\forall p \in \mcalP_{J} \norm{S_{J}[p]\mcalL_{\tau}\bfx - S_{J}[p]\bfx + \tau . \nabla S_{J}[p]\bfx} \approx 0.
					\label{eq:Lipschitz continuity 2}  
				\end{equation}
				
				This estimate of the displacement field can be used for many applications such as object tracking in video sequences or image sequence restoration \cite{brailean1996recursive}.
			\end{rem}
			
		
  \section{Application to classification:}
    \label{sec:ST/Applications to clf}
    %TODO: Examples of Mallat's work.
    
    The scattering transform has been designed in order to provide a ``good'' representation for signal classification. The classical way of using it for such a task is to use the features generated by the scattering transform of the dataset considered as inputs for a discriminative classifier (e.g. Support Vector Machine classifier). Using this model provides results able to compare with state of the art CNNs on some standard datasets \textbf{\textit{cite mallat}}. \\

    
    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Probabilistic graphical models:}
  \label{chap:PGMs}
  
  Those models use a \Important{graph based representation of conditional dependence between a set of random variables} and thus encode a complete distribution over a multi-dimensional space in a compact -or factorized- graph.The field of PGMs can be split into two main families, the \Important{Bayesian Networks} (BNs) and the \Important{Markov models} (MMs). As they are graphical models, both families encompass the properties of factorization and independences defined by the graph, but differ when it comes to the specificities of the set of independences they can encode as well as the factorization of the distribution that they can induce (  \textbf{\textit{cite: Bishop, Christopher M. (2006). "Chapter 8. Graphical Models" (PDF). Pattern Recognition and Machine Learning. Springer. pp. 359–422. ISBN 0-387-31073-8. MR 2247587}}).\\
  
  Add par on how they will be used.\\ %TODO: how they will be used
  
  The aim of this section is not to provide a complete overview of the Probabilistic graphical models but rather to introduce some interesting concepts that have been used in our work. Someone with more interest in PGMs could refer to \textbf{\textit{cite A tutorial on learning BN}} or \textbf{\textit{cite cours dafney koller}}. 
  This chapter introduces those two main classes of probabilistic graphical models. Section~\ref{sec:PGMs/BN} focuses on the Bayesian networks, while section~\ref{sec:PGMs/MM} provides more details about the Markov models.
	
  \section{Bayesian Network:}  
    \label{sec:PGMs/BN}
    A BN is subclass of probabilistic graphical model where the set of random variables and their conditional dependencies are expressed via a \Important{directed acyclic graph} (DAG). The architecture of the Bayesian Networks is further explained in section~\ref{subsec:PGMs/BN/Architecture}. Then section~\ref{subsec:PGMs/BN/Learning} presents a brief overview of the state of the art in terms of learning algorithm for BNs and section~\ref{subsec:PGMs/BN/Inference} describes the inference mechanism for those networks.
        
    \subsection{Architecture:}
      \label{subsec:PGMs/BN/Architecture}
      
      Bayesian networks can be defined as,
      
      \begin{defn} \textbf{Bayesian Network}\\
				For a set of random variables $\bfX = \{X_{i}\}_{i \in \llbracket1,N \rrbracket}$ a Bayesian network consists of a \Important{direct acyclic graph} $\mcalG$ encoding a set of conditional independence assertions about the random variables in $\bfX$ and  a set $P$ of \Important{local probability distribution associated with each variable}. Each node of $\mcalG$ represent one of the random variable $X_{i}$ and each edge $E_{i \rightarrow j}$ represents the conditional dependence between the nodes $X_{i}$ and $X_{j}$.
				\label{def:BN}
      \end{defn}
     
			\begin{figure}
				\begin{center}
					\includegraphics[width=3.5in]{placeholder.jpg}
					\caption{Example of Bayesian network.} %TODO: graph from http://www.cse.unsw.edu.au/~cs9417ml/Bayes/Pages/Bayesian_Networks_Definition.html
					\label{fig:Eg BN}
				\end{center}
			\end{figure}

			For such networks the following property holds,
			
			\begin{prop} \textbf{Conditional independence for Bayesian networks}\\ %TODO: quote [Nilsson, 1998] [Nilsson, 1998] Nilsson N,, Artificial Intelligence: a new synthesis, Morgan Kaufmann,1998 
				In a Bayesian network each node of the graph are "conditionally independent of any subset of the nodes that are not descendants of itself given its parent".
				
				\begin{equation}
				  P(\{X_{i}\}_{i \in \llbracket1,N \rrbracket}) = \prod_{i=1}^{N} P(X_{i} | \rho(X_{i}))
				  \label{eq:BN CI}
				\end{equation}
				
				where $\rho(X_{i})$ are the parents of the node $X_{i}$. 
				\label{pty:CI for BNs}
			\end{prop}
			
			Thus in a Bayesian network a node with no parents is not conditioned on the other random variables. Such a node is called a \Important{prior probability}.\\

			By their architecture, Bayesian networks allow to simplify the computation of the joint probability distribution. For example for the network defined by figure~\ref{fig:Eg BN}, one can obtain $P(X_{1},X_{2},X_{3},X_{4},X_{5},X_{6})$ by the use of chain rules and theory on conditional independent,
			
			\begin{equation*}
			  \begin{split}
			    P(X_{1},X_{2},X_{3},X_{4},X_{5},X_{6})
						&= P(X_{6} | X_{3},X_{4},X_{5}) P(X_{1},X_{2},X_{3},X_{4},X_{5}) \\
						&= P(X_{6} | X_{3},X_{4},X_{5}) P(X_{3} | X_{1},X_{5}) P(X_{4}| X_{2}) P(X_{5}| X_{2}) P(X_{1},X_{2})\\
						&= P(X_{6} | X_{3},X_{4},X_{5}) P(X_{3} | X_{1},X_{5}) P(X_{4}| X_{2}) P(X_{5}| X_{2}) P(X_{2}|X_{1}) P(X_{1}).\\
			  \end{split}
			  \label{eq:Eg BN CI}
			\end{equation*}
      
%       are directed acyclic graphs whose nodes represent random variables in the Bayesian sense. Those nodes can be observable quantities, latent variables, unknown parameters or hypotheses. Edges of the graph represent conditional dependencies between the variables.  nodes that are not connected represent variables that are conditionally independent of each other. Each node is associated with a probability function that takes, as input, a particular set of values for the node's parent variables, and gives (as output) the probability (or probability distribution, if applicable) of the variable represented by the node. For example, if m parent nodes represent m Boolean variables then the probability function could be represented by a table of 2^m entries, one entry for each of the 2^m possible combinations of its parents being true or false

    \subsection{Learning:}
      \label{subsec:PGMs/BN/Learning}
      
      In some application the structure $\mcalG$ and the set $P$ of local probability distribution associated with each variable of the network are provided. In such a case the BNs are ``simply'' used for inference. The task is then to infer the most probable values for a subset $\bfY \subset \bfX$, given a partially complete set of realizations $\bfX_{obs} \subset \bfX \backslash \bfY$.\\
      
      However, most of the time the full characterization of the BN is not provided. Ignoring the case of missing data, one can split the learning problem into two main categories:
      \begin{itemize}
				\item \Important{Local probability distributions:} In this case the structure of the graph $\mcalG$ is known and fixed before hand. It can be provided by an expert (e.g. Microsoft trouble shooting system \textbf{\textit{cite}}) or be imposed by some construction rules (e.g. Boltzmann Machine \textbf{\textit{cite}}, Restricted Boltzmann Machine \textbf{\textit{cite}} ...). The task at end is then to learn the parameters governing the local probability distributions of the Bayesian network.
        \item \Important{Architecture and local probability distributions:} In this case the architecture of the network has to be learned along side with the local probability distributions. This problem is not developed in the rest of this paper, but one could refer to \textbf{\textit{cite A tutorial on learning BN}} for an introduction to this problem.
      \end{itemize}
      
      \subsubsection{Expectation-maximization:}
      
      \subsubsection{Variational Bayes:}

    \subsection{Inference:}
      \label{subsec:PGMs/BN/Inference}
      \url{http://www.cse.unsw.edu.au/~cs9417ml/Bayes/Pages/Bayesian_Networks_Inference.html}
      
  \section{Markov Models:}
    \label{sec:PGMs/MM}
    Quick overview over the main methods for HMMs.
    
    \subsection{Architecture:}
      \label{subsec:PGMs/MM/Architecture}
      TBD

    \subsection{Learning:}
      \label{subsec:PGMs/MM/Learning}
      TBD

    \subsection{Inference:}
      \label{subsec:PGMs/MM/Inference}
      TBD
      
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \chapter{Hidden Markov trees:}
%   \label{chap:HMT}
%     More at length description of the specific properties of HMTs.
% 	
%   \section{The tree structure:}
%     \label{sec:HMT/Tree:}
%     TBD
% 
%   \section{Learning:}
%     \label{sec:HMT/EM}
% 
%     \subsection{Expectation maximization:}
%       \label{subsec:HMT/EM}
%       EM by Crouse and EM by Durand
%       
%     \subsection{Variational methods:}
%       \label{subsec:HMT/Var}
%       Variational like Crouse and variational like Durand    
%   
%     
%   \section{Generation: Vitterbi algorithm:}
%     \label{sec:HMT/Generation}

		
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Scattering hidden Markov tree:}
  \label{chap:SHMT}
  Section~\ref{sec:ST/Applications to clf} introduced the usage of scattering networks combined with an support vector machine classifier to achieve state of the art classification performances on some problems. However this method provides a boolean answer for each class. Some methods to express the output of an SVM as a probability exists \cite{platt1999probabilistic} but they are just a rescaling of the output and not a true probabilistic approach. If one is interested in a true probabilistic model to describe the scattering coefficient it is quite natural to try to express them as a probabilistic graphical model. Indeed if one hide the propagation step from the scattering transform (see Section~\ref{sec:ST/SCN}) the scattering network defines a tree structure. \\

	\begin{figure}
			\begin{center}
				\includegraphics[width=3.5in]{placeholder.jpg}
				\caption{Scattering transform tree.} %TODO: SCHMT without hidden state/node
				\label{fig:ST tree}
			\end{center}
	\end{figure}
  
  To simplify the notations in the remainder of this section, let $\mcalT$ denotes the tree  structure defines as stated above and depicted in figure~\ref{fig:ST tree} created by a  scattering transform restricted to frequency decreasing path of length shorter than $M$.  Let $I$ denotes the total number of nodes -\ie scattering coefficients- and let $S_{i}$ for $i \in \llbracket0, I-1 \rrbracket$ denotes one of the node of $\mcalT$ for a given path $p_{i} = [\lambda_{0} ... \lambda_{u}]$ ($u\in \dsN$). Note that $S_{i}$ represents a node and does not depends on the signal $\bfx$. For a given signal $\bfx$, a realization of the node $i$ is denoted by $S_{i}= s_{i} = S[p_{i}]\bfx$. Note also that in the remainder of the paper the short notation $i \in \mcalT$ will be used to denote $p_{i}$. Let also use the convention $S_{0} = S[\emptyset](.)$. Finally let $\rho_{i}$ and $\mcalC(i)$ denote respectively the parent of a node $i$ and the set of children of the node $i$. Note also that a node $S_{i}$ can have no children, in such a case this node is a leaf of the tree.\\
  
  The remainder of this section is organized as follows: Section~\ref{sec:SHMT/Rel work} introduces related work and provides a description of the SCHMT model. Section~\ref{sec:SHMT/Hypos} details the hypothesis needed to develop this model as well as provides some intuition on their validity. Finally Section~\ref{sec:SHMT/Learning} and~\ref{sec:SHMT/Clf} respectively describe the learning algorithm for the parameters of the model and the classification method.

  
  \section{SCHMT model and related works:}
		%TODO1: add to the scattering transform section a simple notation for the scattering coef
		%TODO2: modify this section accordingly
    \label{sec:SHMT/Rel work}
      
    The idea behind the SCHMT model is to say that the more detailed representation of the signal is somehow correlated to the less detailed one from which it is generated. More formally this means that for a signal $\bfx$, $s_{i}$ is somehow correlated to $s_{\rho(i)}$. To do so one could model the scattering network by a Markov tree and assumes the following:
    
    \begin{equation}
      P(S_{i} |\mcalT) = P(S_{i} | S_{\rho(i)}).
      \label{eq:SMT - false hypo}
    \end{equation}\\

    Those independence properties yield to the graph displays in Figure~\ref{fig:ST tree}. However models trying to describe directly the correlation across coefficients at different scales have been studied for traditional wavelet transforms \cite{lee1996new} but they are in conflict with the compression property of the wavelet -\ie most wavelet representations are sparse \cite{crouse1998wavelet}. Thus it seems that a simple one-step Markovian assumption across scale is not sufficient to describe the complex relationship between wavelet coefficients.\\
    
    A common approach when a direct Markovian model does not hold is to introduce hidden states and to assume the Markovian property across those states. The observed nodes are then only dependent on their state. This is the architecture adopted for the SCHMT and its graph is represented in Figure~\ref{fig:SCHMT 1}. This model has the following independence properties,
    
    \begin{equation}
      P(H_{i} |\mcalT) = P(H_{i} | H_{\rho(i)}),
      \label{eq:SCHMT - hypo 1}
    \end{equation}    
    \begin{equation}
      P(S_{i} |\mcalT) = P(S_{i} | H_{i}).
      \label{eq:SCHMT - hypo 2}
    \end{equation} \\
    
		\begin{figure}
				\begin{center}
					\includegraphics[width=3.5in]{placeholder.jpg}
					\caption{Scattering Hidden Markov Tree.} %TODO: SCHMT
					\label{fig:SCHMT 1}
				\end{center}
		\end{figure}
		
		As the scattering transform is closely related to wavelet transforms it is not surprising to find similar ideas exploited for wavelet trees. MS. Crouse has developed a model where an Hidden Markov Tree Model is used to model the wavelet coefficients \cite{crouse1998wavelet} of a classic wavelet trees. Later N. Kingsbury \cite{kingsbury2001complex} has  adapted MS. Crouse's model to his Dual Wavelet Complex Trees. The resulting hidden Markov tree models provides better classification performances than MS. Course's WHMT as the wavelet used generate a ``better'' representation of the signal -in the sense defined in Section~\ref{sec:Intro/Image rep/Intuition}. Indeed this version can leverage the quasi-translation invariance property of the complex wavelets. The improvement in performances due to the quasi-invariance property provides a good motivation to try the hidden Markov tree modeling on the scattering transform as they have even ``better'' representational properties (see Section~\ref{sec:ST/Pties}). The parameters of the original WHMT were trained using a version of the Expectation-Maximization adapted to binary hidden Markov trees. However this learning method suffered from underflowing issues \cite{devijver1985baum}, and JB. Durand \cite{durand2004computational} has proposed a smoothed version of the training algorithm preventing it from happening.\\
   
		\begin{figure}
				\begin{center}
					\includegraphics[width=3.5in]{placeholder.jpg}
					\caption{Wavelet hidden Markov tree.} %TODO: course's model
					\label{fig:WHMT}
				\end{center}
		\end{figure}
	  
	  The model selected yield to a Scattering Hidden Markov Tree composed of a set of visible nodes $\{\bfS_{i}\}_{i \in \mcalT}$ and a set of hidden node $\{\bfH_{i}\}_{i \in \mcalT}$. Both sets are organized in a tree structure with the following characteristics,\\
	  
    \begin{itemize}
      \item For any index $i$ of the tree, $S_{i} \in \dsR^{d}$ and $H_{i} \in \llbracket 1,K \rrbracket$ where $K$ is the number of possible hidden states.
      
      \item The initial hidden state is drawn from a discrete non uniform initial distribution $\pi_{0}$ such that:
	  	  \begin{equation}
					\forall k \in \llbracket 1,K \rrbracket \;\;\; \pi_{0}(k) = P(H_{0}=k).
					\label{eq:SCHMT - initial distribution}
				\end{equation}
				
      \item For any index $i$ of the tree, the emission distribution describe the probability of the visible node $S_{i}$ conditional to the hidden state $H_{i}$,
				\begin{equation}
				  \forall i \in \mcalT \;\; \forall k \in \llbracket1,K\rrbracket \;\; \text{and} \;\; \forall s \in \dsR
					\;\;\;\;\;\;P(S_{i}=s|H_{i}=k) = P_{\theta_{k,i}}(s)
					\label{eq:SCHMT - Emission distr}
				\end{equation}
				Where $P_{\theta}$ belongs to a parametric distribution family and where $\theta$ is the vector of emission parameters. In the remainder of the paper the emission distribution is a Gaussian so that,  
      	\begin{equation}
				  \forall i \in \mcalT \;\; \forall k \in \llbracket1,K\rrbracket \;\; \text{and} \;\; \forall s \in \dsR
					\;\;\;\;\;\; P(S_{i}=s | H_{i}=k) = \mcalN(\mu_{k,i},\sigma_{k,i}).
				  \label{eq:SCHMT - Mixt of Gaussian}
				\end{equation}
				where $\theta_{k,i}=(\mu_{k,i},\sigma_{k,i})$ with $\mu_{k,i}$ and $\sigma_{k,i}$ being respectively the mean and the variance of the Gaussian for the $k$-th value of the mixture and the node $i$.
				
			\item For any index $i$ of the tree, the probability to be in a state given the father's state is characterized by a transition probability,
				\begin{equation}
					\forall i \in \mcalT \backslash \{ 0 \} \;\;\; \forall g,k \in \llbracket1,K\rrbracket^{2}
					\;\;\;\;\;\; \epsilon_{i}^{(gk)} = P(H_{i}= k | H_{\rho(i)}=g), 
					\label{eq:SCHMT - Transition matrix}
				\end{equation}
				where $\epsilon_{i}$ defines a transition probability matrix such that,
				\begin{equation}
				  \forall i \in \mcalT \backslash\{0\} \;\;\;  \forall k \in \llbracket1,K\rrbracket
					\;\;\;\;\;\;  P(H_{i}=k) = \sum_{g=1}^{K} \epsilon_{i}^{(gk)} P(H_{\rho(i)}=g). %TODO: Check if it is conditional or not.
				  \label{eq:SCHMT - State proba 1}
				\end{equation}
				Note that using the chain rule of probability one can express $P(H_{i})$ from the root node's initial distribution.\\
		\end{itemize}
		
		Thus for a given scattering architecture -\ie fixed $M$ and $J$- the SCHMT model is thus fully parametrized by,
		\begin{equation}
			\Theta = \big(\pi_{0}, \{ \epsilon_{i}, \{ \theta_{k,i} \}_{k\in\llbracket1,K\rrbracket} \}_{i\in\mcalT}\big).
			\label{eq:SCHMT - parameters}
		\end{equation}
		
		
    The SCHMT model differs from the previous works by the shape of its tree structure. Previous works are based on regular binary tree structures where all the leafs have the same depth. The scattering tree is a irregular tree. Indeed as seen in Section~\ref{sec:ST/SCN} each node has a variable number of children, which yields to an architecture where the number of descendant is not constant and where leafs can be found at any depth of the tree. However as detailed in section~\ref{sec:SHMT/Learning}, the learning algorithms can be adapted to this case. Another difference between SCHMT and the previous works is the non-homogeneity of the transition matrix. Indeed by the nature of the scattering transform one can expect a non homogeneous transmission of the information across the orders and thus to reflect this the learning algorithm has also been adapted to this case.\\
    
    Even though the theoretical framework of SCHMT holds for any $K \in \dsN^{\ast}$, in the remainder of the work $K$ is set to $2$. This means that the scattering coefficient are described by a mixture of two Gaussians and can be in two states, either $(H)$ High or $(L)$ Low. This model yields to sparser representations as the number of hidden states is highly constrained.
    
    
  \section{Hypothesis:}
    \label{sec:SHMT/Hypos}
    
    Expressing the dependencies between the scattering coefficients as an Hidden Markov Tree implies to do two modeling assumptions. The first one reflects the fact that the scattering coefficients can effectively be expressed by two hidden states,\\
    
    \begin{assumption}\textbf{Two populations:}\\
			A signal's scattering transform can be described as two clusters of coefficients' value. The smooth regions are represented by small scattering coefficients, while edges, ridges, and other singularities are represented by large coefficients.\\
			\label{assum:2pop}
    \end{assumption}
    
    As this assumption is common for standard or complex wavelets \cite{kingsbury2001complex} and because a scattering coefficient of order $m$ can simply be seen as the modulus of the wavelet transform of a ``new'' signal -\ie the scattering coefficient of order $m-1$, the two-populations assumption for scattering network is reasonable. This intuition can be confirmed by Figure~\ref{fig:2pop - 1} and~\ref{fig:2pop - 2} displaying the scattering coefficients at a given node obtained for several signals.\\
    
		\begin{figure}
				\begin{center}
					\includegraphics[width=3.5in]{placeholder.jpg}
					\caption[Two populations - experiment 1.]{Two populations - experiment 1: The signal is a binary square (0: background, 1: square) with noise. The scattering network has $m=3$ layers, $J=3$ scales and $L=2$ orientations.} %TODO: plots from exp_two_populations from the square
					\label{fig:2pop - 1}
				\end{center}
		\end{figure}    
		
		\begin{figure}
				\begin{center}
					\includegraphics[width=3.5in]{placeholder.jpg}
					\caption[Two populations - experiment 2.]{Two populations - experiment 2: The signal is the picture of Lena. The scattering network has $m=3$ layers, $J=3$ scales and $L=2$ orientations.} %TODO: plots from exp_two_populations from Lena
					\label{fig:2pop - 2}
				\end{center}
		\end{figure}
		
		Figure~\ref{fig:2pop - 1} displays the scattering coefficients of a noisy binary square. Note that for sake of clarity a ``small'' network has been used. This does not affect the observations that can be made and one can notice that the highest values of the scattering coefficient are obtained on highly informational pixels (edges in this case) while the low informational pixels are represented by scattering coefficients near $0$. Similar observations can be made for more complex signal -such as the one displays in Figure~\ref{fig:2pop - 2}. A statistical interpretation of the two-populations implies that scattering coefficients have a non-Gaussian marginal statistics, that is, their marginal probability density functions have a large peak at zero (many small coefficients) and heavy tails (a few large coefficients). Finally since many real-world signals (photograph-like images, for example) consist mostly of smooth regions separated by a few singularities, the two populations assumption tells us that the scattering coefficients are a sparse representation for these signals (this notion of sparsity can be made mathematically precise; see for example~\cite{donoho1993unconditional} or~\cite{devore1992image}). Most of the scattering coefficient magnitudes (representing the smooth regions) are small, while a few of them (representing the singularities and encoding the informational content) are large.\\
		
		The second assumptions transcripts the smoothness of the states across the scattering transform and can be stated as,\\
		
		\begin{assumption}\textbf{Persistence:}\\
		  Along a scattering path high and low scattering coefficient values cascade across the scattering orders.\\
		  \label{assum:Persistence}
		\end{assumption}

		This assumption codifies how the hidden states are structured. Smooth regions/singularities are represented by low/high values at every order. Persistence leads to scattering coefficient values that are statistically dependent along the branches of the scattering tree. Figure~\ref{fig:Persistence - 1} displays the magnitude of the scattering coefficient for a given node $i$ of the tree against those of its father $\rho(i)$. A clear positive correlation can be observed between the magnitude of the father and the son. Furthermore the transitions can be regrouped in two main clusters: Low-Low and High-High. This tends to confirm the validity of the persistence assumption. A second experiment computes the average correlation between each pair father/child of the tree for signal belonging to different classes. Again this experiments advocates in favor of a correlation in between the magnitude of the connected nodes.  
		
		\begin{figure}
				\begin{center}
					\includegraphics[width=3.5in]{placeholder.jpg}
					\caption[Persistence - experiment 1.]{Persistence - experiment 1: Plot the magnitude of the scattering coefficients at a given index $i$ of the tree against those of its father $\rho(i)$.} %TODO: plots from exp_correlation
					\label{fig:Persistence - 1}
				\end{center}
		\end{figure} 
		
		\begin{figure}
				\begin{center}
					\includegraphics[width=3.5in]{placeholder.jpg}
					\caption[Persistence - experiment 2.]{Persistence - experiment 2:.} %TODO: plots from exp_correlation_set
					\label{fig:Persistence - 2}
				\end{center}
		\end{figure}
		
		
  \section{Learning the tree structure:}
    \label{sec:SHMT/Learning}
    
    As seen in Section~\ref{subsec:PGMs/MM/Learning} training Markov  models can be done using Expectation-Maximization methods. Hidden Markov chains use a version of the EM algorithm called Forward-Backward algorithm allowing the propagation of the hidden states along the chain. MS. Crouse \cite{crouse1998wavelet} proposed the Upward-Downward algorithm, an adaptation to the hidden Markov trees of the Forward-Backward algorithm. Both algorithms were suffering from underflowing problems \cite{ephraim2002hidden} and JB. Durand \cite{durand2004computational} adapted P. A. Devijver's smoothing \cite{devijver1985baum} work to create a smoothed version of the learning algorithm for trees. This section proposes a rewriting of JB. Durand's version of the Upward-Downward algorithm adapted to non-regular non-binary HMTs.\\
       
    To do so one needs to introduce the following notations,
    \begin{itemize}
			\item $\forall i \in \mcalT$ let $n_{i}$ be the number of children of the node $i$.
      \item $\forall i \in \mcalT$ let $\bar{\mcalS}_{i}= \bar{s}_{i}$ be the observed subtree rooted at node $i$. By convention $\bar{\mcalS}_{0}$ denotes the entire observed tree.
      \item $\forall i \in \mcalT$ let $\bar{\mcalS}_{c(i)}= \bar{s}_{c(i)}$ be the entire -possibly empty collection of observed subtrees rooted at children of node $i$ (\ie the subtree $\bar{s}_{i}$ except its root $s_{i}$).
      \item If $\bar{\mcalS}_{i}$ is a proper subtree of $\bar{\mcalS}_{j}$, then $\bar{\mcalS}_{j\backslash i} = \bar{s}_{j\backslash i}$ is the subtree rooted at node $j$ except the subtree rooted at node $i$.
      \item $\forall i \in \mcalT$ let $\bar{\mcalS}_{0\backslash c(i)}= \bar{s}_{0\backslash c(i)}$ be the entire tree except for the subtrees rooted at children of node $i$.
    \end{itemize}
    Note that those notations transpose to the hidden state and for instance $\bar{\mcalH}_{i}= \bar{h}_{i}$ is the state subtree rooted at node $i$.\\
    %TODO: Figure for the cliques as Durand (if time)
    
    \subsection{E-Step:}
			\label{subsec:SHMT/Learning/E}
			The smoothed version of the E-step requires the computation of the conditional probability distribution $\xi_{i}(k) = P(H_{i}=k | \bar{\mcalS}_{i}= \bar{s}_{i})$ (smoothed probability) and $P(H_{i}=k, H_{\rho(i)}=g | \bar{\mcalS}_{i}= \bar{s}_{i})$ for each node $i \in mcalT$ and state $k$ and $g$. A decomposition of the smoothed probability adapted to the HMT structure is,
			
			\begin{equation}
			  \xi_{i}(k) = 
					\frac{P(\bar{\mcalS}_{0\backslash i}= \bar{s}_{0\backslash i} | H_{i}=k)}
						{P(\bar{\mcalS}_{0\backslash i}= \bar{s}_{i\backslash i} | \bar{\mcalS}_{1}= \bar{s}_{i})} 
					P(H_{i}= k |\bar{\mcalS}_{i}= \bar{s}_{i})
				\label{eq:Smoothed P decomposition}
			\end{equation}
			
			The smoothed upward-downward algorithm requires the introduction the following quantities,
			
			\begin{equation}
			  \beta_{i}(k) = P(H_{i}=k | \bar{\mcalS}_{i}= \bar{s}_{i})
			  \label{eq:UD beta node}
			\end{equation}
			\begin{equation}
			  \beta_{\rho(i)i}(k) = \frac{P(\bar{\mcalS}_{i}= \bar{s}_{i} | H_{\rho(i)}=k)} {P(\bar{\mcalS}_{i}= \bar{s}_{i}) }
			  \label{eq:UD beta node and parents}
			\end{equation}
						\begin{equation}
			  \alpha_{i}(k) = \frac{P(\bar{\mcalS}_{0\backslash i}= \bar{s}_{0\backslash i} | H_{\rho(i)}=k)} {P(\bar{\mcalS}_{0\backslash i}= \bar{s}_{0\backslash i} | \bar{\mcalS}_{i}= \bar{s}_{i} ) }
			  \label{eq:UD alpha}
			\end{equation}\\
			
			The smoothed upward-downward algorithm also requires the preliminary knowledge  of the marginal state distributions $P(H_{i}=k)$ for each node $i$. However this can simply be achieved by a downward recursion initialized for the root node with $P(H_{0}=k)=\pi_{0}(k)$ and then using the recursive formula~\ref{eq:SCHMT - State proba 1}.
			
			\subsubsection{Upward recursion:}
				\label{subsubsec:SHMT/Learning/E/Up}
				The upward algorithm is initialized at all the leaf of the tree, by computing $\beta_{i}(k)$ using,
				
				\begin{equation}
					\begin{split}
						\beta_{i}(k)	&= P(H_{i}=k | \bar{\mcalS}_{i})= \bar{s}_{i})\\
													&= \frac{P(S_{i}= s_{i} | H_{i}=k)P(H_{i}=k)} {P(S_{i}= s_{i})}\\
													&= \frac{P_{\theta_{k,i}}(s_{i}) P(H_{i}=k)} {N_{i}},
						\end{split}
						\label{eq:upward beta leaf}
				\end{equation}
				
				where the normalization factor for the leaf nodes $N_{i}$ is given by,
				\begin{equation}
					N_{i}	= P(S_{i}= s_{i}) = \sum_{k=1}^{K} P_{\theta_{k,i}}(s_{i}) P(H_{i}=k).
					\label{eq:upward normalization leaf}
				\end{equation}
				
				Then one can recursively (upward recursion) compute $\beta_{i}(k)$ for the remaining nodes of the tree using,
				
				\begin{equation}
					\begin{split}
						\beta_{i}(k)	&= P(H_{i}=k | \bar{\mcalS}_{i}= \bar{s}_{i})\\
													&= \left[ \prod_{j \in c(i)} P(\bar{\mcalS}_{j}= \bar{s}_{j} | H_{i}=k) \right] P(S_{i}= s_{i} | H_{i}=k) \frac{P(H_{i}=k)} {P(\bar{\mcalS}_{i}= \bar{s}_{i})}\\
													&= \left[ \frac{\prod_{j \in c(i)} P(\bar{\mcalS}_{j}= \bar{s}_{j} | H_{i}=k)} {P(\bar{\mcalS}_{j}= \bar{s}_{j})} \right]
														P(S_{i}= s_{i} | H_{i}=k) P(H_{i}=k) 
														\frac{\prod_{j \in c(i)} P(\bar{\mcalS}_{j}= \bar{s}_{j})} {P(\bar{\mcalS}_{i}= \bar{s}_{i})}\\
													&= \frac{ \left[ \prod_{j \in c(i)} \beta_{ij}(k) \right] P_{\theta_{k,i}}(s_{i})P(H_{i}=k)}{N_{i}},
						\end{split}
						\label{eq:upward beta node}
				\end{equation}
				
				where the normalization factor for the non-leaf nodes $N_{i}$ is given by,
				\begin{equation}
					\begin{split}
						N_{i}	&= \frac{P(\bar{\mcalS}_{i}}{\prod_{j \in c(i)} P(\bar{\mcalS}_{j}= \bar{s}_{j})}\\
									&= \sum_{k=1}^{K} \left[ \prod_{j \in c(i)} \beta_{ij}(k) \right]  P_{\theta_{k,i}}(s_{i})P(H_{i}=k).
					\end{split}
					\label{eq:upward normalization others}
				\end{equation}

				For all node $i$, the quantities $\beta_{\rho(i)i}(k)$ can be extracted from $\beta_{i}$ using,
				
				\begin{equation}
					\begin{split}
						\beta_{\rho(i)i}(k)	&= \frac{P(\bar{\mcalS}_{i}= \bar{s}_{i} | H_{\rho(i)}=k)} {P(\bar{\mcalS}_{i}= \bar{s}_{i}) }\\
																&= \frac{ \sum_{g=1}^{K} P(\bar{\mcalS}_{i}= \bar{s}_{i} | H_{i}=g) P(H_{i}= k | H_{\rho(i)}=k)} {P(\bar{\mcalS}_{i}= \bar{s}_{i}) }\\
																&= \sum_{g=1}^{K} \frac{ P(H_{i}=g | \bar{\mcalS}_{i}= \bar{s}_{i} |)}{P(H_{i}=g)} P(H_{i}= g | H_{\rho(i)}=k)\\
																&= \sum_{g=1}^{K} \frac{\beta_{i}(g) \epsilon_{i}^{(kl)}}{P(H_{i}=g)}.
						\end{split}
						\label{eq:upward beta node and parents}
				\end{equation}
				
				Those relationships one can derive the the upward algorithm~\ref{algo:Smoothed upward}.
			
				\begin{center}
					\begin{algorithm}
						\textbf{Meta-parameters:}\\
							$K$\\
								
						\textbf{Initialization:}\\
							\tcp{$P_{\theta_{k,i}}(s_{i})$:}
							\For{All the node $i$ of the tree $\mcalT$}{
								$P_{\theta_{k,i}}(s_{i}) = \mcalN(s_{i} | \mu_{k,i},\sigma_{k,i})$
								}
								
							\tcp{Loop over the leafs $i$ of the tree:}
							\For{All the leaf $i$ of the tree $\mcalT$}{
								$\beta_{i}(k) = \frac{P_{\theta_{k,i}}(s_{i}) P(H_{i}=k)}{\sum_{g=1}^{K} P_{\theta_{g,i}}(s_{i}) P(H_{i}=g)}$\\
								$\beta_{i,\rho(i)}(k) = \sum_{g=1}^{K}\frac{\beta_{i}(g) \epsilon_{i}^{(kg)}}{P(H_{i}=g)} . P(H_{\rho(i)}=k)$ \\
								$l_{i} = 0$
								}
								
						\textbf{Induction:}\\
							\tcp{Bottom-Up loop over the nodes of the tree:}
							\For{All non-leaf node $i$ of the tree $\mcalT$}{
								$M_{i} = \sum_{k=1}^{K} P_{\theta_{k,i}}(s_{i}) \prod_{j \in c(i)} \frac{\beta_{j,i}(k)}{P(H_{i}=k)^{n_{i}-1}}$ \\
								$l_{i} = \log(M_{i}) + \sum_{j \in c(i)}l_{j}$\\
								$\beta_{i}(k) = \frac{P_{\theta_{k,i}(s_{i})} \prod_{j \in c(i)}(\beta_{j,i}(k))}{P(H_{i}=k) ^{n_{i}-1} M_{i}} $\\
								\For{All the children node $j$ of node $i$}{
									$\beta_{i\backslash c(i)}(k) = \frac{\beta_{i}(k)}{\beta_{i,j}(k)}$
									}
								$\beta_{i,\rho(i)}(k) = \sum_{g=1}^{K} \frac{\beta_{i}(g) \epsilon_{i}^{(gk)}}{P(H_{i}=g)} .P(H_{\rho(i)}=k)$
								}
						\caption{Smoothed upward algorithm.}
						\label{algo:Smoothed upward}		
					\end{algorithm}        
				\end{center}
				
			\subsubsection{Downward recursion:}
				\label{subsubsec:SHMT/Learning/E/Downward}
				
				The downward recursion can either be built on the basis of the quantities $\alpha_{i}(k)$ or on the basis of the smoothed probabilities $\xi_{i}(k) = P(H_{i}=k | \bar{\mcalS}_{i}= \bar{s}_{i})$. The downward recursion on $\xi_{i}$ is initialized at the root node with,
				
				\begin{equation}
				  \xi_{0}(k)= P(H_{i}=k | \bar{\mcalS}_{0}=\bar{s}_{0}) = \beta_{0}(k).
				  \label{eq:Downward init}
				\end{equation}

				The quantity can then be computed recursively for each node of the tree using,
				
				\begin{equation}
					\begin{split}
						\xi_{i}(k)	&= P(H_{i}=k | \bar{\mcalS}_{0}=\bar{s}_{0}) \\
												&= \sum_{g=1}^{K} \frac{P(H_{i}=k, H_{\rho(i)}=g, \bar{\mcalS}_{0}= \bar{s}_{0})}{P(H_{\rho(i)}=g, \bar{\mcalS}_{0}= \bar{s}_{0})} P(H_{\rho(i)}=g | \bar{\mcalS}_{0}= \bar{s}_{0})\\
												&= P(\bar{\mcalS}_{i}= \bar{s}_{i} | H_{i}=k)  
													 \sum_{g=1}^{K} \frac{P(H_{i}=k | H_{\rho(i)}=g)}{P(\bar{\mcalS}_{i}= \bar{s}_{i} | H_{\rho(i)}=g)}
% 													 \frac{P(H_{\rho(i)}=g | \bar{\mcalS}_{0 \backslash i}= \bar{s}_{0 \backslash i})}{P(H_{\rho(i)}=g | \bar{\mcalS}_{0 \backslash i}= \bar{s}_{0 \backslash i})}
 													 P(H_{\rho(i)}=g | \bar{\mcalS}_{0}= \bar{s}_{0})\\
 												&= \frac{\beta_{i}(k)}{P(H_{i}=k)} \sum_{g=1}^{K} \frac{ \epsilon_{i}^{(gk)} \xi_{\rho(i)}(g)}{\beta_{\rho(i),i}(g)}.
						\end{split}
						\label{eq:Downward recursion xi}
				\end{equation}
				
				Using the fact that for all $i \in mcalT \;\;\; \xi_{i}(k) = \beta_{i}(k)\alpha_{i}(k)$ and the relationship from~\ref{eq:Downward recursion xi}, one can express the downward pass as presented in~\ref{algo:Smoothed downward}.
    
				\begin{center}
					\begin{algorithm}
						\textbf{Meta-parameters:}\\
							$K$\\
								
						\textbf{Initialization:}\\
							$\alpha_{0}(k) = 1$
								
						\textbf{Induction:}\\
							\tcp{Top-Down loop over the nodes of the tree:}
							\For{All node $i$ of the tree $\mcalT\backslash\{0\}$}{
 								$\alpha_{i}(k) = \frac{1}{P(H_{i}=k)} \sum_{g=1}^{K} \alpha_{\rho(i)}(g) \epsilon_{i}^{(gk)} \beta_{\rho(i)\backslash i}(g) P(H_{\rho(i)}=g)$
								}
						\caption{Smoothed downward algorithm.}
						\label{algo:Smoothed downward}		
					\end{algorithm}        
				\end{center}
				
			\subsubsection{Conditional properties:}
				\label{subsubsec:SHMT/Learning/E/Cond}
				
				To complete the E-step one needs to compute the conditional probabilities for each node. This is done simply by using,
				
				\begin{equation}
				  \forall i \in \mcalT \;\;\; P(H_{i}=k | \bar{\mcalS}_{0}=\bar{s}_{0}) = \alpha_{i}(k) \beta_{i}(k),
				  \label{eq:Estep cond proba node}
				\end{equation}
				and
				\begin{equation}
				  \forall i \in \mcalT\backslash\{0\} 
				  \;\;\; P(H_{\rho(i)}=g, H_{i}=k | \bar{\mcalS}_{0}=\bar{s}_{0}) = \frac{\beta_{i}(k)\epsilon_{i}^{(gk)}\alpha_{\rho(i)}(g)\beta_{\rho(i)}(g)}{P(H_{i}=k)}.
				  \label{eq:E-step cond proba and parent}
				\end{equation}
		
		\subsection{M-Step:}
			\label{subsec:SHMT/Learning/M}
			
			The \textit{Maximization} step of the EM algorithm aims to maximize the log-likelihood of the observation with regards to the parameters and then use those pseudo-optimal parameters for the next \textit{Expectation} step. Meaning that at the iteration $m$ of the EM process, the M-step does,
			
			\begin{equation}
				\Theta^{m+1} = \argmax_{\Theta} \left(E[\ln f(\bfx,H|\Theta)|\bfx,\Theta^{l}] \right).
				\label{eq:Mstep argax theta}
			\end{equation}
			
			The $\Theta$ maximizing the log-likelihood in~\ref{eq:Mstep argax theta} can be express analytically and this yields to the algorithm~\ref{algo:Mstep}
			
			\begin{center}
				\begin{algorithm}
					\textbf{Meta-parameters:}\\
						$K$,\\
						Distribution family for $P_{\theta}$ \tcp*{Here Gaussian}
						$N$ \tcp*{Number of observed realizations of the signal}
							
					\textbf{Initialization:}\\
						$\pi_{0}(k) = \frac{1}{N} \sum_{n=1}^{N} P(H_{0}^{n}=m|s_{0}^{n},\Theta^{l})$
							
					\textbf{Induction:}\\
						\tcp{Loop over the nodes of the tree:}
						\For{All node $i$ of the tree $\mcalT\backslash\{0\}$}{
							$P(H_{i}=k) = \frac{1}{N} \sum_{n=1}^{N} P(H_{i}^{n}=m|s_{i}^{n},\Theta^{l})$,\\
							$\epsilon_{i}^{gk} = \frac{\sum_{n=1}^{N} P(H_{i}^{n} = k, H_{\rho(i)}^{n}=g |w^{n}, \Theta^{l})} {N P(H_{\rho(i)}=k)}$,\\
							$\mu_{k,i} = \frac{\sum_{n=1}^{N} s_{i}^{n} P(H_{i}^{n} = k |w^{n}, \Theta^{l})} {N P(H_{i}=k)}$,\\
							$\sigma_{k,i}^{2} = \frac{\sum_{n=1}^{N} (s_{i}^{n} - \mu_{k,i})^{2} P(H_{i}^{n} = k |w^{n}, \Theta^{l})} {N P(H_{i}=k)}$.\\
							}
					\caption{M-step of the EM algorithm.}
					\label{algo:Mstep}
				\end{algorithm}        
			\end{center}
			
		\subsection{EM algorithm:}
			\label{subsec:SHMT/Learning/EM}
			
			Finally the EM algorithm iterates over the \textit{E-step} and the \textit{M-step} as describe by the algorithm~\ref{algo:EM}.
			
			\begin{center}
				\begin{algorithm}
					\textbf{Meta-parameters:}\\
						$K$;\\
						Distribution family for $P_{\theta}$;
						Convergence criteria \tcp*{Iteration limit or information based}
						Initialization method for $\Theta$ \tcp*{Random or prior knowledge}
						
					\textbf{Initialization:}\\
						$l=0$ \tcp*{Iteration counter}
						$\text{Initialize}(\Theta)$
					
					\textbf{Iteration:}\\
					\While{Not convergence}{
						\textbf{E-step:} Calculate $P(\bar{\mcalH}|\bar{\mcalH}, \Theta^{l})$.\\
						\textbf{M-step:} Set $\Theta^{m+1} = \argmax_{\Theta} \left(E[\ln f(\bfx,H|\Theta)|\bfx,\Theta^{l}] \right)$.\\
						l = l+1
						}
					\caption{EM algorithm.}
					\label{algo:EM}
				\end{algorithm}        
			\end{center}
			    
  \section{Classification:}
    \label{sec:SHMT/Clf}
    
    Let $\Theta_{c}$ now be a set of parameters for an SCHMT $\mcalT$ learned using the algorithm described in Section~\ref{sec:SHMT/Learning} and a training set $X_{c}$ composed of $N$ realization of a signal of class $c$ . Let also $x_{new}$ be another realization of this signal, not used for training and $\mcalT^{(new)}$ be the instance of the SCHMT generated by this realization.\\
    
    In this context the MAP algorithm aims at finding the optimal hidden tree $\hat{h}_{0}^{new}=(\hat{h}_{0}^{new} \dots \hat{h}_{I-1}^{new},\Theta_{c})$ maximizing the probability of this sequence given the model's parameters $P(\mcalH_{0}= \hat{h}_{0}^{new}|\mcalT^{(new)})$. The MAP framework also provides $\hat{P}$ the value of this maximum.\\
    
    For SCHMT the MAP algorithm has the form describe by the Algorithm~\ref{algo:MAP}.
    
		\begin{center}
			\begin{algorithm}
				\textbf{Meta-parameters:}\\
					$K$;\\
					
				\textbf{Initialization:}\\
					\For{all leaf $i$ of $\mcalT$}{
						$\gamma_{i}(k) = \beta_{i}(k)$ \tcp*{The gamma for all $k$ must be computed before the next step}
						$\gamma_{i,\rho{i}}(k) = \max_{1\leq g \leq K} \gamma_{i}(g)\epsilon_{i}^{kg}$ \\
						$\xi_{i}(k) = \argmax_{1\leq g\leq K} \gamma_{i}(g)\epsilon_{i}^{kg}$\\
					}
					
				\textbf{Induction:}\\
					\tcp{Top-Down loop over the nodes of the tree:}
					\For{All node $i$ of the tree $\mcalT\backslash\{0\}$}{					
						$\gamma_{i}(k) = P_{\theta_{k,i}}(s_{i}) \prod_{j \in c(i)} \gamma_{j,i}(k)$\\
						$\gamma_{i,\rho{i}}(k) = \max_{1\leq g \leq K} \gamma_{i}(g)\epsilon_{i}^{kg}$ \tcp*{except at root node}
						$\xi_{i}(k) = \argmax_{1\leq g\leq K} \gamma_{i}(g)\epsilon_{i}^{kg}$\\
					}
				
				\textbf{Termination:}\\
					$\hat{P} = \max_{1\leq g \leq K} \gamma_{0}(g)$\\
					$\hat{h}_{0} = \argmax_{1\leq g \leq K} \gamma_{0}(g)$\\
				
				\textbf{Downward tracking:}\\
					\tcp{Creation of the hidden tree from the root node}
					\For{All node $i$ of the tree $\mcalT\backslash\{0\}$}{
						$\hat{h}_{i}=\xi_{i}(\hat{h}_{\rho(i)})$
					}
				
				\caption{MAP algorithm.}
				\label{algo:MAP}
			\end{algorithm}        
		\end{center}
		
		The MAP Algorithm~\ref{algo:MAP} can be used in a multi-class classification problem simply by training a SCHMT model per class and then for a new realization $\bfx_{new}$ simply compare the MAP provided by each model as describe by the Algorithm~\ref{algo:MAP clf}.
		
		\begin{center}
			\begin{algorithm}
				\textbf{Meta-parameters:}\\
					$K$; $C$ \tcp*{Number of class}
					
				\For{All the class $c$}{
					$\hat{P}_{c} = \text{MAP}(x_{new},\Theta_{c},K)$\\
				}
				
				$\hat{P}= \max_{0\leq c < C} \hat{P}_{c}$\\
				$l= \argmax_{0\leq c < C} \hat{P}_{c}$\\
				
				\caption{MAP algorithm applied to multi-class classification problem.}
				\label{algo:MAP clf}
			\end{algorithm}        
		\end{center}
    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Experimental results:}
  \label{chap:Exp}
  This section presents some experimental results obtained using scattering hidden Markov trees for classification tasks. First a binary classification task is performed on simple simulated shapes. Then SCHMT are applied to classify seabed and ripples in sonar imagery.
  
  \section{Shapes:}
		\label{sec:Exp/Shape:}
		
		The SCHMT performances on classification tasks are at first assessed on telling apart different geometrical shapes. The data are noisy and translated realizations of two signals. The first signal is an $100$ by $100$ binary image picturing a centered black square on a white background. Examples of this class can be seen in Figure~\ref{fig:Shape square}. The second class is a $100$ by $100$ binary image picturing a centered black triangle on a white background. Examples of this class can be seen in Figure~\ref{fig:Shape triangle}. 
		
		\begin{figure}
			\begin{center}
				\includegraphics[width=3.5in]{placeholder.jpg}
			  \caption[Shape classification: square]{(a) Orignial signal (b) realization}
			  \label{fig:Shape square}
			\end{center}
			%TODO: images
		\end{figure}
		
		\begin{figure}
			\begin{center}
				\includegraphics[width=3.5in]{placeholder.jpg}
			  \caption[Shape classification: triangle]{(a) Orignial signal (b) realization}
			  \label{fig:Shape triangle}
			\end{center}
			%TODO: images
		\end{figure}
		%TODO: ST USED
		%TODO: RESULTS
		
  \section{Sonar Imagery:}
		\label{sec:Exp/Sonar:}
		The SCHMT is now tested on a more complex set of images. The data used are extracted from the \textit{UDRC MCM} sonar imagery dataset \cite{udrc}. This dataset, designed initially for underwater mines detection, contains targets comparable to those faced by coalition forces in current operational engagements and comprises of Synthetic Aperture RADAR (SAS imagery) and hyper-spectral data. From those very high dimensional images, $100$ by $100$ patches have been extracted and labeled as either seabed or ripple (see respectively Figure~\ref{fig:Seabed patch} and Figure~\ref{fig:Ripple patch}).\\

		\begin{figure*}[t!]
			\centering
			\begin{subfigure}[t]{0.5\textwidth}
				\centering
				\includegraphics[height=1.2in]{patch_seabed.eps}
			\end{subfigure}%
			~ 
			\begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=1.2in]{patch_seabed_2.eps}
			\end{subfigure}
			~
			\begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=1.2in]{patch_seabed_3.eps}
			\end{subfigure}
			\caption{Sample of seabed patches}
			\label{fig:Seabed patch}
		\end{figure*}
		
		
		\begin{figure*}[t!]
			\centering
			\begin{subfigure}[t]{0.5\textwidth}
				\centering
				\includegraphics[height=1.2in]{patch_ripple.eps}
			\end{subfigure}%
			~ 
			\begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=1.2in]{patch_ripple_2.eps}
			\end{subfigure}
			~
			\begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=1.2in]{patch_ripple_3.eps}
			\end{subfigure}
			\caption{Sample of ripple patches}
			\label{fig:Ripple patch}
		\end{figure*}
		
		The task at hand is the classification of those realizations of those two signals. The scattering transform used has $m=3$ orders, $J=5$ scales, $M=3$ orientations and uses a Morlet wavelet. The hidden Markov tree has $K=2$ states and is using a mixture of Gaussian to describe the relationship between the scattering coefficients and the hidden states. Two models -one for each class- $\Theta_{ripple}$ and $\Theta_{seabed}$ are trained on $200$ realizations of their class signal. The testing is then realized on $80$ images -$40$ of each classes. The performances of the SCHMT are assessed on $100$ instances of this experiment and the results are displayed in Table~\ref{table:Clf_ripple}\\
		
		\begin{center}
			\begin{table}
				\begin{center}				
					\begin{tabular}{ |p{3cm}||p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}| }
						\hline
						\multicolumn{6}{|c|}{Classification results} \\
						\hline
						Classification score:	&	N			& Mean	& Variance	& Maximum	& Minimum \\
						\hline
						Full:									& 100   & 0.74	& 0.101			& 0.9				&	0.5\\
						$\geq 60\%$: 					& 91    & 0.76	& 0.079			& 0.9				&	0.6\\
						$\geq 70\%$: 					& 73    & 0.79	& 0.058			& 0.9				&	0.7\\
						\hline
					\end{tabular}
					\label{table:Clf_ripple}
				\end{center}
				\caption{Classification performances over $100$ experiments of Ripple/Seabed classification.}
			\end{table}
		\end{center}
		
		The results with best scores at $90\%$ accuracy shows encouraging results as per the classification performances of the SCHMTs. However one can notice that some experiment provides scores as low as $50\%$ accuracy. Those scores are obtained when one model has shown a better convergence than the other, resulting to classifying all the instances of the test set to one class. This problem could be addressed by using a validation set to discard those models before testing. Another leads would be to work on the EM algorithm itself to make it more robust by adding information based convergence test.   
		
  \section{Segmentation:}
		\label{sec:Exp/Segmentation:}
		
		On its simplest form  a segmentation task can be seen as a set of independent classification tasks on subpart of an image. Hence one can use the models trained in the previous section to realize the segmentation of a full sonar imagery.\\
		
		One of the  $2001 \times 7333$ image from the \textit{UDRC MCM} is cut into a set $100 \times 100$ patches -some regions of the original image are not considered. And each of those patches is presented to the classifier. Results of this procedure can be seen in Figure~\ref{fig:Segmentation}.
		
		\begin{figure}
			\begin{center}
				\includegraphics[width=3.5in]{segmentation.eps}
				\caption{Segmentation of a sonar imagery.}
				\label{fig:Segmentation}
			\end{center}
			%TODO: images
		\end{figure}
		
		Even without introducing any form of spatial smoothing or correlation between nearby patches, the SCHMT model provides an accurate segmentation of the seabed.
		
		
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion:}

  \paragraph{Scattering hidden Markov tree:}
    The SCHMT appears to be a powerful model combining the interesting properties of the scattering transform for signal representation and the representational power of the hidden Markov models. It provides with analytically tractable interesting representation of high-dimensional signal as well as a probabilistic classification method. Experimental results confirm this intuition but also reveal some weakness of our model.\\
    
    
    
    
  \paragraph{Next steps:}
		Despite showing good performances when the learning phase has converged properly, SCHMT's performances are undermined by a sometime poor learning quality. This observation is one of the main drivers for the upcoming work. The other main driver of for the next step is to keep a well define probabilist framework.\\
		
		At this point the version of the EM algorithm considered in this report is using full Bayesian inference. However this methods is computationally expensive and sometimes yields to poor learning. Furthermore this version of the EM algorithm provides only a point-wise estimate of the model's parameters. It would be interesting to apply \Important{variational methods} the this problem \textbf{\textit{cite: Graphical Models, Exponential Families, and Variational Inference}}. Beside a potential improvement of the performance variational inference would also provide a estimated distribution for the model's parameter, thus allowing us to have access to a \Important{measure of uncertainty} for our learning.\\
		
		Another interesting direction to follow is the works on \Important{hierarchical graphical models} \textbf{\textit{cite: The Hierarchical Hidden Markov Model: Analysis and Applications}}. Those models would allow us to use the SCHMT model as a node of a wider probabilistic graphical model. Using such architecture, one could model a network of sensors each providing information on a targeted scene.\\
		
		Finally another interesting lead would be to consider other architecture for the graphical model and make it includes the representation learning step. This would be possible using \Important{Bayesian neural networks} 
     \textbf{\textit{cite: Bayes backprop and PBP}}.
    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Acknowledgements:}
	Jean-Baptiste Regli is funded by a Dstl/UCL Impact studentship and James Nelson is partially supported by grants from the Dstl and Innovate UK/EPSRC.Fpap

	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%   
% Bibliography %
%%%%%%%%%%%%%%%%
\bibliographystyle{apalike}%plainnat}
\bibliography{bib_SCHMT}
% this shows references stored in the myrefs.bib file
\nocite{*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%*%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THE END
\end{document}