\relax 
\providecommand\hyper@newdestlabel[2]{}
\catcode `:\active 
\catcode `;\active 
\catcode `!\active 
\catcode `?\active 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{french}
\@writefile{toc}{\select@language{french}}
\@writefile{lof}{\select@language{french}}
\@writefile{lot}{\select@language{french}}
\citation{Ahmed_2008,Bengio_2007,Larochelle_2007,Ranzato_2008,Vincent_2008}
\citation{Salakhutdinov_2009}
\citation{Hinton_2006,Salakhutdinov_2007}
\citation{Osindero_2008}
\citation{Hadsell_2008}
\citation{Collobert_2008,Mnih_2009}
\citation{Bengio_2009,Saxe_2013}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction:}{4}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:Introduction}{{1}{4}{Introduction:}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}General introduction:}{4}{section.1.1}}
\newlabel{seq:Introduction/General introduction}{{1.1}{4}{General introduction:}{section.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Image representation:}{4}{section.1.2}}
\newlabel{seq:Introduction/Deep neural architecture}{{1.2}{4}{Image representation:}{section.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Deep learning procedure applied to a computer vision example\relax }}{5}{figure.caption.3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:Example of deep architecture for a computer vision}{{1.1}{5}{Deep learning procedure applied to a computer vision example\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Outline of the report:}{5}{section.1.3}}
\newlabel{seq:Introduction/Outline of the report}{{1.3}{5}{Outline of the report:}{section.1.3}{}}
\citation{LeCun_webPage}
\citation{Rosenblatt_1958}
\citation{McCulloch_1943}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Scattering transform:}{7}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:Scattering transform}{{2}{7}{Scattering transform:}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Convolutional Neural Network:}{7}{section.2.1}}
\newlabel{seq:Artificial neural networks/Perceptron network}{{2.1}{7}{Convolutional Neural Network:}{section.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The perceptron network}}{7}{figure.caption.4}}
\newlabel{fig:The perceptron network}{{2.1}{7}{The perceptron network}{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Activation function: The step function}}{8}{figure.caption.5}}
\newlabel{fig:The step function}{{2.2}{8}{Activation function: The step function}{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Activation function: The sigmoid}}{9}{figure.caption.6}}
\newlabel{fig:The sigmoid function}{{2.3}{9}{Activation function: The sigmoid}{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Activation function: The hyperbolic tangent}}{9}{figure.caption.7}}
\newlabel{fig:The hyperbolic tangent function}{{2.4}{9}{Activation function: The hyperbolic tangent}{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Activation function: The rectified linear}}{10}{figure.caption.8}}
\newlabel{fig:The rectified linear function}{{2.5}{10}{Activation function: The rectified linear}{figure.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Scattering network:}{10}{section.2.2}}
\newlabel{seq:Artificial neural networks/The cost function}{{2.2}{10}{Scattering network:}{section.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}0-1 loss:}{10}{subsection.2.2.1}}
\citation{Cybenko_1989}
\citation{Hornik_1991}
\citation{LeCun_1985,Rumelhart_1986}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Squared loss:}{11}{subsection.2.2.2}}
\newlabel{eq:squared}{{2.9}{11}{Squared loss:}{equation.2.2.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Cross-entropy loss:}{11}{subsection.2.2.3}}
\newlabel{eq:Cross-entropy}{{2.10}{11}{Cross-entropy loss:}{equation.2.2.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Spatial Wavelet transform:}{11}{section.2.3}}
\newlabel{seq:Artificial neural networks/Multilayer perceptron network}{{2.3}{11}{Spatial Wavelet transform:}{section.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces A Multi-layers perceptron network}}{11}{figure.caption.9}}
\newlabel{fig:The multi-layers perceptron network}{{2.6}{11}{A Multi-layers perceptron network}{figure.caption.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Roto-translation wavelet transform:}{11}{section.2.4}}
\newlabel{seq:Artificial neural networks/Training by error backpropagation}{{2.4}{11}{Roto-translation wavelet transform:}{section.2.4}{}}
\newlabel{eq:last layer error for backpropagation}{{2.11}{12}{Roto-translation wavelet transform:}{equation.2.4.11}{}}
\newlabel{eq:backpropagation}{{2.13}{12}{Roto-translation wavelet transform:}{equation.2.4.13}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces The backpropagation algorithm on a d-layer perceptron\relax }}{12}{algocf.1}}
\newlabel{algo:backpropagation}{{1}{12}{Roto-translation wavelet transform:}{algocf.1}{}}
\citation{Ng_course}
\citation{Ng_course,Bengio_2009}
\newlabel{eq:gradient descent updates}{{2.14}{13}{Roto-translation wavelet transform:}{equation.2.4.14}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces A simple gradient descent algorithm}}{13}{algocf.2}}
\newlabel{algo:Gradient descent algorithm}{{2}{13}{Roto-translation wavelet transform:}{algocf.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Application to classification:}{13}{section.2.5}}
\newlabel{seq:Artificial neural networks/Applications and Limits}{{2.5}{13}{Application to classification:}{section.2.5}{}}
\citation{LeCun_webPage}
\citation{West_2000}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Probabilistic graphical models:}{15}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:Deep neural networks}{{3}{15}{Probabilistic graphical models:}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Bayesian Network:}{15}{section.3.1}}
\newlabel{seq:Deep neural networks/Depth definition}{{3.1}{15}{Bayesian Network:}{section.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces The computation of depth}}{15}{figure.caption.10}}
\newlabel{fig:Computation of the depth}{{3.1}{15}{The computation of depth}{figure.caption.10}{}}
\citation{Bengio_2009}
\citation{Freund_1996}
\citation{Serre_2007}
\citation{Hastad_1986}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Hidden Markov Models:}{16}{section.3.2}}
\newlabel{seq:Deep neural networks/Advantages of a deep architecture}{{3.2}{16}{Hidden Markov Models:}{section.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Computational advantages:}{16}{subsection.3.2.1}}
\newlabel{subseq:Deep neural networks/Advantages of a deep architecture/Computational advantages}{{3.2.1}{16}{Computational advantages:}{subsection.3.2.1}{}}
\citation{Bengio_2009,Ng_course}
\citation{Bengio_2009}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Statistical advantages:}{17}{subsection.3.2.2}}
\newlabel{subseq:Deep neural networks/Advantages of a deep architecture/Statistical advantages}{{3.2.2}{17}{Statistical advantages:}{subsection.3.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Biological advantages:}{17}{subsection.3.2.3}}
\newlabel{subseq:Deep neural networks/Advantages of a deep architecture/Biological advantages}{{3.2.3}{17}{Biological advantages:}{subsection.3.2.3}{}}
\citation{Hinton_1986}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Hidden Markov trees:}{19}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:DBN:}{{4}{19}{Hidden Markov trees:}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Model:}{19}{section.4.1}}
\newlabel{seq:DBN/The Boltzmann machine:}{{4.1}{19}{Model:}{section.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces A Boltzmann machine}}{19}{figure.caption.11}}
\newlabel{fig:A Boltzmann machine}{{4.1}{19}{A Boltzmann machine}{figure.caption.11}{}}
\citation{Smolensky_1986}
\newlabel{eq:BM_energy}{{4.3}{20}{Model:}{equation.4.1.3}{}}
\newlabel{eq:BM_derivative}{{4.5}{20}{Model:}{equation.4.1.5}{}}
\newlabel{eq:BM weight update}{{4.6}{20}{Model:}{equation.4.1.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Learning: Expectation maximization:}{20}{section.4.2}}
\newlabel{seq:DBN/The Restricted Boltzmann machine:}{{4.2}{20}{Learning: Expectation maximization:}{section.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces A restricted Boltzmann machine}}{21}{figure.caption.12}}
\newlabel{fig:A Restricted Boltzmann machine}{{4.2}{21}{A restricted Boltzmann machine}{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces The structure of the restricted Boltzmann machine}}{21}{figure.caption.13}}
\newlabel{fig:RBM for DBN}{{4.3}{21}{The structure of the restricted Boltzmann machine}{figure.caption.13}{}}
\newlabel{eq:RBM first sampling}{{4.7}{21}{Learning: Expectation maximization:}{equation.4.2.7}{}}
\newlabel{eq:RBM second sampling}{{4.8}{22}{Learning: Expectation maximization:}{equation.4.2.8}{}}
\newlabel{eq:RBM weight update}{{4.13}{22}{Learning: Expectation maximization:}{equation.4.2.13}{}}
\citation{Kullback_1951}
\citation{Hinton_2002}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Generation: Vitterbi algorithm:}{23}{section.4.3}}
\newlabel{seq:DBN/Training with contrastive divergence:}{{4.3}{23}{Generation: Vitterbi algorithm:}{section.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces The Gibbs sampling procedure}}{24}{figure.caption.14}}
\newlabel{fig:Gibbs sampling}{{4.4}{24}{The Gibbs sampling procedure}{figure.caption.14}{}}
\newlabel{eq:CD weight update}{{4.17}{24}{Generation: Vitterbi algorithm:}{equation.4.3.17}{}}
\citation{Hanlin_2013}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces RBM training with $N$ steps contrastive divergence.\relax }}{25}{algocf.3}}
\newlabel{algo:RBM training with CD}{{3}{25}{Generation: Vitterbi algorithm:}{algocf.3}{}}
\citation{Larochelle_2007,Ranzato_2007,Vincent_2008,Bengio_2007}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Scattering hidden Markov tree:}{26}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:AE and SAE:}{{5}{26}{Scattering hidden Markov tree:}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Hypothesis:}{26}{section.5.1}}
\newlabel{seq:AE and SAE/AEs:}{{5.1}{26}{Hypothesis:}{section.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces An auto-encoder}}{26}{figure.caption.15}}
\newlabel{fig:AE}{{5.1}{26}{An auto-encoder}{figure.caption.15}{}}
\citation{Ng_course}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Results and application:}{27}{section.5.2}}
\newlabel{seq:AE and SAE/Training procedure:}{{5.2}{27}{Results and application:}{section.5.2}{}}
\citation{LeCun_webPage}
\citation{Kaggle_higgs}
\citation{LeCun_webPage}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Experimental results:}{29}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:Experimental results}{{6}{29}{Experimental results:}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}MNIST:}{29}{section.6.1}}
\newlabel{seq:Experimental results/MNIST:}{{6.1}{29}{MNIST:}{section.6.1}{}}
\newlabel{fig:500_layer0}{{6.1a}{30}{100 randomly selected sub-representations in the first hidden layer\relax }{figure.caption.16}{}}
\newlabel{sub@fig:500_layer0}{{a}{30}{100 randomly selected sub-representations in the first hidden layer\relax }{figure.caption.16}{}}
\newlabel{fig:500_layer1}{{6.1b}{30}{100 randomly selected sub-representations in the second hidden layer\relax }{figure.caption.16}{}}
\newlabel{sub@fig:500_layer1}{{b}{30}{100 randomly selected sub-representations in the second hidden layer\relax }{figure.caption.16}{}}
\newlabel{fig:500_layer2}{{6.1c}{30}{100 randomly selected sub-representations in the third hidden layer\relax }{figure.caption.16}{}}
\newlabel{sub@fig:500_layer2}{{c}{30}{100 randomly selected sub-representations in the third hidden layer\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces A 3 hidden layers 500-500-500 stack auto-encoder}}{30}{figure.caption.16}}
\newlabel{fig:3L500_500_500}{{6.1}{30}{A 3 hidden layers 500-500-500 stack auto-encoder}{figure.caption.16}{}}
\newlabel{fig:1000_layer0}{{6.2a}{31}{100 randomly selected sub-representations in the first hidden layer\relax }{figure.caption.17}{}}
\newlabel{sub@fig:1000_layer0}{{a}{31}{100 randomly selected sub-representations in the first hidden layer\relax }{figure.caption.17}{}}
\newlabel{fig:1000_layer1}{{6.2b}{31}{100 randomly selected sub-representations in the second hidden layer\relax }{figure.caption.17}{}}
\newlabel{sub@fig:1000_layer1}{{b}{31}{100 randomly selected sub-representations in the second hidden layer\relax }{figure.caption.17}{}}
\newlabel{fig:1000_layer2}{{6.2c}{31}{100 randomly selected sub-representations in the third hidden layer\relax }{figure.caption.17}{}}
\newlabel{sub@fig:1000_layer2}{{c}{31}{100 randomly selected sub-representations in the third hidden layer\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces A 3 hidden layers 1000-1000-1000 stack auto-encoder}}{31}{figure.caption.17}}
\newlabel{fig:3L1000_1000_1000}{{6.2}{31}{A 3 hidden layers 1000-1000-1000 stack auto-encoder}{figure.caption.17}{}}
\citation{Kaggle_higgs}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Kaggle higgs competition:}{32}{section.6.2}}
\newlabel{seq:Experimental results/kaggle:}{{6.2}{32}{Kaggle higgs competition:}{section.6.2}{}}
\citation{Xavier_2010}
\citation{Bengio_2009}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Conclusion:}{33}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {paragraph}{Scattering hidden Markov tree:}{33}{paragraph*.18}}
\@writefile{toc}{\contentsline {paragraph}{Next steps:}{33}{paragraph*.19}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Acknowledgements:}{34}{chapter.8}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\bibcite{Rosenblatt_1958}{1}
\bibcite{LeCun_1985}{2}
\bibcite{Rumelhart_1986}{3}
\bibcite{McCulloch_1943}{4}
\bibcite{Cybenko_1989}{5}
\bibcite{Hornik_1991}{6}
\bibcite{LeCun_webPage}{7}
\bibcite{Bengio_2009}{8}
\bibcite{Freund_1996}{9}
\bibcite{Serre_2007}{10}
\bibcite{Hastad_1986}{11}
\bibcite{Bengio_2007}{12}
\bibcite{Geman_1984}{13}
\bibcite{Larochelle_2009}{14}
\bibcite{Erhan_2009}{15}
\bibcite{Ahmed_2008}{16}
\bibcite{Larochelle_2007}{17}
\bibcite{Ranzato_2008}{18}
\bibcite{Vincent_2008}{19}
\bibcite{Salakhutdinov_2009}{20}
\bibcite{Osindero_2008}{21}
\bibcite{Salakhutdinov_2007}{22}
\bibcite{Hinton_2006}{23}
\bibcite{Hinton_2006b}{24}
\bibcite{Hadsell_2008}{25}
\bibcite{Collobert_2008}{26}
\bibcite{Mnih_2009}{27}
\bibcite{Ranzato_2006}{28}
\bibcite{Bengio_2006}{29}
\bibcite{Hinton_2007}{30}
\bibcite{Erhan_2010}{31}
\bibcite{Hinton_1986}{32}
\bibcite{Smolensky_1986}{33}
\bibcite{Geman_1992}{34}
\bibcite{Ng_course}{35}
\bibcite{Rifai_2011}{36}
\bibcite{Matsuoka_1992}{37}
\bibcite{Bishop_1995}{38}
\bibcite{Rifai_2011b}{39}
\bibcite{Hinton_2012}{40}
\bibcite{Theano}{41}
\bibcite{Larochelle_2007}{42}
\bibcite{Kaggle_higgs}{43}
\bibcite{West_2000}{44}
\bibcite{Xavier_2010}{45}
\bibcite{Saxe_2013}{46}
\bibcite{Kullback_1951}{47}
\bibcite{Hinton_2002}{48}
\bibcite{Hanlin_2013}{49}
\bibcite{LeRoux_2008}{50}
\bibcite{Ranzato_2007}{51}
