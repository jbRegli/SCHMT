%%%%%%%%%%%%%%%%%%%%%
% Type of document: %
%%%%%%%%%%%%%%%%%%%%%
\documentclass[a4paper,11pt]{report}

%%%%%%%%%%%%%%%%%%%%%
% My usual settings %
%%%%%%%%%%%%%%%%%%%%%
\usepackage{JB_config_article}

%%%%%%%%%%%%%%%%%%%%%%%
% Headers and footers %
%%%%%%%%%%%%%%%%%%%%%%%
%%% Color definition:
\definecolor{lgray}{gray}{0.6}

\usepackage{fancyhdr}
\renewcommand{\headrulewidth}{0pt}
\lhead{}
\rhead{}
\lfoot{\vspace{0.3cm}\small\color{lgray}Research report, UCL Department of Statistical Science}
\rfoot{\vspace{-0.3cm}\includegraphics[width=1cm]{placeholder.jpg}
\hspace{0.5cm}
\includegraphics[width=1.5cm]{placeholder.jpg}}
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Location of the figures %
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\graphicspath{{Images/}} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Start the document
\begin{document}

	%% Cover page:
	\begin{titlepage}
	%\vspace*{1cm}
		\hspace{-1.5cm}\includegraphics[width=9cm,height= 3cm]{placeholder.jpg}
		\hfill{
		  \raggedleft \includegraphics[width=3cm]{placeholder.jpg}
		}
		\vspace*{5cm}
	
		\begin{center}
			\begin{sc} 	
				%%% Title:
				\huge SCATTERING HIDDEN MARKOV TREES
				\vspace*{0.2cm}
				%%% Subtitle:
				\\ \large IMAGE REPRESENTATION AND SCATTERING TRANSFORM MODELING
				\vspace*{2cm}
			\end{sc}
			%%% Author:
			\\ \LARGE Jean-Baptiste REGLI
			\vspace*{0.2cm}
			%%% Date
			\\ \large 2013-2014
		\end{center}
		%\vspace*{5cm}

		%%% Supervisors:
		\vfill
		\begin{center}
			\vspace*{1cm}
			\Large RESEARCH REPORT
			\vspace*{0.5cm}
			\\ \large Academic supervisor: James Nelson 
			\\ \large Sponsor: Dstl/UCL Impact studentship
			
			\vspace*{1cm}
			\Large UCL
			\\ \normalsize Department of Statistical Science
			\\ London
		\end{center}
	\end{titlepage}
	\clearpage

	\vfill	
		
	% Blank page
	\newpage
	\thispagestyle{empty}
	\mbox{}
 
	% Table of contents
	\setcounter{tocdepth}{3}
	\renewcommand{\contentsname}{Contents:}
	\tableofcontents
	\clearpage

	% List of figures
	\renewcommand{\listfigurename}{List of figures:}
	\listoffigures

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction:}
	\label{chap:Intro}
		   
	\section{Need for a better signal representation:}
		\label{seq:Intro/Need}
		
		Our researches have been focused on \Important{high-dimensional classification problems}. Meaning that throughout this paper we will be working with several -says $N$- realizations of a signal $X =\{\bfx_{1}, \dots  , \bfx_{N}\}$  where,
			
		\begin{equation*}
			\forall i \in \llbracket 1 , N \rrbracket \;\;\; \bfx_{i}=(x(1) \, \dots  \, x(d)) \;\;\;\;\; \text{with} \;\; d \sim 10^{6} \; \text{and} \; x(.) \in \dsR.  
		\end{equation*}
		
		\begin{figure}
			\begin{center}
				\includegraphics[width=3.5in]{placeholder.jpg}
			  \caption[High dimensional signals]{(a) Sound waveform (b) Picture}
			  \label{fig:highDim signals}
			\end{center}
			%TODO: (a) Sound waveform (b) Picture
		\end{figure}
		
		And we are interested in \Important{learning the labeling function} -says $f$- given $N$ labeled sampled values -training examples- $\{x_{i}, y_{i}=f(x_{i})\}_{i\leq N}$.\\
		
		A naive solution to this problem would be to infer the class of a new realization $\bfx$ by looking at its neighbors, e.g. K-Nearest Neighbors (KNN). This approach is working fine in the case of low dimensional problems (\textbf{\textit{citation KNN good perf}}). However it shows limitations in high dimensional cases (\textbf{\textit{citation KNN curse of dimensional}}), because the number of samled values of the signal needed to find a neighbor to a new realization $\bfx$ grow exponentially with the number of dimensions.\\
			
		To over come this issue one could try to reduce the number of dimensions of the problem. In the simple case the signal$\bfx$ belongs to a subset $\Omega \subset \dsR^{d}$ where $\Omega$ .
		
		%TODO: CCL sur image representation and clf
		
	\section{Image representation:}
		\label{seq:Intro/Image rep}
		%TODO: Properties of a ``good'' image representation for classification.
		We are now interested in projecting our signal into a new space where the classification task would be simpler. To do so we need to project the signal into a ``good'' space, i.e. crate a good representation of our data. We will first provide an intuition of what ``good'' is and then provide more formal mathematical definitions for our intuition and how this can be achieved.
%     
		\subsection{Intuition of a ``good'' image representation:}
      \label{seq:Intro/Image rep/Intuition}
      One way to develop an intuition on what properties a ``good'' representation for classification have is to look at the human visual function and what he is able to tell apart.\\
      
      Based on that, we think our representation should be:
      
      \begin{itemize}
				\item \Important{Informative} enough to permit classification.\\
	
				\item \Important{Invariant to translations}. Indeed to a human eye there is no difference in the information carried by a signal if it is shifted.\\
		
				\begin{figure}
				  \begin{center}
				    \includegraphics[width=3.5in]{placeholder.jpg}
				    \caption[Translation invariance]{A human can easily tell that those two images are from the same class.}
				    \label{fig:Illustration translation invariance}
				  \end{center}
				  %TODO: (a) initial image (b) sifted one
				\end{figure}
	
				\item \Important{Stable to deformations}. Once again to a human eye, it is still possible to recognize a signal if it has undergone -small- deformations. Yet if the deformations is too important the informational content of the signal is lost.\\
				
				\begin{figure}
				  \begin{center}
				    \includegraphics[width=3.5in]{placeholder.jpg}
				    \caption[stability to deformations]{A human can easily tell that (a) and (b) are from the same class. (c) can still be recognized even though it is slightly more challenging.}
				    \label{fig:illustration stability deformations}
				    %TODO: (a) initial image (b) sightly deformed one (c) heavy deformation
				  \end{center}
				\end{figure}
				
				\item \Important{To a certain degree invariant to rotations}. Rotations cannot be handled as easily as translation. Indeed here one is after a local rotational invariance rather than a global one. Solutions exist to develop a scattering transform with such behaviour \textbf{\textit{citation}} but this will not be addressed in this review. %TODO: citation and change if it is
			
					\begin{figure}
					  \begin{center}
					    \includegraphics[width=3.5in]{placeholder.jpg}
					    \caption[Rotation invariance]{A human can easily tell that (a) and (b) are from the same class. (c) could be a  '6' slightly rotated or a '9' heavily rotated.}
					    \label{fig:Illustration rotation invariance}
					    %TODO: (a) initial image (b) sightly rotated one (c) 180 rotation
					  \end{center}
					\end{figure}	
			  	\end{itemize}
      
		\subsection{Formalization of a ``good'' image representation:}
      \label{seq:Intro/Image rep/Formalization}
      
      To formalize the intuitions on the representation stated earlier, we first need to define the signal. Throughout this document, a signal will be defined as,
			
			\begin{defn} \textbf{Signal}\\ 
				A signal $f$ is a square-integrable $d$ dimensional real function.
				\begin{equation*}
					f \in \mathcal{L}^{2}(\dsR^{d}).
				\end{equation*}
				\label{def:Signal}
			\end{defn}
			
			To be informative enough, a representation has to preserve separability between elements of different classes. Formally this is,
      
			\begin{prop} \textbf{Discriminability preservation}
				A representation $\Phi$ preserves discriminabilty if all elements of two different classes are distance of margin $C$ in the representation space, i.e.:
				\begin{equation*}
					\forall (x,x') \in \left(\dsR^{d}\right)^{2} \; \exists C \in \dsR \;\; | \;\; \abs{f(x)-f(x')} = 1 \;\; \Rightarrow \;\; \norm{\Phi(x)-\Phi(x')} \geq C^{-1}
				\end{equation*}
				\label{pty:Discriminabilty}
			\end{prop}
  
      The class associated to a representation of a signal appears to be invariant to small shifts. In this document we call $L_{(.)}$ the translation operator for the function in $\mathcal{L}^{2}(\dsR^{d})$, \ie for $f \in \mathcal{L}^{2}(\dsR^{d}) \;\; \text{and} \;\; (x,c) \in (\dsR^{d})^{2} \;\;\;\; L_{c}f(x) = f(x-c)$. An operator is translation invariant -resp: canonical translation invariant- if,

      \begin{prop} \textbf{Translation invariant}\\ 
				An operator $\Phi: \mathcal{L}^{2}(\dsR^{d}) \rightarrow \mathcal{H}$ where $\mathcal{H}$ is an Hilbert space is translation invariant if:
	      	\begin{equation*}
			  		\forall c \in \dsR^{d} 
			  		\;\; \text{and}  \;\;
			  		\forall f \in \mathcal{L}^{2}(\dsR^{d}) \;\;
			  		\Phi(L_{c}f) = \Phi(f).
				\end{equation*}
				\label{pty:Translation invariance - intuition}
      \end{prop}
      \vspace{-30pt}
      \begin{prop} \textbf{Canonical translation invariant}\\ 
				An operator $\Phi: \mathcal{L}^{2}(\dsR^{d}) \rightarrow \mathcal{H}$ where $\mathcal{H}$ is an Hilbert space is canonical translation invariant if:
        \begin{equation*}
			  		\forall f \in \mathcal{L}^{2}(\dsR^{d}) \;\;
			  		\Phi(L_{a}f) = \Phi(f) 
			  		\;\; \text{where} \; a \in \dsR^{d} \; \text{is function of} \; f.
				\end{equation*}
				\label{pty:Canonical translation invariance - intuition}
      \end{prop}
      
      For the usual representation operators instabilities to deformations are known to appear -especially at high frequencies. To prevent this, one would like the representation to be non-expansive,
      
      \begin{defn} \textbf{Non-expensive representation}
				A representation $\Phi$ is non-expensive if,
				\begin{equation}
			  		\forall (f,h) \in (\mathcal{L}^{2}(\dsR^{d}))^{2} \;\; 
			  		\norm{\Phi(f) - \Phi(h)} \leq \norm{f-h}.
				\end{equation}
				\label{def:Non-expansivity - intuition}
      \end{defn}
      
      The stability to deformations of a non-expansive operator can be expressed as its Lipschitz continuity to the action of deformations close to translations \textbf{\textit{cite mallat GIS}}. Such a diffeormorphism transform can be expressed as,
      \begin{equation*}
      		\begin{split}
      			L_{\tau}	: \; & \mathcal{L}^{2}(\dsR^{d}) \rightarrow \mathcal{L}^{2}(\dsR^{d})\\
      							  & \;\;\;\; f \;\;\;\;\; \rightarrow  f(\mathds{1} - \tau)
				\end{split}
      \end{equation*}

			where $\tau(x) \in \dsR^{d}$ is a displacement field.
			
			\begin{prop} \textbf{Lipschitz continuous}
				A translation invariant operator $\Phi$ is said to be Lipschitz continuous to the action of $mathcal{C}^{2}$ diffeomorphisms if for any compact $\Omega \in \dsR^{d}$ there exists $C$ such that for all $f \in \mathcal{L}^{2}(\dsR^{d})$ supported in $\Omega$ and all $\tau \in mathcal{C}^{2}(\dsR^{d})$,
				\begin{equation}
					\norm{\Phi(f) - \Phi(L_{\tau}f)}_{\mathcal{H}} \leq 
					C \norm{f} \left(\sup_{x \in \dsR^{d}} \abs{\nabla \tau(x)} + \sup_{x \in \dsR^{d}} \abs{\mathit{H}\tau(x)}\right)
					\label{eq:Lipschitz continuity}
				\end{equation}
				\label{pty:Lipschitz continuity - intuition}
			\end{prop}
      
      where $\abs{\nabla \tau(x)}$ and $\abs{\mathit{H}\tau(x)}$ are respectively the sup-norm and the sup-norm of the Hessian tensor of the matrix $\tau(x)$.\\
      
      Hence a Lipschitz continuous operator $\Phi$ is almost invariant to "local" translations by $\tau(x)$, up to the fist and second order deformations terms. The equation~\ref{eq:Lipschitz continuity} also implies that $\Phi$ is invariant to global translations.\\

    \subsection{State of the art in image representation:}
      \label{seq:Intro/Image rep/State of the art}      
      
			Now that we have listed the properties we would like our representation to have, let us have a look at the usual signal representation tools and see if they which of them they fulfil.\\
      
			The first representation method one can think of is the modulus of the \Important{Fourier transform}. This operator is informational enough to allow -to a certain extent- discrimination different type of signal \textbf{\textit{find a citation for clf with fourier transform}}. It is also translation invariant \textbf{\textit{find a citation}}. However it is well known that those operators present instabilities to deformation at high frequencies \textbf{\textit{cite 10 from mallat}} and thus are not Lipschitz continuous to the action of diffeomorphisms.\\
      
			\Important{Wavelet transform} is another popular representation method. Again they provide a "good enough" representation to allow classification of different signals \textbf{\textit{find citations}}. Plus by grouping high frequencies into dyadic packet in $\dsR^{d}$, wavelet operators are stable to deformations \textbf{\textit{citation mallat's book}}. 
			
% 			\begin{equation}
% 				 = \begin{pmatrix} 
% 			\end{equation}
			
			\begin{equation}
				\begin{matrix}
					W\bfx =
					\begin{pmatrix}
						\bfx \ast \phi \\[0.5em]
						\bfx \ast \psi_{\lambda} \\[0.5em]
					\end{pmatrix}
					\begin{aligned}
						\begin{matrix}
							\rightarrow \text{averaging part}				\\[0.5em]
							\rightarrow \text{high frequency part}	\\[0.5em]
						\end{matrix}
					\end{aligned}
				\end{matrix}
			\end{equation}
		

			However only the averaging part of a wavelet is invariant to translation ans thus wavelets themselves are known to be non-invariant to translations.\\
			
			Another signal representation method popular at the moment are the \Important{convolutional neural networks} \textbf{\textit{cite LeCun}}. As opposed to the two previously mentioned representation methods, those operators are not fixed but learned from the data \textbf{\textit{cite learning method from CNN}}. Over the past few year they have provided state of the art results on many standard classification task, such as MNIST \textbf{\textit{cite}}, CIFAR \textbf{\textit{cite}}, ImageNet \textbf{\textit{cite }} or \textbf{\textit{find a example in speech processing}}. Those good results are used to advocate that those networks are learning "good" representations. However it seems that in certain cases they learn representation of the data that are -for example- not invariant to deformations \textbf{\textit{cite Bruna and Al strange pties of NN}}.\\
			         
	\section{Probabilistic graphical model:}
		??? - not sure yet
    
	\section{Outline of the report:}
    \label{seq:Intro/Outline of the report}    
		The part~\ref{chap:ST} of this report will summarize and explain the recent work Stephane Mallat and his group on the \Important{Scattering Transform} (ST), a wavelet-based operator fulfilling all the properties of what we have defined as a ``good'' representation for signal classification. Second (see~\ref{chap:PGMs}) we will introduce the \Important{Probabilistic Graphical Models} (PGMs) as generative models that can be used -among other tasks- for classification. Then in~\ref{chap:SHMT} we will describe how the the representation produced by the scattering transform can be modeled by an hidden markov tree, using what we have named \Important{Scattering Hidden Markov Trees} (SCHMTs). Finally in~\ref{chap:Experimental results} we will provide some example of applications.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{The Scattering transform:}
  \label{chap:ST}
  %TODO: In this section we will introduce the scattering transform as a mathematical object providing a deterministic representation of a signal.
  %TODO2:Properly cite mallat, bruna and al
  
	In this section we describe the construction process of a mathematical operator - the scattering transform (ST)- designed to generate what we have considered to be an interesting representation of our data (see \ref{seq:Intro/Image rep}). Therefore a scattering transform builds \Important{invariant}, \Important{stable} and \Important{informative representation} of signals through a \Important{non-linear},
  \Important{unitary transform}. It is an operator delocalizing signal informational content into scattering decomposition path, computed by \Important{cascading wavelet/modulus operators}.
  This architecture is similar to a \Important{convolutional neural network} (CNN) where the synaptic weights would be given by a wavelet operator instead of learned.\\
  
	In this section we study a wavelet-based representation method -the scattering transform- having the properties of what we have defined as a ``good'' representation for signal classification. We do so by first explaining how are built the scattering operators (see \ref{seq:ST/SCN}) and review some of their important properties (see {\textbf{\textit{ref: correct section}}). Once more familiar with the theory of the scattering transform we will see how similar in their architecture they are to Convolutional Neural Network (CNN) (see~\ref{seq:ST/CNN}). Finally, in \ref{seq:ST/Applications to clf}, we describe how the scattering transform is usually used in classification tasks.
	
% 	\section{Scattering Convolution Network:}
% 		%TODO Description of  the scattering tool -- Progressive construction of a roto-translation invariant representation.
	    
    In this section we first introduce a wavelet-based scattering transform built to have interesting properties for classification tasks, meaning being translation invariant and stable to $\mathbf{\mathcal{L}^{2}}$ deformations, while preserving the discriminabilty between classes. Then we explained how those operators can be stacked to create a ``deep'' scattering transform using a convolutional architecture.
      
		\section{Scattering wavelets:}
			\label{seq:ST/Scattering wavelets}

			A two-dimensional directional wavelet is obtained by scaling and rotating a single band-pass filter $\psi$. If we let $G$ be a discrete, finite rotation group of $\dsR^{2}$, multi-scale directional wavelet filters are defined for any scale $j \in \dsZ$ and rotation $r \in G$ by
      
      \begin{equation}
				\label{eq:multi-scale directional wavelet}
				\psi_{2^{j}r}(u) = 2^{2j} \psi(2^{j}r^{-1}u).
      \end{equation}
      
      To simplify the notations, we will now denote $\lambda = \lambda(j,r) \; \eqdef \; 2^{j}r \in \Lambda \; \eqdef \; G \times \dsZ$.\\
      
      A wavelet transform filters the signal $x$ using a family of wavelets $\{x \ast \psi_{\lambda}(u)\}_{\lambda}$. This is computed from a filter bank of dilated and rotated wavelets having no orthogonality property and it creates a multi-scale and orientation representation of the input.\\ 
      %TODO: maybe add part on aliasing and invertibility
      
      If $u.u'$ and $\|u\|$ define respectively the inner product and the norm in $\dsR^{2}$, the Morlet wavelet $\psi$ is an example of wavelet given by,
      
      \begin{equation*} 
				\label{eq:Morlet wavelet}
				\psi(u) = C_{1}(e^{iu.\xi} - C_{2}) e^{\|u\|^{2}/(2 \sigma^{2})},
      \end{equation*}

      where $C_{1}$, $\xi$ and $\sigma$ are meta-parameters of the wavelet and $C_{2}$ is adjusted so that $\int \psi(u) du = 0$. Figure~\ref{fig:Morlet wavelet} shows a Morlet wavelet for  $\xi= 3\pi/4$ and $\sigma=0.85$.\\
      
      \begin{figure}
				\begin{center}
					\includegraphics[width=3.5in]{placeholder.jpg}
					\caption{Complex Morlet wavelet.}
					\label{fig:Morlet wavelet}
				\end{center}	
      \end{figure}
      
      As opposed to the Fourier sinusoidal waves, wavelets are operators stable to $\mathbf{\mathcal{L}^{2}}$ deformations as they can be expressed as localized waveforms (\textbf{\textit{citation}}). However, wavelet transforms compute convolutions with wavelets, hence they are translation covariant operators (\textbf{\textit{citation}}).\\
        
      To ensure a translation invariant behavior to an operator commuting with them, one has to introduce a non-linearity. For example if $R$ is a linear or non-linear operator commuting with translations $L_{c}$, \ie $R(L_{c}x) = L_{c}R(x)$), then the integral $\int R(x(u))du$ is  translation invariant. One can apply this to $R(x) = x \ast \psi_{\lambda}$ and gets the trivial invariant, 
      
      \begin{equation*}
				\label{eq:Trivial invariant}
				\int x \ast \psi_{\lambda}(u)du = 0,
      \end{equation*}
      
      for all $x$ as $\int \psi_{\lambda}(u)du = 0$. However to preserve the informative character of the  scattering operator, one has to ensure that the integral does not vanish. To do so an operator $M$ such that $R(x) = M(x \ast \psi_{\lambda})$ is introduced. If $M$ is a linear  transformation commuting with translation then the integral still vanishes. Hence one has to choose $M$ to be a non-linear.\\
      
      Keeping in mind that the scattering transform has to be stable to deformations and taking advantages of the wavelet transform stability to small deformations in the input space, we also impose that $M$ commutes with deformations, 
      
      \begin{equation*}
				\label{eq:Commute with deformations}
				\forall \tau(u) \; , \; M L_{\tau} = L_{\tau} M.
      \end{equation*}
      
      If a weak differentiability condition is added, one can prove (\textbf{\textit{ref ISCN 6}}) that $M$ must necessarily be a point-wise operator, \ie $Mx(u)$ only depends on the value of $x(u)$. Finally, by adding an $\mathbf{\mathcal{L}^{2}}(\dsR^{2})$ stability constraint,
      
      \begin{equation*}
				\label{eq:L2 stability}
				\forall (x,y) \in \mathbf{\mathcal{L}^{2}}(\dsR^{2})^{2} \; , \; 
				\norm{Mx} = \norm{x} 
				\; \text{and} \;
				\norm{Mx-My} \leq \norm{x-y},
      \end{equation*}     
      
      one can show (\textbf{\textit{ref ISCN 6}}) that necessarily $Mx= e^{i\alpha}\abs{x}$. For the scattering transform, the simplest solution of setting $\alpha$ to $0$ is choosed and therefore the resulting coefficients are the $\mathbf{\mathcal{L}^{1}}(\dsR^{2})$ norms:
       
      \begin{equation*}
				\label{eq:L1 norm}
				\norm{x \ast \psi_{\lambda}}_{1} = \int \abs{x \ast \psi_{\lambda}} du
      \end{equation*}      
      
      The family of $\mathbf{\mathcal{L}^{1}}(\dsR^{2})$ normed wavelet $\{ \norm{x \ast \psi_{\lambda}}_{1}\}_{\lambda}$ generate a crude signal representation which measures the sparsity of the wavelet coefficients. One can prove (\textbf{\textit{ref ISCN 36}}) that $x$ can be reconstructed from $\{ \abs{ x \ast \psi_{\lambda}(u)} \}_{\lambda}$ up to a multiplicative constant. Which means that the information loss in $\{ \norm{x \ast \psi_{\lambda}}_{1} \}_{\lambda}$ comes from the integration of the absolute value $\abs{ x \ast \psi_{\lambda}(u)}$ which removes all non-zero frequencies. However those components can be recovered by calculating the wavelet coefficients $\abs{ x \ast \psi_{\lambda_{1}}} \ast \psi_{\lambda_{2}}(u)$. By doing so their $\mathbf{\mathcal{L}^{1}}(\dsR^{2})$ norms define a much larger family of invariants:
      
      \begin{equation*}
				\label{eq:2nd order}
				\forall (\lambda_{1}, \lambda_{2}) 
				\norm{ \abs{ x \ast \psi_{\lambda_{1}}} \ast \lambda_{2}}_{1} =
				\int \abs{ \abs{ x \ast \psi_{\lambda_{1}}(u)} \ast \psi_{\lambda_{2}}} du
      \end{equation*}          
      
      By further iterating on the wavelet/modulus operators more translation invariant coefficients can be computed. Let us define:
      
      \begin{defn} \textbf{Scattering Propagator}\\ 
				The scattering operator $U$ for a scale and an orientation $\lambda \in G \times \dsZ$ is defined as the absolute value of the input convolved with the wavelet operator at this scale and orientation.
				
				\begin{equation}
					\label{eq:scattering propagator}
					U[\lambda](x) \eqdef \abs{x \ast \psi_{\lambda}}
				\end{equation}
				\label{def:SO}
			\end{defn}
						
			\begin{defn} \textbf{Path Ordered Scattering Propagators}\\ 
				Any sequence $p = (\lambda_{1}, \lambda_{2},\dots ,\lambda_{m})$ where $\forall i \in \llbracket 1,m \rrbracket \lambda_{i} \in G \times \dsZ$ defines a \Important{path} of length $m$, \ie the ordered product of non-linear and non-commuting operators,
				
				\begin{equation}
					\label{eq:path ordered SP}
					\begin{split}
						U[p]x &\eqdef U[\lambda_{m}]\dots U[\lambda_{2}]U[\lambda_{1}](x) \\
							&= \abs{\abs{\abs{\abs{x \ast \psi_{\lambda_{1}}} \ast \psi_{\lambda_{2}}} 
							\dots } \ast \psi_{\lambda_{m}}}.   
					\end{split}
				\end{equation}
				
				With the convention: $U[\emptyset]x = x$\\
				\label{def:path ordered SO}
      \end{defn} 
      
      From there one can provide a first formal definition of the scattering transform:
      
      \begin{defn} \textbf{Scattering Coefficient}\\
				A scattering coefficient along the path $p$ is defined as an integral of the $p$ ordered scattering propagators, normalized by the response of a Dirac:

				\begin{equation}
					\label{eq:ST1}
					\bar{S}[p](x) \eqdef \mu_{p}^{-1} \int U[p]x(u) du
				\end{equation}
				
				with,
				
				\begin{equation*}
					\label{eq:ST normalization}
					\mu_{p} \eqdef \int U[p]\delta(u)du      
				\end{equation*}
				\label{def:SC}
      \end{defn}
      
      We shall see later (\textbf{\textit{reference}}) that each scattering coefficient $\bar{S}[p](x)$ is -as desired - invariant to translation of the input $x$ and Lipschitz continuous to deformations.\\
      
      For classification tasks, one might want to compute localized descriptors only invariant to translations smaller than a predefined scale $2^{J}$, while keeping the spatial variability at 
      scales larger than $2^{J}$. One can achieved this by localizing the scattering integral with a scaled spatial window $\phi_{2^{J}}(u) = 2^{-2J} \phi(2^{-2J}u)$. This yield to the definition of the windowed scattering transform:
      
      \begin{defn} \textbf{-Windowed- Scattering Coefficient Of Order $m$}\\
				If $p$ is a path of length $m \in \mathds{N}$, the -windowed- scattering coefficient of order $m$ at scale $2^{J}$ is defined as:
				
				\begin{equation}
					\label{eq:ST windowed}
					\begin{split}
						S_{J}[p](x) &\eqdef U[p]x \ast \phi_{2^{J}}(u) \\
									&= \int U[p]x(v) \phi_{2^{J}}(u-v) dv \\
									&= \abs{\abs{\abs{\abs{x \ast \psi_{\lambda_{1}}} \ast \psi_{\lambda_{2}}} 
							\dots } \ast \psi_{\lambda_{m}}} \ast \phi_{2^{J}}(u)
					\end{split}
				\end{equation}
				
				With the convention: $S_{J}[\emptyset]x = x \ast \phi_{2^{J}}$\\
      	\label{def:SC windozed}
      \end{defn}

      % TODO: Move this to the properties section
%       For each path $p$, $S_{J}[p]x(u)$ is a function of the window position $u$, which can be sub-sampled
%       at interval proportional to the window size $2^{J}$. Averaging by $\phi_{2^{J}}$ implies that 
%       $S_{J}[p]x(u)$ is nearly invariant 

		\section{Scattering Convolution Network:}
			\label{seq:ST/SCN}
			
			This section introduces the scattering transform as an \Important{iterative process over a one-step operator} and creates a parallel with convolutional neural networks \textbf{\textit{cite LeCun convNet 11 of mallat long paper}}. Let us note denote $U_{J}[\Omega] \eqdef \{U_{J}[p]\}_{p \in \Omega}$ and $S_{J}[\Omega] \eqdef \{S_{J}[p]\}_{p \in \Omega}$ a family of operators indexed by a path set $\Omega$.\\
			
			One can compute a windowed scattering transform by iterating on the \Important{one-step propagator $U$} defined by,
			
			\begin{defn}  \textbf{One-step propagator}\\
				The one-step propagator $U$ can be defined as,
				\begin{equation}
					\forall x \in \mathcal{L}^{2}(\dsR^{d}) \;\;\; U_{J} x = \{ A_{J}x , \; (U[\lambda]x)_{\lambda \in \Lambda_{J}} \},
					\label{eq:One step propagator}
				\end{equation}
				
				with $A_{J}x = x \ast \phi_{2J}$ and $U[\lambda]x = \abs{x \ast \psi_{\lambda}}$.
				\label{def:One step propagator}
			\end{defn}
			
			Indeed after calculating $U_{J}x$, applying $U_{J}$ again to each $U{\lambda}x$ yields a larger infinite family of functions. Furthermore since $U[\lambda]U[p] = U[p+\lambda]$ and $ A_{J}U[p] = S_{J}[p]$ it holds that,
			
			\begin{equation}
					\forall x \in \mathcal{L}^{2}(\dsR^{d}) \;\;\; U_{J}U[p] x = \{ S_{J}[p]x , \; (U[p+\lambda]x)_{\lambda \in \Lambda_{J}} \},
					\label{eq:SCN propagation 1}
			\end{equation}
			
			Let $\Lambda_{J}^{m}$ be a set of path of length $m$ with the convention $\Lambda_{J}^{0} = \{ \emptyset \}$, its propagation is,
			
			\begin{equation}
					\forall x \in \mathcal{L}^{2}(\dsR^{d}) \;\;\; U_{J}U[\Lambda_{J}^{m}] x = \{ S_{J}[\Lambda_{J}^{m}]x , \; (U[\Lambda_{J}^{m+1}]x)_{\lambda \in \Lambda_{J}} \},
					\label{eq:SCN propagation 2}
			\end{equation}
			
			Hence $S_{J}[\mathcal{P}_{J}]x$ can be computed from $x= U[\emptyset]x$ by iteratively computing $U_{J}U[\Lambda_{J}^{m}] x$ for $m$ going from $0$ to $\infty$. This iterative process is illustrated in Figure~\ref{fig:SCN} and one can notice that the scattering calculation as the same general architecture as the convolutional neural networks introduced by LeCun \textbf{\textit{cite LeCun convNet 11 of mallat long paper}}. Both CNN and scattering convolutional network (SCN) cascade convolutions and a ``pooling'' non linearity. However while convolution networks use kernel filters learned from the data with back-propagation algorithm, SCNs use a fixed wavelet filter bank. 
			%TODO: more on the topic from the roto translation paper

      \begin{figure}
				\begin{center}
					\includegraphics[width=3.5in]{placeholder.jpg}
					\caption[The scattering convolutional network architecture]{see mallat long paper p1341}
					\label{fig:SCN}
				\end{center}	
      \end{figure}
      
      
	\section{Properties of the scattering transform:}
		\label{seq:ST/Pties}
		
		This section provides a formal version of the previously mentioned properties of the scattering transform.

		\subsection{Non-expansivity:}
			\label{subseq:ST/Pties/Non-expansivity}
			%TODO-DSTL: no proof.
			%TODO-UCL: add proof of the property (in line as it is short)
			
			The scattering propagator $U_{J} x = \{ A_{J}x , \; (\abs{W_{J}x})_{\lambda \in \Lambda_{J}} \}$ results of the composition of a wavelet transform $W_{J}$ that is unitary and of a modulus operator that is non-expansive - as $\forall (a,b) \in \dsC^{2} \abs{\abs{a}-\abs{b}} \leq \abs{a-b}$- and is thus non-expansive. Since $S_{J}[\mathcal{P}_{J}]$ iterates on $U_{J}$, which is non-expansive, the proposition (\textbf{\textit{cite mallat's long report 12}}) proves that $S_{J}[\mathcal{P}_{J}]$ is also non-expansive.
			
			\begin{prop} \textbf{Non-expansivity}\\ 
				The scattering transform is non expansive.
				
				\begin{equation}
				  \forall (x,z) \in \mathcal{L}^{2}(\dsR^{d})^{2} \;\;\; \norm{S_{J}[\mathcal{P}_{J}]x - S_{J}[\mathcal{P}_{J}]z} \leq \norm{x-z}
				\end{equation}

			  \label{pty:Nonexpansivity}
			\end{prop}

		\subsection{Energy preservation:}
			\label{subseq:ST/Pties/Energy}
			%TODO-DSTL: no proof.
			%TODO-UCL: add (proof for theorem ref available in annexe ref \textbf{\textit{proof in annexe}})
			
			We have seen (\textbf{\textit{reference to previous section if true or TODO}} that each propagator $U[\lambda]x = \abs{f \ast \psi_{\lambda}}$ captures the frequency energy contained in the signal $x$ over a frequency band covered by the Fourier transform $\hat{\psi}_{\lambda}$ and propagates this energy towards lower frequencies. We can thus prove  that under some assumption on the wavelet, the whole scattering energy ultimately reaches the minimum frequency $2^{-J}$ and is trapped by the low-pass filter $\phi_{2^{J}}$. Thus the energy propagated by a -windowed- scattering transform goes to $0$ as the path length increases, implying that $\norm{S_{J}[\mathcal{P}_{J}]} = \norm{x}$\\
			
			But first we need to define what an \Important{admissible scattering wavelet} is.
			
			\begin{prop} \textbf{Admissible scattering wavelet}\\ 
				A scattering wavelet $\psi$ is admissible if there exist $\eta \in \dsR^{d}$ and $\rho \geq 0$, with $\abs{\hat{\rho}(\omega)} \leq \abs{\hat{\phi}(2\omega)}$ and $\hat{\rho}(\omega)=0$, such that the function,
				
				\begin{equation}
					\hat{\Psi}(\omega) = \abs{\hat{\rho}(\omega - \eta)}^{2} - \sum_{k=1}^{+\infty}k(1-\abs{\hat{\rho}(2^{-k}(\omega - \eta)}^{2}),
				\end{equation}
				
				satisfies,
				
				\begin{equation}
				  \alpha = \inf_{1 \leq \abs{\omega}\leq 2} \sum_{j=-\infty}^{+\infty}\sum_{r \in G} \hat{\Psi}(2^{-j}r^{-1}\omega) \abs{\hat{\psi}(2^{-j}r^{-1}\omega)}^{2} > 0.
				\end{equation}
				
				\label{pty:Admissible wavelet}
			\end{prop}

			We can now state,
			
			\begin{thm} \textbf{Energy conservation}\\ 
				If the scattering wavelet $\psi$ is admissible, then for all signal $x \in \mathcal{L}^{2}(\dsR^{d})$, %TODO: define \Lambda
				
				\begin{equation}
				  \lim_{m \rightarrow +\infty} \norm{U[\Lambda_{J}^{m}]x}^{2} = \lim_{m \rightarrow +\infty} \sum_{n=m}^{+\infty} \norm{S_{J}[\Lambda_{J}^{n}]x}^{2} = 0,
					\label{eq:energy conservation 1}
				\end{equation}
				
				and
				
				\begin{equation}
				  \norm{S_{J}[P_{J}]x}^{2} = \norm{x}.
				  \label{eq:energy conservation 2}
				\end{equation}
				\label{thm:Energy conservation}
			\end{thm}

			The proof of the theorem~\ref{thm:Energy conservation} also shows that the scattering energy propagates progressively towards lower frequencies and that \Important{the energy of $U[p]x$ is mainly concentrated along frequency decreasing paths $p=(\lambda_{k})_{k\leq m}$}, i.e. for which $\abs{\lambda_{k+1}} \leq \abs{\lambda_{k}}$. The energy contained in the other paths is negligible and thus for the rest of the paper \Important{only frequency decreasing path will be considered}.\\
			
			The decay of $\sum_{n=m}^{+\infty} \norm{S_{J}[\Lambda_{J}^{n}]x}^{2}$ implies that there exist a path length $m > 0$ after which all longer path can be neglected. For signal processing applications, this decay appears to be exponential. And \Important{for classification applications path of length $m = 3$} provides the most interesting results \textbf{\textit{cite mallat's long paper 1 3}}.
			
			The two restrictions stated above yields to an easier parametrization of a scattering network. Indeed when only the frequency decreasing paths up until a given order a scattering network is completely defined by:
			\begin{itemize}
			  \item $M$: The maximum path length considered.
			  \item $J$: The coarsest scale level considered.
			  \item $L$: The number of orientation considered, which can be define as the cardinality of the previously define $G$.
			\end{itemize}
			
			Hence for a given set of parameter $(M,J,L)$, one can generate one and only one scattering network -with frequency decreasing paths. Each node of this network generates a -possibly empty- set of of nodes of size,
			
			 %TODO: add formula of number of child node.
			 
			
		\subsection{Translation invariance:}
			\label{subseq:ST/Pties/Translation}
			%TODO-DSTL: no proof.
			%TODO-UCL: no proof neither -maybe an intuition.
			
			The translation invariance of the scattering transform $S_{J}[\mathcal{P}_{J}]$ can be proved for a limit metric when $J$ goes to infinity. To do so one can first prove that the scattering distance $\norm{S_{J}[\mathcal{P}_{J}]x - S_{J}[\mathcal{P}_{J}]z}$ converges when $J$ goes to infinity - as it is non-increasing when $J$ increases (see section~\ref{subseq:ST/Pties/Non-expansivity}). From there one can bound the distance between the scattering transform of the signal and the one of its translated version $\norm{S_{J}[ S_{J}[\mathcal{P}_{J}]\mathcal{L}_{c}x - \mathcal{P}_{J}]x}$ and prove that this bound tends to $0$ when $J$ goes to infinity. This yields to the following theorem,
			
			\begin{thm} \textbf{Translation invariance}\\
				For admissible scattering wavelets,
				
				\begin{equation}
					\forall x \in \mathcal{L}^{2}(\dsR^{d}), \; \forall c \in \dsR^{d} \;\;\; \lim_{J \rightarrow \infty} \norm{S_{J}[ S_{J}[\mathcal{P}_{J}]\mathcal{L}_{c}x - \mathcal{P}_{J}]x} = 0
					\label{eq:Tranlation invariance}
				\end{equation}
				\label{thm:Translation invariance}
			  
			\end{thm}

			\begin{note}
			  A formal proof of~\ref{thm:Translation invariance} can be found in \textbf{\textit{cite Mallat's long paper}}.
			\end{note}
	
		\subsection{Lipschitz continuity to the action of diffeomorphisms:}
			\label{subseq:ST/Pties/Lipschitz continuity}
			%TODO-DSTL: no proof.
			%TODO-UCL: no proof neither -maybe an intuition.
			%TODO: change for the version from ISCN
			
			The Lipschitz continuity to the action of diffeomorphisms of $\dsR^{d}$ can be proved for those sufficiently close to a translation. Such diffeomorphisms maps $u$ to $u-\tau(u)$ where $\tau(u)$ is a displacement filed such that $\norm{\nabla \tau}_{\infty} < 1$. Let $L_{\tau}x(u)=x(u-\tau(u))$ denote the action of such diffeomorphisms on the signal $x$. Once again one can find an upper bound to the distance between the scattering transform of the signal and the one of its deformed version $\norm{S_{J}[\mathcal{P}_{J}]\mathcal{L}_{\tau}x - S_{J}[\mathcal{P}_{J}]x}$. With a bit of work on this bound one can then proved that the consequences of the action of $L_{\tau}$ is bounded by a translation term proportional to $2^{-J} \norm{\tau}_{\infty}$ and a deformation error proportional to $\norm{\nabla \tau}_{\infty}$. Finally some more work on the bounding term yields to the theorem,
			
			\begin{thm} \textbf{Lipschitz continuity to the action of diffeomorphisms} \\
			  There exists $C$ such that all $x \in \mathcal{L}(\dsR^{d})$ with $\norm{U[\mathcal{P}_{J}]x}_{1} < \infty$ and all $\tau \in \mathcal{C}^{2}(\dsR^{d})$ with $\norm{\nabla \tau}_{\infty} < \frac{1}{2}$ satisfy,
			  
			  \begin{equation}
					\norm{S_{J}[\mathcal{P}_{J}]\mathcal{L}_{\tau}x - S_{J}[\mathcal{P}_{J}]x + \tau . \nabla S_{J}[\mathcal{P}_{J}]x} \leq C \norm{U[\mathcal{P}_{J}]x}_{1} K(\tau),
					\label{eq:Lipschitz continuity 1}
			  \end{equation}

			  with
			  
			  \begin{equation}
					K(\tau) = 2^{-2J} \norm{\tau}_{\infty}^{2} + \norm{\nabla \tau}_{\infty} \left(\max \left( \log \frac{\norm{\Delta \tau}_{\infty}}{\norm{\nabla \tau}_{\infty}},1 \right) \right) + \norm{H\tau}_{\infty} 
					\label{eq:Lipschitz continuity K(tau)}
			  \end{equation}
			  
			  \label{thm:Lipschitz continuity}
			\end{thm}
			
			\begin{note}
			  Again a formal proof of~\ref{thm:Translation invariance} can be found in \textbf{\textit{cite Mallat's long paper}}.
			\end{note}

			\begin{rem}
				If the case where $2^{J} \gg \norm{\tau}_{\infty}$ and $\norm{\nabla \tau}_{\infty} + \norm{H \tau}_{\infty} \ll 1$, then $K(\tau)$ becomes negligible and the displacement field $\tau(u)$ can be estimated at each $u \in \dsR^{d}$. This can be done by solving the linear equation resulting from~\ref{eq:Lipschitz continuity 1} under the assumptions mentioned above,
				
				\begin{equation}
					\forall p \in \mathcal{P}_{J} \norm{S_{J}[p]\mathcal{L}_{\tau}x - S_{J}[p]x + \tau . \nabla S_{J}[p]x} \approx 0.
					\label{eq:Lipschitz continuity 2}  
				\end{equation}

				\textbf{\textit{Find reference for Application of displacement field estimation - best if not video}}
			\end{rem}
			
%       \subsubsection{The first order scattering coefficient:}
% 	\label{subsubseq:ST/SCN/SCN/1st order}
% 
%       A scattering transform computes higher-order coefficients by further iterating on the wavelet transform/modulus operators. At a maximum scale $2^{J}$, wavelet coefficients are computed at frequencies $2^{j} \geq 2^{-J}$, and lower frequencies are filtered by $\phi_{2^{J}} =  2^{-2J} \phi(2^{-J}u)$.\\
%       
%       In the image processing case, as images are real-valued signal, one can only consider the ``positive'' rotations $r \in G^{+}$ with angles $[0, \pi)$:
%       
%       \begin{equation}
% 				\label{eq:positive WT}
% 				W_{J}x(u)
%       \end{equation}

      
      
%     \subsection{Spatial Wavelet transform:}
%       \label{subseq:ST/RT ST networks/Spatial WT}
%       Introduction of the translation invariance.
% 		
%     \subsection{Roto-translation wavelet transform:}
%       \label{subseq:ST/RT ST networks/RT WT}
%       Introduction of the pseudo rotation invariance.       
 
		
  \section{Application to classification:}
    \label{seq:ST/Applications to clf}
    %TODO: Examples of Mallat's work.
    
    The scattering transform has been designed in order to provide a ``good'' representation for signal classification. The classical way of using it for such a task is to use the features generated by the scattering transform of the dataset considered as inputs for a discriminative classifier (e.g. Support Vector Machine classifier). Using this model provides results able to compare with state of the art CNNs on some standard datasets \textbf{\textit{cite mallat}}. \\
    
    The energy contained in the scattering transform is mostly contained in the short frequency decreasing paths (see~\ref{subseq:ST/Pties/Energy}). Thus for most of the classification path of length $m=3$ provides the best trade out between classification performance and computational time. Unless stated otherwise this length will be use in all our experiments.
    
    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Probabilistic graphical models:}
  \label{chap:PGMs}
  
  Those models use a \Important{graph based representation of conditional dependence between a set of random variables} and thus encode a complete distribution over a multi-dimensional space in a compact -or factorized- graph.The field of PGMs can be split into two main families, the \Important{Bayesian Networks} (BNs) and the \Important{Markov models} (MMs). As they are graphical models, both families encompass the properties of factorization and independences defined by the graph, but differ when it comes to the specificities of the set of independences they can encode as well as the factorization of the distribution that they can induce (  \textbf{\textit{cite: Bishop, Christopher M. (2006). "Chapter 8. Graphical Models" (PDF). Pattern Recognition and Machine Learning. Springer. pp. 359â€“422. ISBN 0-387-31073-8. MR 2247587}}).\\
  
  Add par on how they will be used.\\ %TODO: how they will be used
  
  The aim of this section is not to provide a complete overview of the Probabilistic graphical models but rather to introduce some interesting concepts that have been used in our work. Someone with more interest in PGMs could refer to \textbf{\textit{cite A tutorial on learning BN}} or \textbf{\textit{cite cours dafney koller}}. 
  This chapter introduces those two main classes of probabilistic graphical models. Section~\ref{seq:PGMs/BN} focuses on the Bayesian networks, while section~\ref{seq:PGMs/MM} provides more details about the Markov models.
	
  \section{Bayesian Network:}  
    \label{seq:PGMs/BN}
    A BN is subclass of probabilistic graphical model where the set of random variables and their conditional dependencies are expressed via a \Important{directed acyclic graph} (DAG). The architecture of the Bayesian Networks is further explained in section~\ref{subseq:PGMs/BN/Architecture}. Then section~\ref{subseq:PGMs/BN/Learning} presents a brief overview of the state of the art in terms of learning algorithm for BNs and section~\ref{subseq:PGMs/BN/Inference} describes the inference mechanism for those networks.
        
    \subsection{Architecture:}
      \label{subseq:PGMs/BN/Architecture}
      
      Bayesian networks can be defined as,
      
      \begin{defn} \textbf{Bayesian Network}\\
				For a set of random variables $\bfX = \{X_{i}\}_{i \in \llbracket1,N \rrbracket}$ a Bayesian network consists of a \Important{direct acyclic graph} $\mathcal{G}$ encoding a set of conditional independence assertions about the random variables in $\bfX$ and  a set $P$ of \Important{local probability distribution associated with each variable}. Each node of $\mathcal{G}$ represent one of the random variable $X_{i}$ and each edge $E_{i \rightarrow j}$ represents the conditional dependence between the nodes $X_{i}$ and $X_{j}$.
				\label{def:BN}
      \end{defn}
     
			\begin{figure}
				\begin{center}
					\includegraphics[width=3.5in]{placeholder.jpg}
					\caption{Example of Bayesian network.} %TODO: graph from http://www.cse.unsw.edu.au/~cs9417ml/Bayes/Pages/Bayesian_Networks_Definition.html
					\label{fig:Eg BN}
				\end{center}
			\end{figure}

			For such networks the following property holds,
			
			\begin{prop} \textbf{Conditional independence for Bayesian networks}\\ %TODO: quote [Nilsson, 1998] [Nilsson, 1998] Nilsson N,, Artificial Intelligence: a new synthesis, Morgan Kaufmann,1998 
				In a Bayesian network each node of the graph are "conditionally independent of any subset of the nodes that are not descendants of itself given its parent".
				
				\begin{equation}
				  P(\{X_{i}\}_{i \in \llbracket1,N \rrbracket}) = \prod_{i=1}^{N} P(X_{i} | \rho(X_{i}))
				  \label{eq:BN CI}
				\end{equation}
				
				where $\rho(X_{i})$ are the parents of the node $X_{i}$. 
				\label{pty:CI for BNs}
			\end{prop}
			
			Thus in a Bayesian network a node with no parents is not conditioned on the other random variables. Such a node is called a \Important{prior probability}.\\

			By their architecture, Bayesian networks allow to simplify the computation of the joint probability distribution. For example for the network defined by figure~\ref{fig:Eg BN}, one can obtain $P(X_{1},X_{2},X_{3},X_{4},X_{5},X_{6})$ by the use of chain rules and theory on conditional independent,
			
			\begin{equation*}
			  \begin{split}
			    P(X_{1},X_{2},X_{3},X_{4},X_{5},X_{6})
						&= P(X_{6} | X_{3},X_{4},X_{5}) P(X_{1},X_{2},X_{3},X_{4},X_{5}) \\
						&= P(X_{6} | X_{3},X_{4},X_{5}) P(X_{3} | X_{1},X_{5}) P(X_{4}| X_{2}) P(X_{5}| X_{2}) P(X_{1},X_{2})\\
						&= P(X_{6} | X_{3},X_{4},X_{5}) P(X_{3} | X_{1},X_{5}) P(X_{4}| X_{2}) P(X_{5}| X_{2}) P(X_{2}|X_{1}) P(X_{1}).\\
			  \end{split}
			  \label{eq:Eg BN CI}
			\end{equation*}
      
%       are directed acyclic graphs whose nodes represent random variables in the Bayesian sense. Those nodes can be observable quantities, latent variables, unknown parameters or hypotheses. Edges of the graph represent conditional dependencies between the variables.  nodes that are not connected represent variables that are conditionally independent of each other. Each node is associated with a probability function that takes, as input, a particular set of values for the node's parent variables, and gives (as output) the probability (or probability distribution, if applicable) of the variable represented by the node. For example, if m parent nodes represent m Boolean variables then the probability function could be represented by a table of 2^m entries, one entry for each of the 2^m possible combinations of its parents being true or false

    \subsection{Learning:}
      \label{subseq:PGMs/BN/Learning}
      
      In some application the structure $\mathcal{G}$ and the set $P$ of local probability distribution associated with each variable of the network are provided. In such a case the BNs are ``simply'' used for inference. The task is then to infer the most probable values for a subset $\bfY \subset \bfX$, given a partially complete set of realizations $\bfX_{obs} \subset \bfX \backslash \bfY$.\\
      
      However, most of the time the full characterization of the BN is not provided. Ignoring the case of missing data, one can split the learning problem into two main categories:
      \begin{itemize}
				\item \Important{Local probability distributions:} In this case the structure of the graph $\mathcal{G}$ is known and fixed before hand. It can be provided by an expert (e.g. Microsoft trouble shooting system \textbf{\textit{cite}}) or be imposed by some construction rules (e.g. Boltzmann Machine \textbf{\textit{cite}}, Restricted Boltzmann Machine \textbf{\textit{cite}} ...). The task at end is then to learn the parameters governing the local probability distributions of the Bayesian network.
        \item \Important{Architecture and local probability distributions:} In this case the architecture of the network has to be learned along side with the local probability distributions. This problem is not developed in the rest of this paper, but one could refer to \textbf{\textit{cite A tutorial on learning BN}} for an introduction to this problem.
      \end{itemize}
      
      \subsubsection{Expectation-maximization:}
      
      \subsubsection{Variational Bayes:}

    \subsection{Inference:}
      \label{subseq:PGMs/BN/Inference}
      \url{http://www.cse.unsw.edu.au/~cs9417ml/Bayes/Pages/Bayesian_Networks_Inference.html}
      
  \section{Markov Models:}
    \label{seq:PGMs/MM}
    Quick overview over the main methods for HMMs.
    
    \subsection{Architecture:}
      \label{subseq:PGMs/MM/Architecture}
      TBD

    \subsection{Learning:}
      \label{subseq:PGMs/MM/Learning}
      TBD

    \subsection{Inference:}
      \label{subseq:PGMs/MM/Inference}
      TBD
      
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \chapter{Hidden Markov trees:}
%   \label{chap:HMT}
%     More at length description of the specific properties of HMTs.
% 	
%   \section{The tree structure:}
%     \label{seq:HMT/Tree:}
%     TBD
% 
%   \section{Learning:}
%     \label{seq:HMT/EM}
% 
%     \subsection{Expectation maximization:}
%       \label{subseq:HMT/EM}
%       EM by Crouse and EM by Durand
%       
%     \subsection{Variational methods:}
%       \label{subseq:HMT/Var}
%       Variational like Crouse and variational like Durand    
%   
%     
%   \section{Generation: Vitterbi algorithm:}
%     \label{seq:HMT/Generation}

		
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Scattering hidden Markov tree:}
  \label{chap:SHMT}
  Section~\ref{seq:ST/Applications to clf} introduced the usage of scattering networks combined with an support vector machine classifier to achieve state of the art classification performances on some problems. However this method provides a boolean answer for each class. Some methods to express the output of an SVM as a probability exists (\textbf{\textit{cite: Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods (1999)}} but they are just a rescaling of the output and not a true probabilistic approach. If one is interested in a true probabilistic model to describe the scattering coefficient it is quite natural to try to express them as a probabilistic graphical model. Indeed if one hide the propagation step from the scattering transform (see~\ref{seq:ST/SCN}) the scattering network defines a tree structure. \\

	\begin{figure}
			\begin{center}
				\includegraphics[width=3.5in]{placeholder.jpg}
				\caption{Scattering transform tree.} %TODO: SCHMT without hidden state/node
				\label{fig:ST tree}
			\end{center}
	\end{figure}
  
  To simplify the notation in the remainder of this section, let $\mcalT$ denotes the tree  structure defines as state above and depicted in figure~\ref{fig:ST tree}. Let $I$ denotes the total number of nodes -i.e. scattering coefficient and let $S_{i}$ for $i \in \llbracket0, I-1 \rrbracket$ denotes one of the node $S[p_{i}]x \in \mcalT$ for a given path $p_{i} = [\lambda_{0} ... \lambda_{u}]$ ($u\in \dsN$). Note that $S_{i}$ represents a node and does not depends on the signal $x$. For a given signal $x$, a realisation of the node $i$ is denoted by $S_{i}= s_{i} = S[\lambda_{i}]x$ where $\lambda_{i}$ denotes the path in the scattering tree to the node $i$. Note also that in the remainder of the paper the short notation $i \in \mcalT$ will be use to denote $\lambda_{i}$. Let also use the convention $S_{0} = S[\emptyset]x$. Finally let $\rho_{i}$ and $\mcalC(i)$ denote respectively the parent of a node $i$ and the set of children of the node $i$. Note also that a node $S_{i}$ can have no children, in such a case this node is a leaf of the tree.\\
  
  The remainder of this section is organized as follows: Section~\ref{seq:SHMT/Rel work} introduces related work and provide a description of the SCHMT model. Section~\ref{seq:SHMT/Hypos} details the hypothesis needed to develop this model as well as provides some intuition on their validity. Finally section~\ref{seq:SHMT/Learning} and~\ref{seq:SHMT/Clf} respectively describe the learning algorithm for the parameters of the model and the classification method.

  
  \section{SCHMT model and related works:}
		%TODO1: add to the scattering transform section a simple notation for the scattering coef
		%TODO2: modify this section accordingly
    \label{seq:SHMT/Rel work}
      
    The idea behind the SCHMT model is to say that the more detailed representation of the signal is somehow correlated to the less detailed one from which it is generated. More formally this means that for a signal $x$, $S_{i}$ is somehow correlated to $S_{\rho(i)}$. To do so one could model the scattering network by a Markov tree and assumes the following:
    
    \begin{equation}
      P(S_{i} |\mcalT) = P(S_{i} | S_{\rho(i)}).
      \label{eq:SMT - false hypo}
    \end{equation}\\

    Those independence properties yield to the graph displays in figure~\ref{fig:ST tree}. However models trying to describe directly the correlation across coefficients at different scales have been studied for traditional wavelet transforms \textbf{\textit{cite: in Crouse previous work}} and it seems that a simple one-step Markovian assumption across scale is notsufficient to describe the complex relationship between wavelet coefficients.\\
    
    A common approach when a direct Markovian model does not hold is to introduce hidden states and to assume the Markovian property across those states. The observed nodes are then only dependent on their state. This is the architecture adopted for the SCHMT and its graph is represented in figure~\ref{fig:SCHMT 1}. This model has the following independence properties,
    
    \begin{equation}
      P(H_{i} |\mcalT) = P(H_{i} | H_{\rho(i)}),
      \label{eq:SCHMT - hypo 1}
    \end{equation}    
    \begin{equation}
      P(S_{i} |\mcalT) = P(S_{i} | H_{i}).
      \label{eq:SCHMT - hypo 2}
    \end{equation} \\
    
		\begin{figure}
				\begin{center}
					\includegraphics[width=3.5in]{placeholder.jpg}
					\caption{Scattering Hidden Markov Tree.} %TODO: SCHMT
					\label{fig:SCHMT 1}
				\end{center}
		\end{figure}
		
		As the scattering transform is closely related to wavelet transforms it is not surprising to find similar ideas exploited for wavelet trees. MS. Crouse proposed where an Hidden Markov Tree Model is used to model the wavelet coefficients \textbf{\textit{cite: Crouse}} of a classic wavelet trees. Later N. Kingsbury \textbf{\textit{cite: Kingsbury}} adapted MS. Crouse's model to his Dual wavelet complex trees. The resulting hidden Markov tree models provides better classification performances than MS. Course's WHMT as the wavelet used generate a ``better'' representation of the signal -in the sense defined in section~\ref{seq:Intro/Image rep/Intuition}. Indeed this version can leverage the quasi-translation invariance property of the complex wavelets. The improvement in performances due to the quasi-invariance property provides a good motivation to try the hidden Markov tree modeling on the scattering transform as they have even ``better'' representational properties (see~\ref{seq:ST/Pties}. The parameters of the original WHMT were trained using a version of the \Important{Expectation-Maximization} adapted to binary hidden Markov trees. However this learning method suffered from overflowing issues \textbf{\textit{cite: ? - Durand?}}, JB. Durand \textbf{\textit{cite: Durand}} proposed a smoothed version of the training algorithm preventing overflowing.\\
   
		\begin{figure}
				\begin{center}
					\includegraphics[width=3.5in]{placeholder.jpg}
					\caption{Wavelet hidden Markov tree.} %TODO: course's model
					\label{fig:WHMT}
				\end{center}
		\end{figure}
	  
	  A scattering hidden Markov tree is composed by a set of visible nodes $\{\bfS_{i}\}_{i \in \mcalT}$ and a set of of hidden node $\{\bfH_{i}\}_{i \in \mcalT}$. Then the tree is parametrised as follow,
	  
    \begin{itemize}
      \item For any index $i$ of the tree, $S_{i} \in \dsR^{d}$ and $H_{i} \in \llbracket 1,K \rrbracket$ where $K$ is the number of possible hidden states.
      
      \item The initial hidden state is drawn from a discrete non uniform \Important{initial distribution} $\pi_{0}$ such that:
	  	  \begin{equation}
					\forall k \in \llbracket 1,K \rrbracket \;\;\; \pi_{0}(k) = P(H_{0}=k).
					\label{eq:SCHMT - initial distribution}
				\end{equation}
				
      \item For any index $i$ of the tree, the \Important{emission distribution} describe the probability of the visible node $S_{i}$ conditional to the hidden state $H_{i}$,
				\begin{equation}
				  \forall i \in \mcalT \;\; \forall k \in \llbracket1,K\rrbracket \;\; \text{and} \;\; \forall s \in \dsR
					\;\;\;\;\;\;P(S_{i}=s|H_{i}=k) = P_{\theta_{k,i}}(s)
					\label{eq:SCHMT - Emission distr}
				\end{equation}
				Where $P_{\theta}$ belongs to a parametric distribution family and where $\theta$ is the vector of \Important{emission parameters}. In the remainder of the paper the emission distribution is a Gaussian so that,  
      	\begin{equation}
				  \forall i \in \mcalT \;\; \forall k \in \llbracket1,K\rrbracket \;\; \text{and} \;\; \forall s \in \dsR
					\;\;\;\;\;\; P(S_{i}=s | H_{i}=k) = \mcalN(\mu_{k,i},\sigma_{k,i}).
				  \label{eq:SCHMT - Mixt of Gaussian}
				\end{equation}
				where $\theta_{k,i}=(\mu_{k,i},\sigma_{k,i})$ where $\mu_{k,i}$ and $\sigma_{k,i})$ are respectively the mean and the variance of the Gaussian for the $k$-th value of the mixture and the node $i$.
				
			\item For any index $i$ of the tree, the probability to move from one state to the other in characterized by a \Important{transition probability},
				\begin{equation}
					\forall i \in \mcalT \backslash \{ 0 \} \;\;\; \forall g,k \in \llbracket1,K\rrbracket^{2}
					\;\;\;\;\;\; \epsilon_{i}^{(gk)} = P(H_{i}= k | H_{\rho(i)}=g), 
					\label{eq:SCHMT - Transition matrix}
				\end{equation}
				where $\epsilon_{i}$ defines a transition probability matrix such that,
				\begin{equation}
				  \forall i \in \mcalT \backslash\{0\} \;\;\;  \forall k \in \llbracket1,K\rrbracket
					\;\;\;\;\;\;  P(H_{i}=k) = \sum_{g=1}^{K} \epsilon_{i}^{(gk)} P(H_{\rho(i)}=g). %TODO: Check if it is conditional or not.
				  \label{eq:SCHMT - State proba 1}
				\end{equation}
				And using the chain rule of probability one can express $P(H_{i})$ from the root node's initial distribution.\\
		\end{itemize}
		
		The SCHMT model is thus fully described by,
		\begin{equation}
			\Theta = \big(\pi_{0}, \{ \epsilon_{i}, \{ \theta_{k,i} \}^{k\in\llbracket1,K\rrbracket} \}_{i\in\mcalT}\big).
			\label{eq:SCHMT - parameters}
		\end{equation}
		
		
    The SCHMT model differs from the previous work by the shape of its tree structure. Previous work are based on regular binary tree structures where all the leafs have the same depth the scattering tree is a irregular tree. As seen in section~\ref{seq:ST/SCN} each node has $TODO$ children. This yields to an architecture where each node have a different number of children and where leafs can be found at any depth of the tree. However as detailed in section~\ref{seq:SHMT/Learning}, the learning algorithm can be adapted to this case. Another difference between SCHMT and the previous works is the non-homogeneity of the transition matrix. Indeed by the nature of the scattering transform one can expect a non homogeneous transmission of the information across the orders and thus to accomodate this the learning algorithm has also be adapted to this case.\\
    
    Even though the theoretical framework of SCHMT holds for any $K \in \dsN^{\ast}$, in the remainder of the work $K$ is set to $2$. This means that the scattering coefficient are described by a mixture of two Gaussians and can be in \Important{two states, $(H)$ High or $(L)$ Low}. This model yields to sparser representations as the number of hidden states is highly constrained.
    
    
  \section{Hypothesis:}
    \label{seq:SHMT/Hypos}
    
    Expressing the dependencies between the scattering coefficients as an Hidden Markov Tree implies that two modeling assumptions are done. The first assumption reflects the fact that the scattering coefficients can effectively be expressed by two hidden states,\\
    
    \begin{assumption}\textbf{Two populations:}\\
			A signal's scattering transform can be described as two clusters of coefficients' value. The smooth regions are represented by small scattering coefficients, while edges, ridges, and other singularities are represented by large coefficients.\\
			\label{assum:2pop}
    \end{assumption}
    
    As this assumption is common for standard or complex wavelet (\textbf{\textit{cite: Kingsbury}} and because a scattering coefficient of order $m$ can simply be seen as the modulus of a ``new'' signal -i.e. the scattering coefficient of order $m-1$, the two-populations assumption for scattering network is reasonable. This intuition can be confirmed by plotting the scattering coefficients obtained for several signals.\\
    
		\begin{figure}
				\begin{center}
					\includegraphics[width=3.5in]{placeholder.jpg}
					\caption[Two populations - experiment 1.]{Two populations - experiment 1: The signal is a binary square (0: background, 1: square) with noise. The scattering network has $m=3$ layers, $J=3$ scales and $L=2$ orientations.} %TODO: plots from exp_two_populations from the square
					\label{fig:2pop - 1}
				\end{center}
		\end{figure}    
		
		\begin{figure}
				\begin{center}
					\includegraphics[width=3.5in]{placeholder.jpg}
					\caption[Two populations - experiment 2.]{Two populations - experiment 1: The signal is the picture of Lena. The scattering network has $m=3$ layers, $J=3$ scales and $L=2$ orientations.} %TODO: plots from exp_two_populations from Lena
					\label{fig:2pop - 2}
				\end{center}
		\end{figure}
		
		Figure~\ref{fig:2pop - 1} displays the scattering coefficients of a noisy binary square. Note that for sake of clarity a ``small'' network has been used. This does not affect the observations that can be made and one can notice that the highest values of the scattering coefficient are obtained on highly informational pixels (edges in this case) while the low informational pixels are represented by scattering coefficients near $0$. Similar observations can be made for more complex signal -such as the one displays in~\ref{fig:2pop - 2}. A statistical interpretation of the two-populations implies that scattering coefficients have a non-Gaussian marginal statistics, that is, their marginal probability density functions have a large peak at zero (many small coefficients) and heavy tails (a few large coefficients). Finally since many real-world signals (photograph-like images, for example) consist mostly of smooth regions separated by a few singularities, the two populations assumption tells us that the scattering coefficients are a sparse representation for these signals (this notion of sparsity can be made mathematically precise; see \textbf{\textit{cite: Kingsbury - 4, 5]}} for example). Most of the scattering coefficient magnitudes (representing the smooth regions) are small, while a few of them (representing the singularities and encoding the informational content) are large.\\
		
		The second assumptions transcripts the smoothness of the states across the scattering transform and can be stated as,\\
		
		\begin{assumption}\textbf{Persistence:}\\
		  Along a scattering path high and low scattering coefficient values cascade across the scattering orders.\\
		  \label{assum:Persistence}
		\end{assumption}

		This assumption codifies how the hidden states are structured. Smooth regions/singularities are represented by low/high values at every order. Persistence leads to scattering coefficient values that are statistically dependent along the branches of the scattering tree. Figure~\ref{fig:Persistence - 1} displays the magnitude of the scattering coefficient for a given node $i$ of the tree against those of its father $\rho(i)$. A clear positive correlation can be observed between the magnitude of the father and the son. Furthermore the transitions can be regrouped in two main clusters: Low-Low and High-High. This tends to confirm the validity of the persistence assumption. A second experiment computes the average correlation between each pair father/child of the tree for signal belonging to different classes. Again this experiments advocates in favor of a correlation in between the magnitude of the connected nodes.  
		
		\begin{figure}
				\begin{center}
					\includegraphics[width=3.5in]{placeholder.jpg}
					\caption[Persistence - experiment 1.]{Persistence - experiment 1: Plot the magnitude of the scattering coefficients at a given index $i$ of the tree against those of its father $\rho(i)$.} %TODO: plots from exp_correlation
					\label{fig:Persistence - 1}
				\end{center}
		\end{figure} 
		
		\begin{figure}
				\begin{center}
					\includegraphics[width=3.5in]{placeholder.jpg}
					\caption[Persistence - experiment 2.]{Persistence - experiment 2:.} %TODO: plots from exp_correlation_set
					\label{fig:Persistence - 2}
				\end{center}
		\end{figure}
		
		
  \section{Learning the tree structure:}
    \label{seq:SHMT/Learning}
    
    As seen in~\ref{subseq:PGMs/MM/Learning} training Markov  models can be done using \Important{Expectation-Maximization} methods. Hidden Markov chains use a version of the EM algorithm called \Important{Forward-Backward} algorithm allowing the propagation of the hidden states along the chain. MS. Crouse \textbf{\textit{cite: Crouse}} proposed the \Important{Upward-Downward algorithm}, an adaptation to the hidden Markov trees of the Forward-Backward algorithm. Both algorithm were suffering from overflowing problems \textbf{\textit{cite: Durand 9 }} and JB. Durand \textbf{\textit{cite: Durand}} adapted \textbf{\textit{name + cite: Durand 7}} work to create a smoothed version of the learning algorithm for trees. This section proposes a rewriting of JB. Durand's version of the Upward-Downward algorithm adapted to non-regular non-binary HMTs.\\
       
    To do so one needs to introduce the following notations,
    \begin{itemize}
      \item $\forall i \in \mcalT$ let $\bar{\mcalS}_{i}= \bar{s}_{i}$ be the observed subtree rooted at node $i$. By convention $\bar{\mcalS}_{0}$ denotes the entire observed tree.
      \item $\forall i \in \mcalT$ let $\bar{\mcalS}_{c(i)}= \bar{s}_{c(i)}$ be the entire -possibly empty collection of observed subtrees rooted at children of node $i$ (i.e. the subtree $\bar{s}_{u}$ except its root $s_{u}$).
      \item If $\bar{\mcalS}_{i}$ is a proper subtree of $\bar{\mcalS}_{j}$, then $\bar{\mcalS}_{j\backslash i} = \bar{s}_{j\backslash i}$ is the subtree rooted at node $j$ except the subtree rooted at node $i$.
      \item $\forall i \in \mcalT$ let $\bar{\mcalS}_{1\backslash c(i)}= \bar{s}_{1\backslash c(i)}$ be the entire tree except for the subtrees rooted at children of node $i$.
    \end{itemize}
    Note that those notations transpose to the hidden state and for instance $\bar{\mcalH}_{i}= \bar{h}_{i}$ is the state subtree rooted at node $i$.\\
    %TODO: Figure for the cliques as Durand (if time)
    
    \subsection{E-Step:}
			\label{subseq:SHMT/Learning/E}
			The smoothed version of the E-step requires the computation of the conditional probability distribution $\xi_{i}(k) = P(H_{i}=k | \bar{\mcalS}_{i}= \bar{s}_{i})$ (smoothed probability) and $P(H_{i}=k, H_{\rho(i)}=g | \bar{\mcalS}_{i}= \bar{s}_{i})$ for each node $i \in mcalT$ and state $k$ and $g$. A decomposition of the smoothed probability adapted to the HMT structure is,
			
			\begin{equation}
			  \xi_{u}(k) = 
					\frac{P(\bar{\mcalS}_{1\backslash i}= \bar{s}_{1\backslash i} | H_{i}=k)}
						{P(\bar{\mcalS}_{1\backslash i}= \bar{s}_{i\backslash i} | \bar{\mcalS}_{1}= \bar{s}_{i})} 
					P(H_{i}= k |\bar{\mcalS}_{i}= \bar{s}_{i})
				\label{eq:Smoothed P decomposition}
			\end{equation}
			
			The smoothed upward-downward algorithm requires the introduction the following quantities,
			
			\begin{equation}
			  \beta_{i}(k) = P(H_{i}=k | \bar{\mcalS}_{i}= \bar{s}_{i})
			  \label{eq:UD beta node}
			\end{equation}
			\begin{equation}
			  \beta_{\rho(i)i}(k) = \frac{P(\bar{\mcalS}_{i}= \bar{s}_{i} | H_{\rho(i)}=k)} {P(\bar{\mcalS}_{i}= \bar{s}_{i}) }
			  \label{eq:UD beta node and parents}
			\end{equation}
						\begin{equation}
			  \alpha_{i}(k) = \frac{P(\bar{\mcalS}_{1\backslash i}= \bar{s}_{1\backslash i} | H_{\rho(i)}=k)} {P(\bar{\mcalS}_{1\backslash i}= \bar{s}_{1\backslash i} | \bar{\mcalS}_{i}= \bar{s}_{i} ) }
			  \label{eq:UD alpha}
			\end{equation}\\
			
			The smoothed upward-downward algorithm also requires the preliminary knowledge  of the marginal state distributions $P(H_{i}=k)$ for each node $i$. However this can simply be achieved by a downward recursion initialized for the root node with $P(H_{0}=k)=\pi_{0}(k)$ and then using the recursive formula~\ref{eq:SCHMT - State proba 1}.
			
			\subsubsection{Upward recursion:}
				The upward algorithm is initialized at all the leaf of the tree, by computing $\beta_{i}(k)$ using,
				
				\begin{equation}
					\begin{split}
						\beta_{i}(k)	&= P(H_{i}=k | \bar{\mcalS}_{i}= \bar{s}_{i})\\
													&= \frac{P(S_{i}= s_{i} | H_{i}=k)P(H_{i}=k)} {P(S_{i}= s_{i})}\\
													&= \frac{P_{\theta_{k,i}}(s_{i}) P(H_{i}=k)} {N_{i}},
						\end{split}
						\label{eq:upward beta leaf}
				\end{equation}
				
				where the normalization factor for the leaf nodes $N_{i}$ is given by,
				\begin{equation}
					N_{i}	= P(S_{i}= s_{i}) = \sum_{k=1}^{K} P_{\theta_{k,i}}(s_{i}) P(H_{i}=k).
					\label{eq:upward normalization leaf}
				\end{equation}
				
				Then one can recursively (upward recursion) compute $\beta_{i}(k)$ for the remaining nodes of the tree using,
				
				\begin{equation}
					\begin{split}
						\beta_{i}(k)	&= P(H_{i}=k | \bar{\mcalS}_{i}= \bar{s}_{i})\\
													&= \left[ \prod_{j \in c(u)} P(\bar{\mcalS}_{j}= \bar{s}_{j} | H_{i}=k) \right] P(S_{i}= s_{i} | H_{i}=k) \frac{P(H_{i}=k)} {P(\bar{\mcalS}_{i}= \bar{s}_{i})}\\
													&= \left[ \frac{\prod_{j \in c(u)} P(\bar{\mcalS}_{j}= \bar{s}_{j} | H_{i}=k)} {P(\bar{\mcalS}_{j}= \bar{s}_{j})} \right]
														P(S_{i}= s_{i} | H_{i}=k) P(H_{i}=k) 
														\frac{\prod_{j \in c(u)} P(\bar{\mcalS}_{j}= \bar{s}_{j})} {P(\bar{\mcalS}_{i}= \bar{s}_{i})}\\
													&= \frac{ \left[ \prod_{j \in c(i)} \beta_{ij}(k) \right] P_{\theta_{k,i}}(s_{i})P(H_{i}=k)}{N_{i}},
						\end{split}
						\label{eq:upward beta node}
				\end{equation}
				
				where the normalization factor for the non-leaf nodes $N_{i}$ is given by,
				\begin{equation}
					\begin{split}
						N_{i}	&= \frac{P(\bar{\mcalS}_{i}}{\prod_{j \in c(u)} P(\bar{\mcalS}_{j}= \bar{s}_{j})}\\
									&= \sum_{k=1}^{K} \left[ \prod_{j \in c(i)} \beta_{ij}(k) \right]  P_{\theta_{k,i}}(s_{i})P(H_{i}=k).
					\end{split}
					\label{eq:upward normalization others}
				\end{equation}

				For all node $i$, the quantities $\beta_{\rho(i)i}(k)$ can be extracted from $\beta_{i}$ using,
				
				\begin{equation}
					\begin{split}
						\beta_{\rho(i)i}(k)	&= \frac{P(\bar{\mcalS}_{i}= \bar{s}_{i} | H_{\rho(i)}=k)} {P(\bar{\mcalS}_{i}= \bar{s}_{i}) }\\
																&= \frac{ \sum_{l=1}^{K} P(\bar{\mcalS}_{i}= \bar{s}_{i} | H_{i}=l) P(H_{i}= k | H_{\rho(i)}=k)} {P(\bar{\mcalS}_{i}= \bar{s}_{i}) }\\
																&= \sum_{l=1}^{K} \frac{ P(H_{i}=l | \bar{\mcalS}_{i}= \bar{s}_{i} |)}{P(H_{i}=l)} P(H_{i}= l | H_{\rho(i)}=k)\\
																&= \sum_{l=1}^{K} \frac{\beta_{i}(l) \epsilon_{i}^{(kl)}}{P(H_{i}=l)}.
						\end{split}
						\label{eq:upward beta node and parents}
				\end{equation}
				
				Those relationships one can derive the the upward algorithm,
			
				\begin{center}
					\begin{algorithm}
						\textbf{Meta-parameters:}\\
							$K$\\
								
						\textbf{Initialization:}\\
							\tcp{$P_{\theta_{k,i}}(s_{i})$:}
							\For{All the node $i$ of the tree $\mcalT$}{
								$P_{\theta_{k,i}}(s_{i}) = \mcalN(s_{i} | \mu_{k,i},\sigma_{k,i})$
								}
								
							\tcp{Loop over the leafs $i$ of the tree:}
							\For{All the leaf $i$ of the tree $\mcalT$}{
								$\beta_{i}(k) = \frac{P_{\theta_{k,i}}(s_{i}) P(H_{i}=k)}{\sum_{l=1}^{K} P_{\theta_{l,i}}(s_{i}) P(H_{i}=l)}$\\
								$\beta_{i,\rho(i)}(k) = \sum_{l=1}^{K}\frac{\beta_{i}(l) \epsilon(k,l)}{P(H_{i}=l)} . P(H_{\rho(i)}=k)$ \\
								$l_{i} = 0$
								}
% 						
% 										
% 						\While{convergence}{
% 							- \textit{Descent direction:} \\
% 							$\bfp^{(\ell)} = -\bfB_{(\ell)}^{-1} \nabla U(f|\hat\beta^{(\ell)})$ \\
% 							
% 							- \textit{Optimal step in the direction $\bfp^{(\ell)}$:} \\
% 							$\mu^{(\ell)} = \argmin_{\mu \in \mathbb{R}} \left[
% 							U(f| (\bftheta_{(\ell)} + \mu \bfp^{(\ell)})^\top \bfrho) \right]$\\
% 							
% 							$\bftheta^{(\ell+1)} = \bftheta^{(\ell)} + \mu^{(\ell)} \bfp^{(\ell)}$\\      
% 							$\hat\beta^{(\ell+1)} = \bftheta^{(\ell) \top} \bfrho $ \\
% 							
% 							- \textit{Hessian matrix estimate:} \\
% 							$\bfeta^{(\ell)} = 
% 							\nabla U(f|\hat\beta^{(\ell+1)})-\nabla U(f|\hat\beta^{(\ell)})$\\
% 							
% 							$\bfB_{(\ell+1)} \!\! = \!\! \bfB_{(\ell)} + \displaystyle \frac{\bfeta^{(\ell)} \bfeta^{(\ell)\top}}
% 							{\mu^{(\ell)} \bfeta^{(\ell)\top} \bfp^{(\ell)}}
% 									- \frac{\bfB_{(\ell)} \bfp^{(\ell)} \bfp^{(\ell)\top} \bfB_{(\ell)}}
% 									{\bfp^{(\ell)\top} \bfB_{(\ell)} \bfp^{(\ell)}}$
% 									\\
% 							$l = l+1$
% 						}
						\caption{Smoothed upward algorithm}
						\label{algo:Smoothed upward}		
					\end{algorithm}        
				\end{center}

    
  \section{Classification:}
    \label{seq:SHMT/Clf}
    
    The 
    
    %TODO: MAP algo
    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Experimental results:}
  \label{chap:Experimental results}
  TBD
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion:}
  \paragraph{Scattering hidden Markov tree:}
    TBD
  \paragraph{Next steps:}
    Variational methods
    General graphical models
    Bayesian neural networks
    
    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Acknowledgements:}

	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%   
% Bibliography %
%%%%%%%%%%%%%%%%
\begin{thebibliography}{99}
  
\end{thebibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THE END
\end{document}