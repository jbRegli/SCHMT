%%%%%%%%%%%%%%%%%%%%%
% Type of document: %
%%%%%%%%%%%%%%%%%%%%%
\documentclass[a4paper,11pt]{report}

%%%%%%%%%%%%%%%%%%%%%
% My usual settings %
%%%%%%%%%%%%%%%%%%%%%
\usepackage{JB_config_article}

%%%%%%%%%%%%%%%%%%%%%%%
% Headers and footers %
%%%%%%%%%%%%%%%%%%%%%%%
%%% Color definition:
\definecolor{lgray}{gray}{0.6}

\usepackage{fancyhdr}
\renewcommand{\headrulewidth}{0pt}
\lhead{}
\rhead{}
\lfoot{\vspace{0.3cm}\small\color{lgray}Research report, UCL Department of Statistical Science}
\rfoot{\vspace{-0.3cm}\includegraphics[width=1cm]{placeholder.jpg}
\hspace{0.5cm}
\includegraphics[width=1.5cm]{placeholder.jpg}}
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Location of the figures %
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\graphicspath{{Images/}} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Start the document
\begin{document}

	%% Cover page:
	\begin{titlepage}
	%\vspace*{1cm}
		\hspace{-1.5cm}\includegraphics[width=9cm,height= 3cm]{placeholder.jpg}
		\hfill{
		  \raggedleft \includegraphics[width=3cm]{placeholder.jpg}
		}
		\vspace*{5cm}
	
		\begin{center}
			\begin{sc} 	
				%%% Title:
				\huge SCATTERING HIDDEN MARKOV TREES
				\vspace*{0.2cm}
				%%% Subtitle:
				\\ \large IMAGE REPRESENTATION AND SCATTERING TRANSFORM MODELING
				\vspace*{2cm}
			\end{sc}
			%%% Author:
			\\ \LARGE Jean-Baptiste Regli
			\vspace*{0.2cm}
			%%% Date
			\\ \large 2013-2014
		\end{center}
		%\vspace*{5cm}

		%%% Supervisors:
		\vfill
		\begin{center}
			\vspace*{1cm}
			\Large RESEARCH REPORT
			\vspace*{0.5cm}
			\\ \large Academic supervisor: James Nelson 
			\\ \large Sponsor: Dstl/UCL Impact studentship
			
			\vspace*{1cm}
			\Large UCL
			\\ \normalsize Department of Statistical Science
			\\ London
		\end{center}
	\end{titlepage}
	\clearpage

	\vfill	
		
	% Blank page
	\newpage
	\thispagestyle{empty}
	\mbox{}
 
	% Table of contents
	\setcounter{tocdepth}{3}
	\renewcommand{\contentsname}{Contents:}
	\tableofcontents
	\clearpage

	% List of figures
	\renewcommand{\listfigurename}{List of figures:}
	\listoffigures

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction:}
  \label{chap:Intro}
		
  \section{Image representation:}
    \label{seq:Intro/Image rep}
    %TODO: Properties of a ``good'' image representation for classification.
    The first part of this report will be dedicated to explaining the motivations as well as the construction process of the scattering transform (ST). To understand those motivations we will first provide an intuition of what a ``good'' representation of signal -for classification- is. We will then provide more formal mathematical definitions for our intuition and how this can be achieved.
    
    \subsection{Intuition of a ``good'' image representation:}
      \label{seq:Intro/Image rep/Intuition}
      One way to develop an intuition on what properties a ``good'' representation for classification have is to look at the human visual function and what he is able to tell apart.\\
      
      Based on that, we think our representation should be:
      
      \begin{itemize}
				\item \Important{Informative} enough to permit classification.\\
	
				\item \Important{Invariant to translations}. Indeed to a human eye there is no difference in the information carried by a signal if it is shifted.\\
		
				\begin{figure}
				  \begin{center}
				    \includegraphics[width=3.5in]{placeholder.jpg}
				    \caption[Translation invariance]{A human can easily tell that those two images are from the same class.}
				    \label{fig:illustration translation invariance}
				  \end{center}
				  %TODO: (a) initial image (b) sifted one
				\end{figure}
	
				\item \Important{Stable to deformations}. Once again to a human eye, it is still possible to recognize a signal if it has undergone -small- deformations. Yet if the deformations is too important the informational content of the signal is lost.\\
				
				\begin{figure}
				  \begin{center}
				    \includegraphics[width=3.5in]{placeholder.jpg}
				    \caption[stability to deformations]{A human can easily tell that (a) and (b) are from the same class. (c) can still be recognized even though it is slightly more challenging.}
				    \label{fig:illustration stability deformations}
				    %TODO: (a) initial image (b) sightly deformed one (c) heavy deformation
				  \end{center}
				\end{figure}
				
				\item \Important{To a certain degree invariant to rotations}. Rotations cannot be handled as easily as translation. Indeed here one is after a local rotational invariance rather than a global one. Solutions exist to develop a scattering transform with such behaviour \textbf{\textit{citation}} but this will not be addressed in this review. %TODO: citation and change if it is
			
					\begin{figure}
					  \begin{center}
					    \includegraphics[width=3.5in]{placeholder.jpg}
					    \caption[Rotation invariance]{A human can easily tell that (a) and (b) are from the same class. (c) could be a  '6' slightly rotated or a '9' heavily rotated.}
					    \label{fig:illustration rotation invariance}
					    %TODO: (a) initial image (b) sightly rotated one (c) 180 rotation
					  \end{center}
					\end{figure}	
			  	\end{itemize}
      
		\subsection{Formalization of a ``good'' image representation:}
      \label{seq:Intro/Image rep/Formalization}
      
      Let us now try to provide formal mathematical framework to the properties listed above.\\
      
      In this document we define a signal as,
      
     \begin{defn} \textbf{Signal}\\ 
				A signal $f$ is a square-integrable $d$ dimensional real function.
        \begin{equation*}
	  				f \in \mathcal{L}^{2}(\mathds{R}^{d}).
				\end{equation*}
				\label{def:signal}
      \end{defn}
      
      We will call $L_{(.)}$ the translation operator for the function in 
      $\mathcal{L}^{2}(\mathds{R}^{d})$, \ie for $f \in \mathcal{L}^{2}(\mathds{R}^{d}) \;\; 
      \text{and} \;\; (x,c) \in (\mathds{R}^{d})^{2} \;\;\;\; L_{c}f(x) = f(x-c)$. An operator is translation invariant -resp: canonical translation invariant- if,

      \begin{prop} \textbf{Translation invariant}\\ 
				An operator $\Phi: \mathcal{L}^{2}(\mathds{R}^{d}) \rightarrow \mathcal{H}$ where $\mathcal{H}$ is an Hilbert space is translation invariant if:
	      	\begin{equation*}
			  		\forall c \in \mathds{R}^{d} 
			  		\;\; \text{and}  \;\;
			  		\forall f \in \mathcal{L}^{2}(\mathds{R}^{d}) \;\;
			  		\Phi(L_{c}f) = \Phi(f).
				\end{equation*}
				\label{def:translation inv}
      \end{prop}
      \vspace{-30pt}
      
      \begin{prop} \textbf{Canonical translation invariant}\\ 
				An operator $\Phi: \mathcal{L}^{2}(\mathds{R}^{d}) \rightarrow \mathcal{H}$ where $\mathcal{H}$ is an Hilbert space is canonical translation invariant if:
        \begin{equation*}
			  		\forall f \in \mathcal{L}^{2}(\mathds{R}^{d}) \;\;
			  		\Phi(L_{a}f) = \Phi(f) 
			  		\;\; \text{where} \; a \in \mathds{R}^{d} \; \text{is function of} \; f.
				\end{equation*}
				\label{ppt:translation inv}
      \end{prop}
      
      For the usual representation operators instabilities to deformations are known to appear -especially at high frequencies. To prevent this, one would like the representation to be non-expansive,
      
      \begin{defn} \textbf{Non-expensive representation}
				A representation $\Phi$ is non-expensive if,
				\begin{equation}
			  		\forall (f,h) \in (\mathcal{L}^{2}(\mathds{R}^{d}))^{2} \;\; 
			  		\norm{\Phi(f) - \Phi(h)} \leq \norm{f-h}.
				\end{equation}
				\label{def:non expensive}
      \end{defn}
      
      The stability to deformations of a non-expansive operator can be expressed as its Lipschitz continuity to the action of deformations close to translations \textbf{\textit{cite mallat GIS}}. Such a diffeormorphism transform can be expressed as,
      \begin{equation*}
      		\begin{split}
      			L_{\tau}	: \; & \mathcal{L}^{2}(\mathds{R}^{d}) \rightarrow \mathcal{L}^{2}(\mathds{R}^{d})\\
      							  & \;\;\;\; f \;\;\;\;\; \rightarrow  f(\mathds{1} - \tau)
				\end{split}
      \end{equation*}

			where $\tau(x) \in \mathds{R}^{d}$ is a displacement field.
			
			\begin{prop} \textbf{Lipschitz continuous}
				A translation invariant operator $\Phi$ is said to be Lipschitz continuous to the action of $mathcal{C}^{2}$ diffeomorphisms if for any compact $\Omega \in \mathds{R}^{d}$ there exists $C$ such that for all $f \in \mathcal{L}^{2}(\mathds{R}^{d})$ supported in $\Omega$ and all $\tau \in mathcal{C}^{2}(\dsR^{d})$,
				\begin{equation}
					\norm{\Phi(f) - \Phi(L_{\tau}f)}_{\mathcal{H}} \leq 
					C \norm{f} \left(\sup_{x \in \dsR^{d}} \abs{\nabla \tau(x)} + \sup_{x \in \dsR^{d}} \abs{\mathit{H}\tau(x)}\right)
					\label{eq:Lipschitz continuity}
				\end{equation}
				\label{pty:Lipschitz continuity}
			\end{prop}
      
      where $\abs{\nabla \tau(x)}$ and $\abs{\mathit{H}\tau(x)}$ are respectively the sup-norm and the sup-norm of the Hessian tensor of the matrix $\tau(x)$.\\
      
      Hence a Lipschitz continuous operator $\Phi$ is almost invariant to "local" translations by $\tau(x)$, up to the fist and second order deformations terms. The equation~\ref{eq:Lipschitz continuity} also implies that $\Phi$ is invariant to global translations.\\

    \subsection{State of the art in image representation:}
      \label{seq:Intro/Image rep/State of the art}      
      
      Now that we have listed the properties we would like our representation to have, let us have a look at the usual signal representation tools and see if they which of them they fulfil.\\
      
      The first representation method one can think of is the modulus of the \Important{Fourier transform}. This operator is informational enough to allow -to a certain extent- discrimination different type of signal \textbf{\textit{find a citation for clf with fourier transform}}. It is also translation invariant \textbf{\textit{find a citation}}. However it is well known that those operators present instabilities to deformation at high frequencies \textbf{\textit{cite 10 from mallat}} and thus are not Lipschitz continuous to the action of diffeomorphisms.\\
      
			\Important{Wavelet transform} is another popular representation method. Again they provide a "good enough" representation to allow classification of different signals \textbf{\textit{find citations}}. Plus by grouping high frequencies into dyadic packet in $\dsR^{d}$, wavelet operators are stable to deformations \textbf{\textit{citation mallat's book}}. However wavelets are known to be non-invariant to translations.\\
			
			Another signal representation method popular at the moment are the \Important{convolutional neural networks} \textbf{\textit{cite LeCun}}. As opposed to the two previously mentioned representation methods, those operators are not fixed but learned from the data \textbf{\textit{cite learning method from CNN}}. Over the past few year they have provided state of the art results on many standard classification task, such as MNIST \textbf{\textit{cite}}, CIFAR \textbf{\textit{cite}}, ImageNet \textbf{\textit{cite }} or \textbf{\textit{find a example in speech processing}}. Those good results are used to advocate that those networks are learning "good" representations. However it seems that in certain cases they learn representation of the data that are -for example- not invariant to deformations \textbf{\textit{cite Bruna and Al strange pties of NN}}.\\
			
			In the next section we will focus on the construction of a wavelet based operator with a structure somehow similar to a convolutional neural network which will be fulfilling all the properties of what we have defined as a "good" signal representation for classification.
			
      Another representation method popular at the moment is the convolutional neural networks.       
      
      
  \section{Scattering transform:}
    ??? - not sure yet
    
  \section{Probabilistic graphical model:}
    ??? - not sure yet

  \section{Outline of the report:}
    \label{seq:Intro/Outline of the report}    

    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{The Scattering transform:}
  \label{chap:ST}
  %TODO: In this section we will introduce the scattering transform as a mathematical object providing a deterministic representation of a signal.
  %TODO2:Properly cite mallat, bruna and al
  
  In this section we describe the contruction process of a mathematical operator - the scattering
  transform (ST)- designed to generate what we have considered to be an interesting representation of
  our data (see \ref{seq:Intro/Image rep}). Therefore a scattering transform builds \Important{invariant}, \Important{stable} and 
  \Important{informative representation} of signals through a \Important{non-linear},
  \Important{unitary transform}. It is an operator delocalizing signal informational content into 
  scattering decomposition path, computed by \Important{cascading wavelet/modulus operators}.
  This architecture is similar to a \Important{convolutional neural network} (CNN) where the synaptic 
  weights would be given by a wavelet operator instead of learned.\\
  
  In \ref{seq:ST/CNN}, we will quickly introduce the standard architecture of a CNN, then we will
  explain how are built the scattering operators (see \ref{seq:ST/SCN}) and review some of their
  important properties (see {\textbf{\textit{ref: correct section}}). Finally, in 
  \ref{seq:ST/Applications to clf}, we will describe how the scattering transform is usually used
  for classification tasks.
	
  \section{Convolutional Neural Network:}
    \label{seq:ST/CNN}
    
    %TODO: Quick presentation of those network architecture to explain the motivation of the scattering transform
 
    A convolutional neural network is a multilayer architecture cascading... \textbf{\textit{TBD}}
    
    \section{Scattering Convolution Network:}
    \label{seq:ST/SCN}
      %TODO Description of  the scattering tool -- Progressive construction of a roto-translation invariant representation.
	      
      In this section we first introduce a wavelet-based scattering transform built to have interesting
      properties for classification tasks, meaning being translation invariant and stable to 
      $\mathbf{\mathcal{L}^{2}}$ deformations. Then we describe its convolutional architecture.
      
    \subsection{Scattering wavelets:}
      \label{subseq:ST/SCN/Scattering wavelets}

      A two-dimensional directional wavelet is obtained by scaling and rotating a single band-pass 
      filter $\psi$. If we let $G$ be a discrete, finite rotation group of $\dsR^{2}$, 
      multi-scale directional wavelet filters are defined for any scale $j \in \dsZ$ and rotation
      $r \in G$ by
      
      \begin{equation}
	\label{eq:multi-scale directional wavelet}
	\psi_{2^{j}r}(u) = 2^{2j} \psi(2^{j}r^{-1}u).
      \end{equation}
      
      To simplify the notations, we will now denote $\lambda = \lambda(j,r) \; \eqdef \; 2^{j}r \in
      \Lambda \; \eqdef \; G \times \dsZ$.\\
      
      A wavelet transform filters the signal $x$ using a family of wavelets $\{x \ast 
      \psi_{\lambda}(u)\}_{\lambda}$. This is computed from a filter bank of dilated and rotated wavelets
      having no orthogonality property and it creates a multi-scale and orientation representation of the
      input.\\ 
      %TODO: maybe add part on aliasing and invertibility
      
      If $u.u'$ and $\|u\|$ define respectively the inner product and the norm in $\dsR^{2}$,
      the Morlet wavelet $\psi$ is an example of wavelet given by
      
      \begin{equation*}
	\label{eq:Morlet wavelet}
	\psi(u) = C_{1}(e^{iu.\xi} - C_{2}) e^{\|u\|^{2}/(2 \sigma^{2})},
      \end{equation*}

      where $C_{1}$, $\xi$ and $\sigma$ are meta-parameters of the wavelet and $C_{2}$ is adjusted
      so that $\int \psi(u) du = 0$. Figure~\ref{fig:Morlet wavelet} shows a Morlet wavelet for 
      $\xi= 3\pi/4$ and $\sigma=0.85$.\\
      
      \begin{figure}
	\begin{center}
	  \includegraphics[width=3.5in]{placeholder.jpg}
	  \caption{Complex Morlet wavelet.}
	  \label{fig:Morlet wavelet}
	  \end{center}	
      \end{figure}
      
      As opposed to the Fourier sinusoidal waves, wavelets are operators stable to 
      $\mathbf{\mathcal{L}^{2}}$ deformations as they can be expressed as localized waveforms 
      (\textbf{\textit{citation}}). However, wavelet transforms compute convolutions with wavelets,
      hence they are translation covariant operators (\textbf{\textit{citation}}).
        
      To ensure a translation invariant behavior to an operator commuting with them, one has to
      introduce a non-linearity. For example if $R$ is a linear or non-linear operator commuting with
      translations $L_{c}$, \ie $R(L_{c}x) = L_{c}R(x)$), then the integral $\int R(x(u))du$ is 
      translation invariant. One can apply this to $R(x) = x \ast \psi_{\lambda}$ and gets the 
      trivial invariant,
      
      \begin{equation*}
	\label{eq:Trivial invariant}
	\int x \ast \psi_{\lambda}(u)du = 0,
      \end{equation*}
      
      for all $x$ as $\int \psi_{\lambda}(u)du = 0$. However to preserve the informative character of 
      the  scattering operator, one has to ensure that the integral does not vanish. To do so an 
      operator $M$ such that $R(x) = M(x \ast \psi_{\lambda})$ is introduced. If $M$ is a linear 
      transformation commuting with translation then the integral still vanishes. Hence one has to 
      choose $M$ to be a non-linear.\\
      
      Keeping in mind that the scattering transform has to be stable to deformations and taking
      advantages of the wavelet transform stability to small deformations in the input space,
      we also impose that $M$ commutes with deformations
      
      \begin{equation*}
	\label{eq:Commute with deformations}
	\forall \tau(u) \; , \; M L_{\tau} = L_{\tau} M.
      \end{equation*}
      
      If a weak differentiability condition is added, one can prove (\textbf{\textit{ref ISCN 6}}) that
      $M$ must necessarily be a point-wise operator, \ie $Mx(u)$ only depends on the value of $x(u)$.
      Finally, by adding an $\mathbf{\mathcal{L}^{2}}(\dsR^{2})$ stability constraint,
      
      \begin{equation*}
	\label{eq:L2 stability}
	\forall (x,y) \in \mathbf{\mathcal{L}^{2}}(\dsR^{2})^{2} \; , \; 
	\norm{Mx} = \norm{x} 
	\; \text{and} \;
	\norm{Mx-My} \leq \norm{x-y},
      \end{equation*}     
      
      one can show (\textbf{\textit{ref ISCN 6}}) that necessarily $Mx= e^{i\alpha}\abs{x}$. For the 
      scattering transform $\alpha$ is set to $0$ and therefore the resulting coefficients are the 
       $\mathbf{\mathcal{L}^{1}}(\dsR^{2})$ norms:
       
      \begin{equation*}
	\label{eq:L1 norm}
	\norm{x \ast \psi_{\lambda}}_{1} = \int \abs{x \ast \psi_{\lambda}} du
      \end{equation*}      
      
      The family of $\mathbf{\mathcal{L}^{1}}(\dsR^{2})$ normed wavelet $\{ \norm{x \ast 
      \psi_{\lambda}}_{1}\}_{\lambda}$ generate a crude signal representation which measures the 
      sparsity of the wavelet coefficients. One can prove (\textbf{\textit{ref ISCN 36}}) that $x$ can
      be reconstructed from $\{ \abs{ x \ast \psi_{\lambda}(u)} \}_{\lambda}$ up to a multiplicative 
      constant. Which means that the information loss in $\{ \norm{x \ast \psi_{\lambda_{1}}} 
      \}_{\lambda}$ comes from the integration of the absolute value $\abs{ x \ast \psi_{\lambda}(u)}$
      which removes all non-zero frequencies. However those components can be recovered by calculating 
      the wavelet coefficients $\abs{ x \ast \psi_{\lambda_{1}}} \ast \psi_{\lambda_{2}}(u)$. 
      By doing so their $\mathbf{\mathcal{L}^{1}}(\dsR^{2})$ norms define a much larger family
      of invariants:
      
      \begin{equation*}
	\label{eq:2nd order}
	\forall (\lambda_{1}, \lambda_{2}) 
	\norm{ \abs{ x \ast \psi_{\lambda_{1}}} \ast \lambda_{2}}_{1} =
	\int \abs{ \abs{ x \ast \psi_{\lambda_{1}}(u)} \ast \psi_{\lambda_{2}}} du
      \end{equation*}          
      
      By further iterating on the wavelet/modulus operators more translation invariant coefficients can 
      be computed. Let us define:
      
      \begin{defn} \textbf{Scattering Propagator}\\ 
	The scattering operator $U$ for a scale and an orientation $\lambda \in G \times \dsZ$ 
	is defined as the absolute value of the input convolved with the wavelet operator at this 
	scale and orientation.
        
        \begin{equation}
	  \label{eq:scattering propagator}
	  U[\lambda](x) \eqdef \abs{x \ast \psi_{\lambda}}
	\end{equation}
	\label{def:SO}
      \end{defn}
      
      \begin{defn} \textbf{Path Ordered Scattering Propagators}\\ 
	Any sequence $p = (\lambda_{1}, \lambda_{2},...,\lambda_{m})$ where $\forall i \in
	\llbracket 1,m \rrbracket \lambda_{i} \in G \times \dsZ$ defines a \Important{path} of 
	length $m$, \ie the ordered product of non-linear and non-commuting operators
	
	\begin{equation}
	  \label{eq:path ordered SP}
	  \begin{split}
	    U[p]x &\eqdef U[\lambda_{m}]...U[\lambda_{2}]U[\lambda_{1}](x) \\
		    &= \abs{\abs{\abs{\abs{x \ast \psi_{\lambda_{1}}} \ast \psi_{\lambda_{2}}} 
		    ...} \ast \psi_{\lambda_{m}}}	   
	  \end{split}
	\end{equation}
	
	With the convention: $U[\emptyset]x = x$\\
	\label{def:path ordered SO}
      \end{defn} 
      
      From there one can provide a first formal definition of the scattering transform:
      
      \begin{defn} \textbf{Scattering Coefficient}\\
	A scattering coefficient along the path $p$ is defined as an integral of the $p$ ordered 
	scattering propagators, normalized by the response of a Dirac:
      
	\begin{equation}
	  \label{eq:ST1}
	  \bar{S}[p](x) \eqdef \mu_{p}^{-1} \int U[p]x(u) du
	\end{equation}
	
	with,
	
	\begin{equation*}
	  \label{eq:ST normalization}
	  \mu_{p} \eqdef \int U[p]\delta(u)du      
	\end{equation*}
	\label{def:SC}
      \end{defn}
      
      We shall see later (\textbf{\textit{reference}}) that each scattering coefficient $\bar{S}[p](x)$ 
      is -as desired - invariant to translation of the input $x$ and Lipschitz continuous to 
      deformations.\\
      
      For classification tasks, one might want to compute localized descriptors only invariant 
      to translations smaller than a predefined scale $2^{J}$, while keeping the spatial variability at 
      scales larger than $2^{J}$. One can achieved this by localizing the scattering integral with a 
      scaled spatial window $\phi_{2^{J}}(u) = 2^{-2J} \phi(2^{-2J}u)$. This yield to the definition of
      the windowed scattering transform:
      
      \begin{defn} \textbf{-Windowed- Scattering Coefficient Of Order $m$}\\
	If $p$ is a path of length $m \in \mathds{N}$, the -windowed- scattering coefficient of order m
	at scale $2^{J}$ is defined as:
	
	\begin{equation}
	  \label{eq:ST windowed}
	  \begin{split}
	    S_{J}[p](x) &\eqdef U[p]x \ast \phi_{2^{J}}(u) \\
		        &= \int U[p]x(v) \phi_{2^{J}}(u-v) dv \\
		        &= \abs{\abs{\abs{\abs{x \ast \psi_{\lambda_{1}}} \ast \psi_{\lambda_{2}}} 
		    ...} \ast \psi_{\lambda_{m}}} \ast \phi_{2^{J}}(u)
	  \end{split}
	\end{equation}
	
	With the convention: $S_{J}[\emptyset]x = x \ast \phi_{2^{J}}$\\
      	\label{def:SC windozed}
      \end{defn}

      % TODO: Move this to the properties section
%       For each path $p$, $S_{J}[p]x(u)$ is a function of the window position $u$, which can be sub-sampled
%       at interval proportional to the window size $2^{J}$. Averaging by $\phi_{2^{J}}$ implies that 
%       $S_{J}[p]x(u)$ is nearly invariant 

    \subsection{Scattering Convolution Network:}
      \label{subseq:ST/SCN/SCN}
      
      \subsubsection{The first order scattering coefficient:}
	\label{subsubseq:ST/SCN/SCN/1st order}
	
	
      
      A scattering transform computes higher-order coefficients by further iterating on the wavelet 
      transform/modulus operators. At a maximum scale $2^{J}$, wavelet coefficients are computed at
      frequencies $2^{j} \geq 2^{-J}$, and lower frequencies are filtered by $\phi_{2^{J}} = 
      2^{-2J} \phi(2^{-J}u)$.\\
      
      In the image processing case, as images are real-valued signal, one can only consider the 
      ``positive'' rotations $r \in G^{+}$ with angles $[0, \pi)$:
      
      \begin{equation}
	\label{eq:positive WT}
	W_{J}x(u)
      \end{equation}

      
      
    \subsection{Spatial Wavelet transform:}
      \label{subseq:ST/RT ST networks/Spatial WT}
      Introduction of the translation invariance.
		
    \subsection{Roto-translation wavelet transform:}
      \label{subseq:ST/RT ST networks/RT WT}
      Introduction of the pseudo rotation invariance.
		
  \section{Application to classification:}
    \label{seq:ST/Applications to clf}
    Examples of Mallat's work.
      
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Probabilistic graphical models:}
  \label{chap:PGMs}
  In this chapter, we will introduce the two mains classes of probabilistic graphical models. 
  (1) The Bayesian networks. 
  (2) The Hidden Markov Models.
	
  \section{Bayesian Network:}  
    \label{seq:PGMs/BN}
    Quick overview over the main methods for BNs.
    
    \subsection{Architecture:}
      \label{subseq:PGMs/BN/Architecture}
      TBD

    \subsection{Learning:}
      \label{subseq:PGMs/BN/Learning}
      TBD

    \subsection{Inference:}
      \label{subseq:PGMs/BN/Inference}
      TBD
      
  \section{Hidden Markov Models:}
    \label{seq:PGMs/HMM}
    Quick overview over the main methods for HMMs.
    
    \subsection{Architecture:}
      \label{subseq:PGMs/HMM/Architecture}
      TBD

    \subsection{Learning:}
      \label{subseq:PGMs/HMM/Learning}
      TBD

    \subsection{Inference:}
      \label{subseq:PGMs/HMM/Inference}
      TBD
      
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Hidden Markov trees:}
  \label{chap:HMT}
    More at length description of the specific properties of HMTs.
	
  \section{The tree structure:}
    \label{seq:HMT/Tree:}
    TBD

  \section{Learning:}
    \label{seq:HMT/EM}

    \subsection{Expectation maximization:}
      \label{subseq:HMT/EM}
      EM by Crouse and EM by Durand
      
    \subsection{Variational methods:}
      \label{subseq:HMT/Var}
      Variational like Crouse and variational like Durand    
  
    
  \section{Generation: Vitterbi algorithm:}
    \label{seq:HMT/Generation}

		
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Scattering hidden Markov tree:}
  \label{chap:SHMT:}

  \section{Related work:}
    \label{seq:SHMT/Rel work}
  
    Wavelet hidden markov trees
  
  \section{Hypothesis:}
    \label{seq:SHMT/Hypos}
    (1) 2 populations. (2) Persistence. 
      
  \section{Persistence:}
    \label{seq:SHMT/Persistence}
    Hopefully some kind of proof there.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Experimental results:}
  \label{chap:Experimental results}
  TBD
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion:}
  \paragraph{Scattering hidden Markov tree:}
    TBD
  \paragraph{Next steps:}
    Variational methods
    General graphical models
    Bayesian neural networks
    
    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Acknowledgements:}

	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%   
% Bibliography %
%%%%%%%%%%%%%%%%
\begin{thebibliography}{99}
  
\end{thebibliography}
     
% THE END
\end{document}